name: UnitTests for Fine-tuning LLMs

on:
  pull_request:
    types: [opened, synchronize, edited]

jobs:
  run:
    if: false == contains(github.event.pull_request.title, 'WIP')
    runs-on: ${{ matrix.os }}
    timeout-minutes: 20
    strategy:
      matrix:
        os: [ubuntu-latest]
        python-version: ['3.9']
        torch-version: ['2.0.0']
        torchvision-version: ['0.15.0']
        torchaudio-version: ['2.0.0']
    env:
      OS: ${{ matrix.os }}
      PYTHON: '3.9'
    steps:
    - uses: actions/checkout@master
    - name: Setup Python ${{ matrix.python-version }}
      uses: actions/setup-python@master
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install PyTorch ${{ matrix.torch-version }}+cpu
      run: |
        pip install numpy typing-extensions dataclasses
        pip install torch==${{ matrix.torch-version}}+cpu torchvision==${{matrix.torchvision-version}}+cpu torchaudio==${{matrix.torchaudio-version}}+cpu -f https://download.pytorch.org/whl/torch_stable.html
    - name: Install FS
      run: |
        pip install -e .[llm,test]
    - name: Test GPT2
      run: |
        python federatedscope/main.py --cfg federatedscope/llm/baseline/testcase.yaml federate.total_round_num 1 eval.count_flops False train.local_update_steps 2 data.splits "[0.998, 0.001, 0.001]"
        [ $? -eq 1 ] && exit 1 || echo "Passed"
    - name: Test GPT2 with offsite-tuning
      run: |
        python federatedscope/main.py --cfg federatedscope/llm/baseline/testcase.yaml federate.total_round_num 1 eval.count_flops False llm.offsite_tuning.use True llm.offsite_tuning.emu_l 2 llm.offsite_tuning.emu_r 10 train.local_update_steps 2 data.splits "[0.998, 0.001, 0.001]"
        [ $? -eq 1 ] && exit 1 || echo "Passed"