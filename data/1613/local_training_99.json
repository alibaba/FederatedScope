[{"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Given a pair of ultrasound and audio segments we can calculate the distance between them using our model. To predict the synchronisation offset for an utterance, we consider a discretised set of candidate offsets, calculate the average distance for each across utterance segments, and select the one with the minimum average distance.  \n Question: Does their neural network predict a single offset in a recording?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": " The flexibility of the DNN model allowed us to include many more surface level features such as affixes, leading and trailing characters in words and stems, and the presence of words in large gazetteers of named entities. As we show later, these additional features significantly lowered CEER. \n Question: what surface-level features are used?", "output": "affixes, leading and trailing characters in words and stems, and the presence of words in large gazetteers of named entities", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The encoder is a Convolutional Neural Network (CNN) and the decoder is a Long Short-Term Memory (LSTM) network, as presented in Figure 2 . The image is passed through the encoder generating the image representation that is used by the decoder to know the content of the image and generate the description word by word. \n Question: What model is used to encode the images?", "output": "a Convolutional Neural Network (CNN)", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Table 1 shows an example of difficult and easy examples according to our definition of difficulty. The underlined text demarcates the (consensus) reference label provided by domain experts. In the difficult examples, crowd workers marked text distinct from these reference annotations; whereas in the easy cases they reproduced them with reasonable fidelity. The difficult sentences usually exhibit complicated structure and feature jargon. \n Question: Is an instance a sentence or an IE tuple?", "output": "sentence", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "To test this hypothesis we first trained UG-WGAN in English, Chinese and German following the procedure described in Section \"UG-WGAN\" . For this experiment we trained UG-WGAN on the English and Russian language following the procedure described in Section \"UG-WGAN\" .  To do so we trained 6 UG-WGAN models on the following languages: English, Russian, Arabic, Chinese, German, Spanish, French. \n Question: What are the languages they consider in this paper?", "output": "The languages considered were English, Chinese, German, Russian, Arabic, Spanish, French", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Bi-LSTM BIBREF11 is a baseline for neural models. Bi-LSTM$_{+ att. + LEX + POS}$ BIBREF10 is a multi-task learning framework for WSD, POS tagging, and LEX with self-attention mechanism, which converts WSD to a sequence learning task. GAS$_{ext}$ BIBREF12 is a variant of GAS which is a gloss-augmented variant of the memory network by extending gloss knowledge. CAN$^s$ and HCAN BIBREF13 are sentence-level and hierarchical co-attention neural network models which leverage gloss knowledge. \n Question: How does the neural network architecture accomodate an unknown amount of senses per word?", "output": "converts WSD to a sequence learning task  leverage gloss knowledge by extending gloss knowledge", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "However, all previous IE benchmarks BIBREF18 are too small to train neural network models typically used in QA, and thus we need to build a large benchmark. Therefore, we build a large scale benchmark named QA4IE benchmark which consists of 293K Wikipedia articles and 2M golden relation triples with 636 different relation types. We manually find 148 relations which can be projected to a WikiData relation out of 2064 DBpedia relations. \n Question: Was this benchmark automatically created from an existing dataset?", "output": "No", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Hashtag prediction for social media has been addressed earlier, for example in BIBREF15 , BIBREF16 . BIBREF15 also use a neural architecture, but compose text embeddings from a lookup table of words. \n Question: Is this hashtag prediction task an established task, or something new?", "output": "established task", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Macaw also supports Wizard of Oz studies or intermediary-based information seeking studies. The architecture of Macaw for such setup is presented in FIGREF16. As shown in the figure, the seeker interacts with a real conversational interface that supports multi-modal and mixed-initiative interactions in multiple devices. The intermediary (or the wizard) receives the seeker's message and performs different information seeking actions with Macaw. All seeker-intermediary and intermediary-system interactions will be logged for further analysis. \n Question: What is a wizard of oz setup?", "output": "seeker interacts with a real conversational interface intermediary (or the wizard) receives the seeker's message and performs different information seeking actions", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Posts in FBFans dataset are used for this analysis. We calculate the like statistics of each distinct author from these 32,595 posts. \n Question: What is the size of the Chinese data?", "output": "32,595 posts", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We evaluate LinkNBed and baselines on two real world knowledge graphs: D-IMDB (derived from large scale IMDB data snapshot) and D-FB (derived from large scale Freebase data snapshot). \n Question: On what data is the model evaluated?", "output": "D-IMDB (derived from large scale IMDB data snapshot) D-FB (derived from large scale Freebase data snapshot)", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "On the other hand, in the case of MHD, instead of the integration at attention level, we assign multiple decoders for each head and then integrate their outputs to get a final output. \n Question: Does each attention head in the decoder calculate the same output?", "output": "No", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "after a shared CLWE space has been induced, the task is to retrieve target language translations for a test set of source language words retrieve target language translations for a test set of source language words BLI is cast as a ranking task in our BLI setup with only one correct translation for each \u201cquery\u201d word, MAP is equal to mean reciprocal rank (MRR) \n Question: How does BLI measure alignment quality?", "output": "we use mean average precision (MAP) as the main evaluation metric", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR)\nFHIR BIBREF5 is a new open standard for healthcare data developed by the same company that developed HL7v2. Resource Description Framework (RDF)\nRDF is the backbone of the semantic webBIBREF8. \n Question: What do FHIR and RDF stand for?", "output": "Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR) Resource Description Framework (RDF)", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Both systems were optimized on the tst2014 using Minimum error rate training BIBREF20 . A detailed description of the systems can be found in BIBREF21 . \n Question: How is the PBMT system trained?", "output": "systems were optimized on the tst2014 using Minimum error rate training BIBREF20", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": " We used state-of-the-art features that have shown to be useful in ID: some of them are language-independent (e.g., punctuation marks, positive and negative emoticons, quotations, personal pronouns, tweet's length, named entities) while others are language-dependent relying on dedicated lexicons (e.g., negation, opinion lexicons, opposition words). \n Question: What text-based features are used?", "output": "language-independent (e.g., punctuation marks, positive and negative emoticons, quotations, personal pronouns, tweet's length, named entities)  language-dependent relying on dedicated lexicons (e.g., negation, opinion lexicons, opposition words)", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We build and test our MMT models on the Multi30K dataset BIBREF21 . Each image in Multi30K contains one English (EN) description taken from Flickr30K BIBREF22 and human translations into German (DE), French (FR) and Czech BIBREF23 , BIBREF24 , BIBREF25 . The dataset contains 29,000 instances for training, 1,014 for development, and 1,000 for test. We only experiment with German and French, which are languages for which we have in-house expertise for the type of analysis we present. In addition to the official Multi30K test set (test 2016), we also use the test set from the latest WMT evaluation competition, test 2018 BIBREF25 . \n Question: Do they report results only on English dataset?", "output": "No", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The tweetLID workshop shared task requires systems to identify the language of tweets written in Spanish (es), Portuguese (pt), Catalan (ca), English (en), Galician (gl) and Basque (eu).  \n Question: What shared task does this system achieve SOTA in?", "output": "tweetLID workshop shared task", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Secondly, we examine the effect of Washington Post on the views of the users. This is done by looking at the sentiments of the candidates (to predict winners) of a debate before and after the winners are announced by the experts in Washington Post. This way, we can see if Washington Post has had any effect on the sentiments of the users. One can see the winners suggested by the Washington Post in Table TABREF35.  \n Question: How do you establish the ground truth of who won a debate?", "output": "experts in Washington Post", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "For the Russian language, with its rich morphology, lemmatizing the training and testing data for ELMo representations yields small but consistent improvements in the WSD task.  \n Question: What other examples of morphologically-rich languages do the authors give?", "output": "Russian", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "MTMSN BIBREF4 is the first, and only model so far, that specifically tried to tackle the multi-span questions of DROP. Their approach consisted of two parts. The first was to train a dedicated categorical variable to predict the number of spans to extract. The second was to generalize the single-span head method of extracting a span, by utilizing the non-maximum suppression (NMS) algorithm BIBREF7 to find the most probable set of non-overlapping spans. The number of spans to extract was determined by the aforementioned categorical variable \n Question: What approach did previous models use for multi-span questions?", "output": "Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "This dataset is aimed to contribute to developing a partisan news detector. There are several ways that the dataset can be used to devise the system. For example, it is possible to train the detector using publisher-level labels and test with article-level labels. It is also possible to use semi-supervised learning and treat the publisher-level part as unsupervised, or use only the article-level part. We also released the raw survey data so that new mechanisms to decide the article-level labels can be devised. \n Question: What examples of applications are mentioned?", "output": "partisan news detector", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "This chapter describes the underlying formalism of the theorem provers, as it is visible during an interactive proof trace, and present the general strategy followed by the theorem provers.  The rest of this chapter is structured as follows. Section \"Type-logical grammars\" presents a general introduction to type-logical grammars and illustrates its basic concepts using the Lambek calculus, ending the section with some problems at the syntax-semantics interface for the Lambek calculus. Type-logical grammars are a family of grammar formalisms built on a foundation of logic and type theory. Type-logical grammars originated when BIBREF4 introduced his Syntactic Calculus (called the Lambek calculus, L, by later authors). Though Lambek built on the work of BIBREF5 , BIBREF6 and others, Lambek's main innovation was to cast the calculus as a logic, giving a sequent calculus and showing decidability by means of cut elimination. \n Question: What formalism does Grail use?", "output": "a family of grammar formalisms built on a foundation of logic and type theory. Type-logical grammars originated when BIBREF4 introduced his Syntactic Calculus (called the Lambek calculus, L, by later authors).", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Following the previous works, we report the precision ($P.$), recall ($R.$) and $F_1$ scores for target recognition and targeted sentiment.  \n Question: How is the effectiveness of the model evaluated?", "output": "precision ($P.$), recall ($R.$) and $F_1$ scores for target recognition and targeted sentiment", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We distributed Dryhootch chosen surveys among 1,200 users (305 users are self claimed PTSD sufferers and rest of them are randomly chosen from previous 2,423 users) and received 210 successful responses.  Dryhootch defines the intensity of PTSD into four categories based on the weekly survey results of all three clinical survey tools (DOSPERT, BSSS and VIAS ) \n Question: Which clinically validated survey tools are used?", "output": "DOSPERT, BSSS and VIAS", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Our goal is to decouple the content selection from the decoder by introducing an extra content selector. The most intuitive way is training the content selector to target some heuristically extracted contents. For example, we can train the selector to select overlapped words between the source and target BIBREF6 , sentences with higher tf-idf scores BIBREF20 or identified image objects that appear in the caption BIBREF21 . INLINEFORM0 as Latent Variable: Another way is to treat INLINEFORM1 as a latent variable and co-train selector and generator by maximizing the marginal data likelihood. By doing so, the selector has the potential to automatically explore optimal selecting strategies best fit for the corresponding generator component. Reinforce-select (RS) BIBREF24 , BIBREF9 utilizes reinforcement learning to approximate the marginal likelihood. We propose Variational Reinforce-Select (VRS) which applies variational inference BIBREF10 for variance reduction. \n Question: How is the model trained to do only content selection?", "output": "target some heuristically extracted contents treat INLINEFORM1 as a latent variable and co-train selector and generator by maximizing the marginal data likelihood reinforcement learning to approximate the marginal likelihood  Variational Reinforce-Select (VRS) which applies variational inference BIBREF10 for variance reduction", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In this paper, we manually annotate the predicate\u2013argument structures for the 600 L2-L1 pairs as the basis for the semantic analysis of learner Chinese. \n Question: Who manually annotated the semantic roles for the set of learner texts?", "output": "Authors", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Stacked LSTMs Cell-aware Stacked LSTMs\nNow we extend the stacked LSTM formulation defined above to address the problem noted in the previous subsection. Sentence Encoders\nThe sentence encoder network we use in our experiments takes INLINEFORM0 words (assumed to be one-hot vectors) as input. Top-layer Classifiers\nFor the natural language inference experiments, we use the following heuristic function proposed by BIBREF36 in feature extraction: DISPLAYFORM0\n\nwhere INLINEFORM0 means vector concatenation, and INLINEFORM1 and INLINEFORM2 are applied element-wise. \n Question: Which models did they experiment with?", "output": "Stacked LSTMs Cell-aware Stacked LSTMs Sentence Encoders Top-layer Classifiers", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Our first task is the recently-introduced Visual Question Answering challenge (VQA) BIBREF22 . The next set of experiments we consider focuses on GeoQA, a geographical question-answering task first introduced by Krish2013Grounded. \n Question: What benchmark datasets they use?", "output": "VQA and GeoQA", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We first use predictors based on rules that have previously been proposed in the literature: word length, number of phonemes, number of syllables, alphabetical order, and frequency. We collect all binomials but make predictions only on binomials appearing at least 30 times total, stratified by subreddit. However, none of these features appear to be particularly predictive across the board (Table TABREF15). A simple linear regression model predicts close to random, which bolsters the evidence that these classical rules for frozen binomials are not predictive for general binomials. \n Question: How was performance of previously proposed rules at very large scale?", "output": " close to random,", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We collect $37,263$ concept-sets as the inputs, each of which contains three to five common concepts. These concept-sets are sampled from several large corpora of image/video captions, such that the concepts inside them are more likely to co-occur in natural scenes. The expected concept-sets in our task are supposed to be likely co-occur in natural, daily-life scenes . The concepts in images/videos captions, which usually describe scenes in our daily life, thus possess the desired property. We therefore collect a large amount of caption sentences from a variety of datasets, including VATEX BIBREF4, LSMDC BIBREF12, ActivityNet BIBREF13, and SNLI BIBREF15, forming 1,040,330 sentences in total. \n Question: Where do the concept sets come from?", "output": "These concept-sets are sampled from several large corpora of image/video captions", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "For answer retrieval, a dataset is created by INLINEFORM4 , which gives INLINEFORM5 accuracy and INLINEFORM6 coverage, respectively. \n Question: Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "To better analyze model generalization to an unseen, new domain as well as model leveraging the out-of-domain sources, we propose a new architecture which is an extension of the ARED model. In order to better select, aggregate and control the semantic information, a Refinement Adjustment LSTM-based component (RALSTM) is introduced to the decoder side. The proposed model can learn from unaligned data by jointly training the sentence planning and surface realization to produce natural language sentences.  \n Question: What is the difference of the proposed model with a standard RNN encoder-decoder?", "output": "Introduce a \"Refinement Adjustment LSTM-based component\" to the decoder", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In general, the baseline model CAEVO is outperformed by both NN models, and NN model with BERT embedding achieves the greatest performance. \n Question: Do the BERT-based embeddings improve results?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Single-Turn: This dataset consists of single-turn instances of statements and responses from the MiM chatbot developed at Constellation AI BIBREF5. Multi-Turn: This dataset is taken from the ConvAI2 challenge and consists of various types of dialogue that have been generated by human-computer conversations. \n Question: what datasets did they use?", "output": "Single-Turn Multi-Turn", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "For training data, 3 million webpages from this corpus were processed with a CCG parser to produce logical forms BIBREF10 . We also used the test set created by Krishnamurthy and Mitchell, which contains 220 queries generated in the same fashion as the training data from a separate section of ClueWeb. However, as they did not release a development set with their data, we used this set as a development set. This final test set contains 307 queries. \n Question: How big is their dataset?", "output": "3 million webpages processed with a CCG parser for training, 220 queries for development, and 307 queries for testing", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "This work presented MVCNN, a novel CNN architecture for sentence classification. It combines multichannel initialization \u2013 diverse versions of pretrained word embeddings are used \u2013 and variable-size filters \u2013 features of multigranular phrases are extracted with variable-size convolution filters.  \n Question: How is MVCNN compared to CNN?", "output": "MVCNN, a novel CNN architecture for sentence classification. It combines multichannel initialization \u2013 diverse versions of pretrained word embeddings are used \u2013 and variable-size filters \u2013 features of multigranular phrases are extracted with variable-size convolution filters. ", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Our model BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS outperforms all other model experimented in OurNepali and ILPRL dataset respectively. \n Question: What is the best model?", "output": "BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS ", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "For the sentiment analysis, we look at our problem in a multi-label setting, our two labels being sentiment polarity and the candidate/category in consideration. \n Question: How many label options are there in the multi-label task?", "output": " two labels ", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We will compare to the more recent cross-lingual language model XLM BIBREF12, as well as the state-of-the-art CoNLL 2018 shared task results with predicted tokenisation and segmentation in an updated version of the paper. In French, no extensive work has been done due to the limited availability of NER corpora. We compare our model with the strong baselines settled by BIBREF49, who trained both CRF and BiLSTM-CRF architectures on the FTB and enhanced them using heuristics and pretrained word embeddings. In the TRANSLATE-TRAIN setting, we report the best scores from previous literature along with ours. BiLSTM-max is the best model in the original XNLI paper, mBERT which has been reported in French in BIBREF52 and XLM (MLM+TLM) is the best-presented model from BIBREF50. \n Question: What is the state of the art?", "output": "POS and DP task: CONLL 2018\nNER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF\nNLI task: mBERT or XLM (not clear from text)", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In our setup, our starting point is a base model, trained on NLI data. Rather than employing automated adversarial methods, here the model's \u201cadversary\u201d is a human annotator. Given a context (also often called a \u201cpremise\u201d in NLI), and a desired target label, we ask the human writer to provide a hypothesis that fools the model into misclassifying the label. One can think of the writer as a \u201cwhite hat\u201d hacker, trying to identify vulnerabilities in the system. For each human-generated example that is misclassified, we also ask the writer to provide a reason why they believe it was misclassified. \n Question: Do they use active learning to create their dataset?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "a) Parallel Scan Inference b) Vectorized Parsing c) Semiring Matrix Operations Torch-Struct aims for computational and memory efficiency. Implemented naively, dynamic programming algorithms in Python are prohibitively slow. As such Torch-Struct provides key primitives to help batch and vectorize these algorithms to take advantage of GPU computation and to minimize the overhead of backpropagating through chart-based dynamic programmming. \n Question: What general-purpose optimizations are included?", "output": "Parallel Scan Inference Vectorized Parsing Semiring Matrix Operations", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Our WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. \n Question: How higher are F1 scores compared to previous work?", "output": "WordDecoding (WDec) model achieves F1 scores that are $3.9\\%$ and $4.1\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\\%$ and $1.3\\%$ higher than HRL on the NYT29 and NYT24 datasets respectively", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We experiment and compare with the following models.\n\nPointer-Gen is the baseline model trained by optimizing $L_\\text{MLE}$ in Equation DISPLAY_FORM13.\n\nPointer-Gen+Pos is the baseline model by training Pointer-Gen only on positive examples whose sensationalism score is larger than 0.5\n\nPointer-Gen+Same-FT is the model which fine-tunes Pointer-Gen on the training samples whose sensationalism score is larger than 0.1\n\nPointer-Gen+Pos-FT is the model which fine-tunes Pointer-Gen on the training samples whose sensationalism score is larger than 0.5\n\nPointer-Gen+RL-ROUGE is the baseline model trained by optimizing $L_\\text{RL-ROUGE}$ in Equation DISPLAY_FORM17, with ROUGE-L BIBREF9 as the reward.\n\nPointer-Gen+RL-SEN is the baseline model trained by optimizing $L_\\text{RL-SEN}$ in Equation DISPLAY_FORM17, with $\\alpha _\\text{sen}$ as the reward. \n Question: Which baselines are used for evaluation?", "output": "Pointer-Gen Pointer-Gen+Pos Pointer-Gen+Same-FT Pointer-Gen+Pos-FT Pointer-Gen+RL-ROUGE Pointer-Gen+RL-SEN", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each. \n Question: Where did they collect their dataset from?", "output": "from Arabic WikiNews site", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The dataset needed to train the W2V was obtained using the information extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila). \n Question: What dataset is used for training Word2Vec in Italian language?", "output": "extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila)", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": " For a given query $q = \\langle s, r, ? \\rangle $ , we identify mentions in $S_q$ of the entities in $C_q \\cup \\lbrace s\\rbrace $ and create one node per mention. This process is based on the following heuristic:\n\nwe consider mentions spans in $S_q$ exactly matching an element of $C_q \\cup \\lbrace s\\rbrace $ . Admittedly, this is a rather simple strategy which may suffer from low recall.\n\nwe use predictions from a coreference resolution system to add mentions of elements in $C_q \\cup \\lbrace s\\rbrace $ beyond exact matching (including both noun phrases and anaphoric pronouns). In particular, we use the end-to-end coreference resolution by BIBREF16 .\n\nwe discard mentions which are ambiguously resolved to multiple coreference chains; this may sacrifice recall, but avoids propagating ambiguity. \n Question: How did they detect entity mentions?", "output": "Exact matches to the entity string and predictions from a coreference resolution system", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Moreover, to be able to make use of multiple answer styles within a single system, our model introduces an artificial token corresponding to the target style at the beginning of the answer sentence ( $y_1$ ), like BIBREF14 . At test time, the user can specify the first token to control the answer styles. \n Question: Does their model also take the expected answer style as input?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We obtain word vectors of size 300 from the learned word embeddings. To represent a Twitter profile, we retrieve word vectors for all the words that appear in a particular profile including the words appear in tweets, profile description, words extracted from emoji, cover and profile images converted to textual formats, and words extracted from YouTube video comments and descriptions for all YouTube videos shared in the user's timeline. Those word vectors are combined to compute the final feature vector for the Twitter profile. \n Question: How in YouTube content translated into a vector format?", "output": "words extracted from YouTube video comments and descriptions for all YouTube videos shared in the user's timeline", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data. \n Question: What limitations do the authors demnostrate of their model?", "output": "Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We also conduct two human evaluations in order to assess (a) which type of summary participants prefer (we compare extractive and abstractive systems) and (b) how much key information from the document is preserved in the summary (we ask participants to answer questions pertaining to the content in the document by reading system summaries). \n Question: Do they use other evaluation metrics besides ROUGE?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": " Here a small portion of the large parallel corpus for English-German is used as a simulation for the scenario where we do not have much parallel data: Translating texts in English to German.  \n Question: Which languages do they test on for the under-resourced scenario?", "output": "English German", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Conclusion \n Question: What is triangulation?", "output": "Answer with content missing: (Chapter 3) The concept can be easily explained with an example, visualized in Figure 1. Consider the Portuguese (Pt) word trabalho which, according to the MUSE Pt\u2013En dictionary, has the words job and work as possible En translations. In turn, these two En words can be translated to 4 and 5 Czech (Cs) words respectively. By utilizing the transitive property (which translation should exhibit) we can identify the set of 7 possible Cs translations for the Pt word trabalho.", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We propose extended middle context, a new context representation for CNNs for relation classification. The contexts are split into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context.  Hence, we propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. The two contexts are processed by two independent convolutional and max-pooling layers. After pooling, the results are concatenated to form the sentence representation. \n Question: How do they obtain the new context represetation?", "output": "They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation.", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Without ELMo (the same setting as 4-th row in Table 3 ), our data settings is the same as qin-EtAl:2017:Long whose performance was state-of-the-art and will be compared directly. We see that even without the pre-trained ELMo encoder, our performance is better, which is mostly attributed to our better sentence pair representations. \n Question: Why does their model do better than prior models?", "output": "better sentence pair representations", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information. Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer (cf. Figure FIGREF34 for an example) \n Question: Which information about typography is included in the corpus?", "output": "font type font style Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We compare our model with two sets of baselines:\n\nMemN2N BIBREF12 is an end-to-end trainable version of memory networks BIBREF9 . Attentive and Impatient Readers BIBREF6 use bidirectional LSTMs to encode question and evidence, and do classification over a large vocabulary based on these two encodings. \n Question: What are the baselines?", "output": "MemN2N BIBREF12 Attentive and Impatient Readers BIBREF6", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Our contribution leverages pre-trained word embeddings (GloVe, trained on wikipedia+gigaword corpus), the DepecheMood affective lexicon, and convolutional neural networks. \n Question: What embeddings do they use?", "output": "GloVe", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We first introduce a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state. This method freezes the policy used to reach the bottleneck and restarts the training from there on out, additionally conducting a backtracking search to ensure that a sub-optimal policy has not been frozen. The second contribution explore how to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-spaces such as Go-Explore BIBREF9.  \n Question: What are the two new strategies?", "output": "a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-space", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We then show the tweet text and image and we ask them to classify it in one of 6 categories: No attacks to any community, racist, sexist, homophobic, religion based attacks or attacks to other communities. \n Question: What annotations are available in the dataset - tweat used hate speach or not?", "output": "No attacks to any community  racist sexist homophobic religion based attacks attacks to other communities", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "First, it provides an annotation interface that allows users to define content elements, upload documents, and annotate documents Our platform is able to ingest documents in a variety of formats, including PDFs and Microsoft Word, and converts these formats into plain text before presenting them to the annotators. \n Question: What type of documents are supported by the annotation platform?", "output": "Variety of formats supported (PDF, Word...), user can define content elements of document", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We use the publicly available dataset KVRET BIBREF5 in our experiments.  This dataset includes dialogues in 3 domains: calendar, weather, navigation (POI) which is suitable for our mix-domain dialogue experiments.  \n Question: Which domains did they explored?", "output": "calendar weather navigation", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present.  \n Question: How do they collect the control corpus?", "output": "Randomly from Twitter", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In order to validate the proposed classification model in real crowdsourcing settings, Amazon Mechanical Turk (AMT) was used to obtain labels from multiple annotators for two popular datasets: Reuters-21578 BIBREF30 and LabelMe BIBREF31 . In order to first validate the proposed model for classification problems in a slightly more controlled environment, the well-known 20-Newsgroups benchmark corpus BIBREF29 was used by simulating multiple annotators with different levels of expertise.  \n Question: what datasets were used?", "output": "Reuters-21578 BIBREF30  LabelMe BIBREF31 20-Newsgroups benchmark corpus BIBREF29 ", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We compare FSDM with four baseline methods and two ablations.\n\nNDM BIBREF7 proposes a modular end-to-end trainable network. It applies de-lexicalization on user utterances and responses.\n\nLIDM BIBREF9 improves over NDM by employing a discrete latent variable to learn underlying dialogue acts. This allows the system to be refined by reinforcement learning.\n\nKVRN BIBREF13 adopts a copy-augmented Seq2Seq model for agent response generation and uses an attention mechanism on the KB. It does not perform belief state tracking.\n\nTSCP/RL BIBREF10 is a two-stage CopyNet which consists of one encoder and two copy-mechanism-augmented decoders for belief state and response generation. \n Question: What baselines have been used in this work?", "output": "NDM, LIDM, KVRN, and TSCP/RL", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": " In this section, we recommend a set of evaluation design changes that we believe are needed for assessing human\u2013machine parity, and will strengthen the human evaluation of MT in general.\n\nRecommendations ::: (R1) Choose professional translators as raters.\nIn our blind experiment (Section SECREF3), non-experts assess parity between human and machine translation where professional translators do not, indicating that the former neglect more subtle differences between different translation outputs.\n\nRecommendations ::: (R2) Evaluate documents, not sentences.\nWhen evaluating sentences in random order, professional translators judge machine translation more favourably as they cannot identify errors related to textual coherence and cohesion, such as different translations of the same product name. Our experiments show that using whole documents (i. e., full news articles) as unit of evaluation increases the rating gap between human and machine translation (Section SECREF4).\n\nRecommendations ::: (R3) Evaluate fluency in addition to adequacy.\nRaters who judge target language fluency without access to the source texts show a stronger preference for human translation than raters with access to the source texts (Sections SECREF4 and SECREF24). In all of our experiments, raters prefer human translation in terms of fluency while, just as in BIBREF3's BIBREF3 evaluation, they find no significant difference between human and machine translation in sentence-level adequacy (Tables TABREF21 and TABREF30). Our error analysis in Table TABREF34 also indicates that MT still lags behind human translation in fluency, specifically in grammaticality.\n\nRecommendations ::: (R4) Do not heavily edit reference translations for fluency.\nIn professional translation workflows, texts are typically revised with a focus on target language fluency after an initial translation step. As shown in our experiment in Section SECREF24, aggressive revision can make translations more fluent but less accurate, to the degree that they become indistinguishable from MT in terms of accuracy (Table TABREF30).\n\nRecommendations ::: (R5) Use original source texts.\nRaters show a significant preference for human over machine translations of texts that were originally written in the source language, but not for source texts that are translations themselves (Section SECREF35). Our results are further evidence that translated texts tend to be simpler than original texts, and in turn easier to translate with MT. \n Question: What recommendations do they offer?", "output": " Choose professional translators as raters  Evaluate documents, not sentences Evaluate fluency in addition to adequacy Do not heavily edit reference translations for fluency Use original source texts", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The improvement over all benchmark datasets also shows that conditional BERT is a general augmentation method for multi-labels sentence classification tasks. \n Question: Does the new objective perform better than the original objective bert is trained on?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Three incremental levels of document preprocessing are experimented with: raw text, text cleaning through document logical structure detection, and removal of keyphrase sparse sections of the document. In doing so, we present the first consistent comparison of different keyphrase extraction models and study their robustness over noisy text. \n Question: what levels of document preprocessing are looked at?", "output": "raw text text cleaning through document logical structure detection removal of keyphrase sparse sections of the document", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "A few sample advice rules in English (these are converted to first-order logic format and given as input to our algorithm) are presented in Table TABREF11 .  We modified the work of Odom et al. odomAIME15,odomAAAI15 to learn RDNs in the presence of advice. The key idea is to explicitly represent advice in calculating gradients. \n Question: How do they incorporate human advice?", "output": "by converting human advice to first-order logic format and use as an input to calculate gradient", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We conducted our experiments on the CMU ARCTIC database BIBREF33, which contains parallel recordings of professional US English speakers sampled at 16 kHz. One female (slt) was chosen as the target speaker and one male (bdl) and one female (clb) were chosen as sources. We selected 100 utterances each for validation and evaluation, and the other 932 utterances were used as training data. For the TTS corpus, we chose a US female English speaker (judy bieber) from the M-AILABS speech dataset BIBREF34 to train a single-speaker Transformer-TTS model. With the sampling rate also at 16 kHz, the training set contained 15,200 utterances, which were roughly 32 hours long. \n Question: What datasets are experimented with?", "output": "the CMU ARCTIC database BIBREF33  the M-AILABS speech dataset BIBREF34 ", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We use the publicly available dataset KVRET BIBREF5 in our experiments. This dataset includes dialogues in 3 domains: calendar, weather, navigation (POI) which is suitable for our mix-domain dialogue experiments.  \n Question: What multi-domain dataset is used?", "output": "KVRET", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "For the future work, we plan to solve the triples with multiple entities as the second entity, which is excluded from problem scope in this paper. The input of QA4IE is a document $D$ with an existing knowledge base $K$ and the output is a set of relation triples $R = \\lbrace e_i, r_{ij}, e_j\\rbrace $ in $D$ where $e_i$ and $e_j$ are two individual entities and $r_{ij}$ is their relation. \n Question: Can this approach model n-ary relations?", "output": "No", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In order to train and evaluate open-domain factoid QA system for real-world questions, we build a new Chinese QA dataset named as WebQA. \n Question: What languages do they experiment with?", "output": "Chinese", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In BIBREF8 a refined collection of tweets gathered from twitter is presented. Their dataset which is labeled for named entity recognition task contains 8,257 tweets. There are 12,784 entities in total in this dataset. Table TABREF19 shows statistics related to each named entity in training, development and test sets.\n\n \n Question: What datasets did they use?", "output": "BIBREF8 a refined collection of tweets gathered from twitter", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The two popular segmentation methods are morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5. After word segmentation, we additionally add an specific symbol behind each separated subword unit, which aims to assist the NMT model to identify the morpheme boundaries and capture the semantic information effectively.  We utilize the Zemberek with a morphological disambiguation tool to segment the Turkish words into morpheme units, and utilize the morphology analysis tool BIBREF12 to segment the Uyghur words into morpheme units.  \n Question: How does the word segmentation method work?", "output": "morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5 Zemberek BIBREF12", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In this paper, we use three encoders (NBOW, LSTM and attentive LSTM) to model the text descriptions. \n Question: What neural models are used to encode the text?", "output": "NBOW, LSTM, attentive LSTM", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Encoders with induced latent structures have been shown to benefit several tasks including document classification, natural language inference BIBREF12, BIBREF13, and machine translation BIBREF11.  \n Question: Is there any evidence that encoders with latent structures work well on other tasks?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The learning model for retrieval is trained by an oracle constructed using distant supervision. Using the answer labels in the training set, we can find appropriate articles that include the information requested in the question. First, the question and article are embedded into vector sequences, using the same method as the comprehension model. We do not use anonymization here, to retain simplicity. Otherwise, the anonymization procedure would have to be repeated several times for a potentially large collection of documents. These vector sequences are next fed to a Bi-GRU, to produce the outputs $v$ (for the question) and $H_c$ (for the document) similar to the previous section. \n Question: How can a neural model be used for a retrieval if the input is the entire Wikipedia?", "output": "Using the answer labels in the training set, we can find appropriate articles that include the information requested in the question.", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 ). After untying parameters in the softmax prediction layer, implicit discourse relation classification performance was improved across all four relations, meanwhile, the explicit discourse relation classification performance was also improved. Then we also created ensemble models by applying majority voting to combine results of ten runs. From table 5 , each ensemble model obtains performance improvements compared with single model. The full model achieves performance boosting of (51.84 - 48.82 = 3.02) and (94.17 - 93.21 = 0.96) in macro F1-scores for predicting implicit and explicit discourse relations respectively.  \n Question: What discourse relations does it work best/worst for?", "output": "explicit discourse relations", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": " Last, we evaluate our approaches in 9 commonly used text classification datasets. We evaluate our methods on several commonly used datasets whose themes range from sentiment, web-page, science to medical and healthcare. \n Question: What NLP tasks do they consider?", "output": "text classification for themes including sentiment, web-page, science, medical and healthcare", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Routines were developed to simulate input data based on the authors experience with real healthcare data. The reasons for this choice were twofold: One, healthcare data can be high in incidental complexity, requiring one-off code to handle unusual inputs, but not necessarily in such a way as to significantly alter the fundamental engineering choices in a semantic enrichment engine such as this one. Two, healthcare data is strictly regulated, and the process for obtaining access to healthcare data for research can be cumbersome and time-consuming.\n\nA simplified set of input data, in a variety of different formats that occur frequently in a healthcare setting, was used for simulation. \n Question: What type of simulations of real-time data feeds are used for validaton?", "output": "simplified set of input data, in a variety of different formats that occur frequently in a healthcare setting", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Among all three pre-training tasks, SR works slightly better than the other two tasks (i.e., NSG and MDG). \n Question: Which of the three pretraining tasks is the most helpful?", "output": "SR", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In this section, we consider 3 categories of data augmentation techniques that are effectively applicable to video datasets. To further increase training data size and diversity, we can create new audios via superimposing each original audio with additional noisy audios in time domain. To obtain diverse noisy audios, we use AudioSet, which consists of 632 audio event classes and a collection of over 2 million manually-annotated 10-second sound clips from YouTube videos BIBREF28. We consider applying the frequency and time masking techniques \u2013 which are shown to greatly improve the performance of end-to-end ASR models BIBREF23 \u2013 to our hybrid systems. Similarly, they can be applied online during each epoch of LF-MMI training, without the need for realignment.\n\nConsider each utterance (i.e. after the audio segmentation in Section SECREF5), and we compute its log mel spectrogram with $\\nu $ dimension and $\\tau $ time steps:\n\nFrequency masking is applied $m_F$ times, and each time the frequency bands $[f_0$, $f_0+ f)$ are masked, where $f$ is sampled from $[0, F]$ and $f_0$ is sampled from $[0, \\nu - f)$.\n\nTime masking is optionally applied $m_T$ times, and each time the time steps $[t_0$, $t_0+ t)$ are masked, where $t$ is sampled from $[0, T]$ and $t_0$ is sampled from $[0, \\tau - t)$. Data augmentation ::: Speed and volume perturbation\nBoth speed and volume perturbation emulate mean shifts in spectrum BIBREF18, BIBREF19. To perform speed perturbation of the training data, we produce three versions of each audio with speed factors $0.9$, $1.0$, and $1.1$. The training data size is thus tripled. For volume perturbation, each audio is scaled with a random variable drawn from a uniform distribution $[0.125, 2]$. \n Question: What are the best within-language data augmentation methods?", "output": "Frequency masking Time masking Additive noise Speed and volume perturbation", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments.  \n Question: What are results of comparison between novel method to other approaches for creating compositional generalization benchmarks?", "output": "The MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Given a news Twitter account, we read its tweets from the account's timeline. Then we sort the tweets by the posting date in ascending way and we split them into $N$ chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account. Consequently, we investigate ways to detect suspicious accounts by considering their tweets in groups (chunks). Our hypothesis is that suspicious accounts have a unique pattern in posting tweet sequences. Since their intention is to mislead, the way they transition from one set of tweets to the next has a hidden signature, biased by their intentions. Therefore, reading these tweets in chunks has the potential to improve the detection of the fake news accounts. \n Question: How are chunks defined?", "output": "Chunks is group of tweets from single account that  is consecutive in time - idea is that this group can show secret intention of malicious accounts.", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We collected Japanese-Vietnamese parallel data from TED talks extracted from WIT3's corpus BIBREF15 .  \n Question: did they collect their own data?", "output": "No", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The model has around 836M parameters, of which only 66K are byte embeddings. \n Question: How many parameters does the model have?", "output": "model has around 836M parameters", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We empirically test our approach on a series of experiments on WikiTableQuestions, to our knowledge the only dataset designed for this task. In Algorithm 1 we describe how logical forms are transformed into interpretable textual representations called \"paraphrases\". \n Question: What is the source of the paraphrases of the questions?", "output": "WikiTableQuestions", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Similar trends are seen in the performance of other two state-of-the-art approaches BIBREF9 , BIBREF8 . \n Question: What are the state of the art models?", "output": "BIBREF9  BIBREF8 ", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The Title-to-Story system is a baseline, which generates directly from topic. \n Question: What are the baselines?", "output": "Title-to-Story system", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In both cases, we denote with the word model one of the possible combinations of classic/statistical LSA and a classifier. The used classifiers are Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF) and gradient boosting (XGB). Where applicable, we compare our results with existing results in the literature. \n Question: What are the different methods used for different corpora?", "output": "Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF) and gradient boosting (XGB)", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "One way to analyze the model is to compute model gradients with respect to input features BIBREF26, BIBREF25. Figure FIGREF37 shows that in this particular example, the most important model inputs are verbs possibly associated with the entity butter, in addition to the entity's mentions themselves. It further shows that the model learns to extract shallow clues of identifying actions exerted upon only the entity being tracked, regardless of other entities, by leveraging verb semantics. \n Question: What evidence do they present that the model attends to shallow context clues?", "output": "Using model gradients with respect to input features they presented that the most important model inputs are verbs associated with entities which shows that the model attends to shallow context clues", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We observe $R^2$ correlation coefficients between 0.11 and 0.22 for the input and output length ratios and between 0.81 and 0.88 for the compound divergence.  \n Question: How strong is negative correlation between compound divergence and accuracy in performed experiment?", "output": " between 0.81 and 0.88", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "With the goal of building a generalizable sentiment analysis model, we used three different training sets as provided in Table TABREF5 . One of these three datasets (Amazon reviews BIBREF23 , BIBREF24 ) is larger and has product reviews from several different categories including book reviews, electronics products reviews, and application reviews. The other two datasets are to make the model more specialized in the domain. In this paper we focus on restaurant reviews as our domain and use Yelp restaurant reviews dataset extracted from Yelp Dataset Challenge BIBREF25 and restaurant reviews dataset as part of a Kaggle competition BIBREF26 . \n Question: what dataset was used for training?", "output": "Amazon reviews Yelp restaurant reviews restaurant reviews", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "A prototypical implementation, in which the words are assumed to be in the fundamental representation of the special orthogonal group, INLINEFORM0 , and are conditioned on losses sensitive to the relative actions of words, is the subject of another manuscript presently in preparation. \n Question: Is there a formal proof that the RNNs form a representation of the group?", "output": "No", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Overall, we observe a positive correlation between retweeting and co-voting, which is significantly different from zero. The strongest positive correlations are in the areas Area of freedom, security and justice, External relations of the Union, and Internal markets. Weaker, but still positive, correlations are observed in the areas Economic, social and territorial cohesion, European citizenship, and State and evolution of the Union. The only exception, with a significantly negative coefficient, is the area Economic and monetary system. This implies that in the area Economic and monetary system we observe a significant deviation from the usual co-voting patterns. \n Question: What is the relationship between the co-voting and retweeting patterns?", "output": "we observe a positive correlation between retweeting and co-voting strongest positive correlations are in the areas Area of freedom, security and justice, External relations of the Union, and Internal markets Weaker, but still positive, correlations are observed in the areas Economic, social and territorial cohesion, European citizenship, and State and evolution of the Union significantly negative coefficient, is the area Economic and monetary system", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We quantify the efficiency-accuracy tradeoff compared to two rule-based baselines: Unif and Stopword.  \n Question: What are the baselines used?", "output": "Unif and Stopword", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "BERT: We implement two BERT-based baselines BIBREF51 for evidence identification. First, we train BERT on each query-policy sentence pair as a binary classification task to identify if the sentence is evidence for the question or not (Bert). We also experiment with a two-stage classifier, where we separately train the model on questions only to predict answerability. At inference time, if the answerable classifier predicts the question is answerable, the evidence identification classifier produces a set of candidate sentences (Bert + Unanswerable).\n\n \n Question: What type of neural model was used?", "output": "Bert + Unanswerable", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "UTD aims to automatically identify and cluster repeated terms (e.g. words or phrases) from speech. To circumvent the exhaustive DTW-based search limited by INLINEFORM0 time BIBREF6 , we exploit the scalable UTD framework in the Zero Resource Toolkit (ZRTools) BIBREF7 , which permits search in INLINEFORM1 time. \n Question: How is the vocabulary of word-like or phoneme-like units automatically discovered?", "output": "Zero Resource Toolkit (ZRTools) BIBREF7", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We then propose a group of new interesting tasks, such as analogy query and analogy browsing, and discuss how they can be used in modern digital libraries. \n Question: What new interesting tasks can be solved based on the uncanny semantic structures of the embedding space?", "output": " analogy query analogy browsing", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We compare our model with other strong competitive methods on the SQuAD leaderboard and TriviaQA leaderboard. \n Question: What other solutions do they compare to?", "output": " strong competitive methods on the SQuAD leaderboard and TriviaQA leaderboard", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Our official scores (column Ens Test in Table TABREF19 ) have placed us second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard. \n Question: What were the scores of their system?", "output": "column Ens Test in Table TABREF19", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The incomplete dataset used for training is composed of lower-cased incomplete data obtained by manipulating the original corpora. \n Question: Do they test their approach on a dataset without incomplete data?", "output": "No", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The greatest challenges with the competition arise directly from the potential for ongoing mixed-initiative multi-turn dialogues, which do not follow a particular plan or pursue a particular fixed information need.  This paper describes some of the lessons we learned building SlugBot for the 2017 Alexa Prize, particularly focusing on the challenges of integrating content found via search with content from structured data in order to carry on an ongoing, coherent, open-domain, mixed-initiative conversation More challenging is that at each system turn, there are a large number of conversational moves that are possible.  Finally, most other domains do not have such high quality structured data available, leaving us to develop or try to rely on more general models of discourse coherence.  Search cannot be used effectively here without constructing an appropriate query, or knowing in advance where plot information might be available. In a real-time system, live search may not be able to achieve the required speed and efficiency, so preprocessing or caching of relevant information may be necessary.  \n Question: Why mixed initiative multi-turn dialogs are the greatest challenge in building open-domain conversational agents?", "output": "do not follow a particular plan or pursue a particular fixed information need  integrating content found via search with content from structured data at each system turn, there are a large number of conversational moves that are possible most other domains do not have such high quality structured data available live search may not be able to achieve the required speed and efficiency", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Ternary and fine-grained sentiment classification were part of the SemEval-2016 \u201cSentiment Analysis in Twitter\u201d task BIBREF16 . We use the high-quality datasets the challenge organizers released. \n Question: What dataset did they use?", "output": " high-quality datasets  from SemEval-2016 \u201cSentiment Analysis in Twitter\u201d task", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Trinomials are likely to appear in exactly one order, and when they appear in more than one order the last word is almost always the same across all instances.  \n Question: Are there any new finding in analasys of trinomials that was not present binomials?", "output": "Trinomials are likely to appear in exactly one order", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The release of the FEVER fact extraction and verification dataset BIBREF0 provides a large-scale challenge that tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem. Systems are evaluated on the accuracy of the claim predictions, with credit only given when correct evidence is submitted. As entailment data, premises in FEVER data differ substantially from those in the image caption data used as the basis for the Stanford Natural Language Inference (SNLI) BIBREF1 dataset. Sentences are longer (31 compared to 14 words on average), vocabulary is more abstract, and the prevalence of named entities and out-of-vocabulary terms is higher. The retrieval aspect of FEVER is not straightforward either. A claim may have small word overlap with the relevant evidence, especially if the claim is refuted by the evidence. \n Question: What is the FEVER task?", "output": "tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We compare our approach to two other, the first one uses almost the same tweet data as we use for training, and the second one is the CrowdFlower dataset annotated for emotions.\n\nIn the first one Wang et al. BIBREF21 downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise.  In the second one, the reported results are from a paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets. Here we only report part of their result for CrowdFlower dataset that can be mapped to one of our seven labels. \n Question: What baseline is used?", "output": " Wang et al. BIBREF21 paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "To better exploit such existing data sources, we propose an end-to-end (E2E) model based on pointer networks with attention, which can be trained end-to-end on the input/output pairs of human IE tasks, without requiring token-level annotations. Since our model does not need token-level labels, we create an E2E version of each data set without token-level labels by chunking the BIO-labeled words and using the labels as fields to extract. \n Question: Do they assume sentence-level supervision?", "output": "No", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "WMT14 En-Fr and En-De datasets The WMT 2014 English-French translation dataset, consisting of $36M$ sentence pairs, is adopted as a big dataset to test our model. For medium dataset, we borrow the setup of BIBREF0 and adopt the WMT 2014 English-German translation dataset which consists of $4.5M$ sentence pairs, the BPE vocabulary size is set to $32K$. IWSLT De-En and En-Vi datasets Besides, we perform experiments on two small IWSLT datasets to test the small version of MUSE with other comparable models. The IWSLT 2014 German-English translation dataset consists of $160k$ sentence pairs. We also adopt a joint source and target BPE factorization with the vocabulary size of $32K$. The IWSLT 2015 English-Vietnamese translation dataset consists of $133K$ training sentence pairs. \n Question: What datasets are used?", "output": "WMT14 En-Fr and En-De datasets IWSLT De-En and En-Vi datasets", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The experimental results are shown in Table 1, where it can be seen that autoencoders outperformed MLP and CNN outperformed autoencoders with the highest achieved accuracy of 82.6%. \n Question: What was their performance on the dataset?", "output": "accuracy of 82.6%", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Table 2 lists the accuracies of the all methods on two classifier architectures. The results show that, for various datasets on different classifier architectures, our conditional BERT contextual augmentation improves the model performances most. BERT can also augments sentences to some extent, but not as much as conditional BERT does. \n Question: Do the authors report performance of conditional bert on tasks without data augmentation?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Task 1: Quora Duplicate Question Pair Detection Task 2: Ranking questions in Bing's People Also Ask \n Question: On which tasks do they test their conflict method?", "output": "Task 1: Quora Duplicate Question Pair Detection Task 2: Ranking questions", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "If we re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones, assisting anti-trafficking experts in expanding the lexicon of trafficking flags. This approach also works for acronyms and deliberate typos. \n Question: How is the lexicon of trafficking flags expanded?", "output": "re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The speech content was obtained from the closed captions (CC) provided by the YouTube ASR system which can be though as subtitles. \n Question: What ASR system do they use?", "output": "YouTube ASR system ", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We used the Levenshtein distance metric BIBREF8 implemented in Apache Lucene library BIBREF9 . Another simple approach is the aforementioned diacritical swapping, which is a term that we introduce here for referring to a solution inspired by the work of BIBREF4 . A promising method, adapted from work on correcting texts by English language learners BIBREF11 , expands on the concept of selecting a correction nearest to the spelling error according to some notion of distance. Here, the Levenshtein distance is used in a weighted sum to cosine distance between word vectors. Our ELMo-augmented LSTM is bidirectional. \n Question: What methods are tested in PIEWi?", "output": "Levenshtein distance metric BIBREF8 diacritical swapping Levenshtein distance is used in a weighted sum to cosine distance between word vectors ELMo-augmented LSTM", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "As we can see that, all variants of our CRU model could give substantial improvements over the traditional GRU model, where a maximum gain of 2.7%, 1.0%, and 1.9% can be observed in three datasets, respectively. \n Question: Do experiment results show consistent significant improvement of new approach over traditional CNN and RNN models?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Using a set of semantically oriented tasks that require explicit semantic cross-lingual representations, we showed that mBERT contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks. \n Question: What challenges this work presents that must be solved to build better language-neutral representations?", "output": "contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Since the present study focuses on Spanish tweets, all tweets from the English datasets were translated into Spanish.  \n Question: What other languages did they translate the data from?", "output": "English ", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": " To remain consistent with experiments performed with LSTM's we use the morfessor for the subword tokenization in the Finnish Language. \n Question: Is the LSTM baseline a sub-word model?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": ". It is evident from Table TABREF17 that GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset. \n Question: How does this approach compare to other WSD approaches employing word embeddings?", "output": "GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset.", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We experiment with two image captioning models: the Show&Tell model BIBREF0 and the LRCN1u model BIBREF1. Both models follow the basic encoder-decoder architecture design that uses a CNN encoder to condense the visual information into an image embedding, which in turn conditions an LSTM decoder to generate a natural language caption. The main difference between the two models is the way they condition the decoder. The Show&Tell model feeds the image embedding as the \u201cpredecessor word embedding\u201d to the first produced word, while the LRCN1u model concatenates the image features with the embedded previous word as the input to the sequence model at each time step. \n Question: Which existing models are evaluated?", "output": "Show&Tell and LRCN1u", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The initial analyses of the embedding matrices for both the UK and France revealed that in general, words were grouped by context or influence on the electricity consumption. For instance, we observed that winter words were together and far away from summer ones.  \n Question: Is there any example where geometric property is visible for context similarity between words?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Arab-Tweet. For modeling age and gender, we use Arap-Tweet BIBREF19 , which we will refer to as Arab-Tweet. UBC Twitter Gender Dataset. We also develop an in-house Twitter dataset for gender. We manually labeled 1,989 users from each of the 21 Arab countries. The dialect identification model in AraNet is based on our winning system in the MADAR shared task 2 BIBREF20 as described in BIBREF12. The corpus is divided into train, dev and test, and the organizers masked test set labels. We make use of two datasets, the LAMA-DINA dataset from BIBREF22, a Twitter dataset with a combination of gold labels from BIBREF23 and distant supervision labels. The rest of the dataset is only labeled using distant supervision (LAMA-DIST) ($182,605$ tweets) . For more information about the dataset, readers are referred to BIBREF22.  We use the dataset for irony identification on Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24. We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34. Table TABREF28 shows all the corpora we use.  \n Question: What datasets are used in training?", "output": "Arap-Tweet BIBREF19  an in-house Twitter dataset for gender the MADAR shared task 2 BIBREF20 the LAMA-DINA dataset from BIBREF22 LAMA-DIST Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24 BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The median age is 17 for depressed class versus 19 for control class suggesting either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age for connecting to their peers (social homophily.) Our findings are consistent with the medical literature BIBREF10 as according to BIBREF52 more women than men were given a diagnosis of depression. \n Question: What insights into the relationship between demographics and mental health are provided?", "output": "either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age more women than men were given a diagnosis of depression", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Architecture-based methods either try to induce task-specific architecture during pre-training (task-specific methods), or aim at building a general pre-training architecture to fit all downstream tasks (task-agnostic methods) \n Question: How architecture-based method handle obstacles in NLG?", "output": "task-specific architecture during pre-training (task-specific methods) aim at building a general pre-training architecture to fit all downstream tasks (task-agnostic methods)", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Our video question answering task is novel and to our knowledge, no model has been designed specifically for this task. As a first step towards solving this problem, we evaluated the performance of state-of-the-art models developed for other QA tasks, including a sentence-level prediction task and two segment retrieval tasks. Baselines ::: Baseline1: Sentence-level prediction\nGiven a transcript (a sequence of sentences) and a question, Baseline1 predicts (starting sentence index, ending sentence index). The model is based on RaSor BIBREF13, which has been developed for the SQuAD QA task BIBREF6. RaSor concatenates the embedding vectors of the starting and the ending words to represent a span. Following this idea, Baseline1 represents a span of sentences by concatenating the vectors of the starting and ending sentences. The left diagram in Fig. FIGREF15 illustrates the Baseline1 model.\n\nModel. The model takes two inputs, a transcript, $\\lbrace s_1, s_2, ... s_n\\rbrace $ where $s_i$ are individual sentences and a question, $q$. The output is the span scores, $y$, the scores over all possible spans. GLoVe BIBREF14 is used for the word representations in the transcript and the questions. We use two bi-LSTMs BIBREF15 to encode the transcript.\n\nwhere n is the number of sentences . The output of Passage-level Encoding, $p$, is a sequence of vector, $p_i$, which represents the latent meaning of each sentence. Then, the model combines each pair of sentence embeddings ($p_i$, $p_j$) to generate a span embedding.\n\nwhere [$\\cdot $,$\\cdot $] indicates the concatenation. Finally, we use a one-layer feed forward network to compute a score between each span and a question.\n\nIn training, we use cross-entropy as an objective function. In testing, the span with the highest score is picked as an answer. Baselines ::: Baseline2: Segment retrieval\nWe also considered a simpler task by casting our problem as a retrieval task. Specifically, in addition to a plain transcript, we also provided the model with the segmentation information which was created during the data collection phrase (See Section. SECREF3). Note that each segments corresponds to a candidate answer. Then, the task is to pick the best segment for given a query. This task is easier than Baseline1's task in that the segmentation information is provided to the model. Unlike Baseline1, however, it is unable to return an answer span at various granularities. Baseline2 is based on the attentive LSTM BIBREF17, which has been developed for the InsuranceQA task. The right diagram in Fig. FIGREF15 illustrates the Baseline2 model.\n\nModel. The two inputs, $s$ and $q$ represent the segment text and a question. The model first encodes the two inputs.\n\n$h^s$ is then re-weighted using attention weights.\n\nwhere $\\odot $ denotes the element-wise multiplication operation. The final score is computed using a one-layer feed-forward network.\n\nDuring training, the model requires negative samples. For each positive example, (question, ground-truth segment), all the other segments in the same transcript are used as negative samples. Cross entropy is used as an objective function.\n\n Baselines ::: Baseline3: Pipeline Segment retrieval\nWe construct a pipelined approach through another segment retrieval task, calculating the cosine similarities between the segment and question embeddings. In this task however, we want to test the accuracy of retrieving the segments given that we first retrieve the correct video from our 76 videos. First, we generate the TF-IDF embeddings for the whole video transcripts and questions. The next step involves retrieving the videos which have the lowest cosine distance between the video transcripts and question. We then filter and store the top ten videos, reducing the number of computations required in the next step. Finally, we calculate the cosine distances between the question and the segments which belong to the filtered top 10 videos, marking it as correct if found in these videos. While the task is less computationally expensive than the previous baseline, we do not learn the segment representations, as this task is a simple retrieval task based on TF-IDF embeddings.\n\nModel. The first two inputs are are the question, q, and video transcript, v, encoded by their TF-IDF vectors: BIBREF18:\n\nWe then filter the top 10 video transcripts(out of 76) with the minimum cosine distance, and further compute the TF-IDF vectors for their segments, Stop10n, where n = 10. We repeat the process for the corresponding segments:\n\nselecting the segment with the minimal cosine distance distance to the query. \n Question: What baseline algorithms were presented?", "output": "a sentence-level prediction algorithm, a segment retrieval algorithm and a pipeline segment retrieval algorithm", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We built 16 models of word embeddings using the implementation of CBOW and Skip-gram methods in the FastText tool BIBREF9 . \n Question: What embedding algorithm is used to build the embeddings?", "output": "CBOW and Skip-gram methods in the FastText tool BIBREF9", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Our training data consists of 2.09M sentence pairs extracted from LDC corpus. To test different approaches on Chinese-to-English translation task, we use NIST 2003(MT03) dataset as the validation set, and NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) datasets as our test sets. For English-to-Chinese translation task, we also use NIST 2003(MT03) dataset as the validation set, and NIST 2008(MT08) will be used as test set. \n Question: What dataset did they use?", "output": "LDC corpus NIST 2003(MT03) NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) NIST 2008(MT08)", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "For our single-label topic classification experiments, we use the Switchboard Telephone Speech Corpus BIBREF21 , a collection of two-sided telephone conversations. We further evaluate our topic ID performance on the speech corpora of three languages released by the DARPA LORELEI (Low Resource Languages for Emergent Incidents) Program. \n Question: What datasets are used to assess the performance of the system?", "output": "Switchboard Telephone Speech Corpus BIBREF21 LORELEI (Low Resource Languages for Emergent Incidents) Program", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We also run a manual evaluation which shows for the En-It task a slight quality degradation in exchange of a statistically significant reduction in the average length ratio, from 1.05 to 1.01. \n Question: Do they conduct any human evaluation?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": " In terms of this, rather than starting from Twitter API Search, we look into the archived snapshots of two major news websites (CNN, NBC), and then extract the tweet blocks that are embedded in the news articles. \n Question: How do they determine if tweets have been used by journalists?", "output": " we look into the archived snapshots of two major news websites (CNN, NBC), and then extract the tweet blocks that are embedded in the news articles", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Unsupervised Evaluation\nThe unsupervised tasks include five tasks from SemEval Semantic Textual Similarity (STS) in 2012-2016 BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 and the SemEval2014 Semantic Relatedness task (SICK-R) BIBREF35 .\n\nThe cosine similarity between vector representations of two sentences determines the textual similarity of two sentences, and the performance is reported in Pearson's correlation score between human-annotated labels and the model predictions on each dataset.\n\nSupervised Evaluation\nIt includes Semantic relatedness (SICK) BIBREF35 , SemEval (STS-B) BIBREF36 , paraphrase detection (MRPC) BIBREF37 , question-type classification (TREC) BIBREF38 , movie review sentiment (MR) BIBREF39 , Stanford Sentiment Treebank (SST) BIBREF40 , customer product reviews (CR) BIBREF41 , subjectivity/objectivity classification (SUBJ) BIBREF42 , opinion polarity (MPQA) BIBREF43 . \n Question: How do they evaluate the sentence representations?", "output": "The unsupervised tasks include five tasks from SemEval Semantic Textual Similarity (STS) in 2012-2016 BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 and the SemEval2014 Semantic Relatedness task (SICK-R) BIBREF35 .\n\nThe cosine similarity between vector representations of two sentences determines the textual similarity of two sentences, and the performance is reported in Pearson's correlation score between human-annotated labels and the model predictions on each dataset. Supervised Evaluation\nIt includes Semantic relatedness (SICK) BIBREF35 , SemEval (STS-B) BIBREF36 , paraphrase detection (MRPC) BIBREF37 , question-type classification (TREC) BIBREF38 , movie review sentiment (MR) BIBREF39 , Stanford Sentiment Treebank (SST) BIBREF40 , customer product reviews (CR) BIBREF41 , subjectivity/objectivity classification (SUBJ) BIBREF42 , opinion polarity (MPQA) BIBREF43 .", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "MATT-BiE-LSTM and WATT-BiE-LSTM obtain similar performances when tested on both Vader and human annotated samples, though their ways of computing the attention (weights and vectors) are different that WATT computes attention weights and the senti-emoji embeddings guided by each word, and MATT obtains the senti-emoji embedding based on the LSTM encoder on the whole contexts and computes the attention weights of the senti-emoji embedding across all words. Both models outperforms the state-of-the-art baseline models including ATT-E-LSTM Attention-based LSTM with emojis:We also use the word-emoji embedding to calculate the emoji-word attention following Equation EQREF20 and EQREF21 , and the only difference is that we replace the attention-derived senti-emoji embedding with the pre-trained word-emoji embedding by fasttext, denoted as ATT-E-LSTM. LSTM with bi-sense emoji embedding (proposed): As we have introduced in Section SECREF13 , we propose two attention-based LSTM networks based on bi-sense emoji embedding, denoted as MATT-BiE-LSTM and WATT-BiE-LSTM. \n Question: Which SOTA models are outperformed?", "output": "Attention-based LSTM with emojis", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The real-time tweets scores were calculated in the same way as the historical data and summed up for a minute and sent to the machine learning model with the Bitcoin price in the previous minute and the rolling average price. It predicted the next minute's Bitcoin price from the given data. After the actual price arrived, the RMS value was calculated and the machine learning model updated itself to predict with better understanding the next value. \n Question: What experimental evaluation is used?", "output": "root mean square error between the actual and the predicted price of Bitcoin for every minute", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "As this corpus of annotated patient notes comprises original healthcare data which contains protected health information (PHI) per The Health Information Portability and Accountability Act of 1996 (HIPAA) BIBREF16 and can be joined to the MIMIC-III database, individuals who wish to access to the data must satisfy all requirements to access the data contained within MIMIC-III. To satisfy these conditions, an individual who wishes to access the database must take a \u201cData or Specimens Management\u201d course, as well as sign a user agreement, as outlined on the MIMIC-III database webpage, where the latest version of this database will be hosted as \u201cAnnotated Clinical Texts from MIMIC\u201d BIBREF17. This corpus can also be accessed on GitHub after completing all of the above requirements. \n Question: Is this dataset publicly available for commercial use?", "output": "No", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The reports are published by M\u00e9t\u00e9o France and the Met Office, its British counterpart. They are publicly available on the respective websites of the organizations. Both corpora span on the same period as the corresponding time series and given their daily nature, it yields a total of 4,261 and 4,748 documents respectively. \n Question: How big is dataset used for training/testing?", "output": "4,261  days for France and 4,748 for the UK", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation. Their model solves the subnet waste issue by concatenating an ST decoder to an ASR encoder-decoder model. Notably, their ST decoder can consume representations from the speech encoder as well as the ASR decoder. For a fair comparison, the speech encoder and the ASR decoder are initialized from the pre-trained ASR model. The Triangle model is fine-tuned under their multi-task manner. \n Question: What are the baselines?", "output": "Vanilla ST baseline encoder pre-training, in which the ST encoder is initialized from an ASR model decoder pre-training, in which the ST decoder is initialized from an MT model encoder-decoder pre-training, where both the encoder and decoder are pre-trained many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "An acoustic word embedding is a function that takes as input a speech segment corresponding to a word, INLINEFORM0 , where each INLINEFORM1 is a vector of frame-level acoustic features, and outputs a fixed-dimensional vector representing the segment, INLINEFORM2 . \n Question: How do they represent input features of their model to train embeddings?", "output": "a vector of frame-level acoustic features", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The study of the mathematical structure of grammar has indicated that the fundamental things making up sentences are not the words, but some atomic grammatical types, such as the noun-type and the sentence-type BIBREF23 , BIBREF24 , BIBREF25 . The transitive verb-type is not an atomic grammatical type, but a composite made up of two noun-types and one sentence-type. Hence, particularly interesting here is that atomic doesn't really mean smallest...\n\nOn the other hand, just like in particle physics where we have particles and anti-particles, the atomic types include types as well as anti-types. But unlike in particle physics, there are two kinds of anti-types, namely left ones and right ones. This makes language even more non-commutative than quantum theory! \n Question: Do they break down word meanings into elementary particles as in the standard model of quantum theory?", "output": "No", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "All the datasets consist of videos where only one speaker is in front of the camera. The descriptors we used for each of the modalities are as follows: Language All the datasets provide manual transcriptions. Vision Facet BIBREF26 is used to extract a set of features including per-frame basic and advanced emotions and facial action units as indicators of facial muscle movement. Acoustic We use COVAREP BIBREF27 to extract low level acoustic features including 12 Mel-frequency cepstral coefficients (MFCCs), pitch tracking and voiced/unvoiced segmenting features, glottal source parameters, peak slope parameters and maxima dispersion quotients. \n Question: What modalities are being used in different datasets?", "output": "Language Vision Acoustic", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We also applied our model to MCTest dataset which requires machines to answer multiple-choice reading comprehension questions about fictional stories.  \n Question: Do they experiment with their proposed model on any other dataset other than MovieQA?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "To conclude this section, our model reliable corrects grammatical, spelling and word order errors on , with more mixed performance on lexical choice errors and some unnecessary paraphrasing of the input. \n Question: What error types is their model more reliable for?", "output": "grammatical, spelling and word order errors", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work. \n Question: Is their method capable of multi-hop reasoning?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Of the 144 schemas in the collection at WinogradSchemas there are 33 that can plausibly be translated this way. \n Question: Did they collect their own datasets?", "output": "No", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The results show that NCEL consistently outperforms various baselines with a favorable generalization ability. \n Question: How effective is their NCEL approach overall?", "output": "NCEL consistently outperforms various baselines with a favorable generalization ability", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The accuracy of our model is 7.8% higher than the best result achieved by LSVM. The results show that this model can perform better than state-of-the-art baselines including hybrid CNN BIBREF15 and LSTM with attention BIBREF16 by 3.1% on the validation set and 1% on the test set. \n Question: What are state of the art methods authors compare their work with? ", "output": "ISOT dataset: LLVM\nLiar dataset: Hybrid CNN and LSTM with attention", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Noticeably, despite being both based on and integrated into a BERT$_\\text{base}$ model, our architecture even outperforms a standalone BERT$_\\text{large}$ model by a large margin. \n Question: What models other than standalone BERT is new model compared to?", "output": "Only Bert base and Bert large are compared to proposed approach.", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Our analysis found that gang members have a penchant for using just a small set of emoji symbols that convey their anger and violent behavior through their tweets. Figure FIGREF24 illustrates the emoji distribution for the top 20 most frequent emojis used by gang member profiles in our dataset. The fuel pump emoji was the most frequently used emoji by the gang members, which is often used in the context of selling or consuming marijuana. The pistol emoji is the second most frequent in our dataset, which is often used with the guardsman emoji or the police cop emoji in an `emoji chain'. Figure FIGREF28 presents some prototypical `chaining' of emojis used by gang members. The chains may reflect their anger at law enforcement officers, as a cop emoji is often followed up with the emoji of a weapon, bomb, or explosion. We found that 32.25% of gang members in our dataset have chained together the police and the pistol emoji, compared to just 1.14% of non-gang members. Moreover, only 1.71% of non-gang members have used the hundred points emoji and pistol emoji together in tweets while 53% of gang members have used them. A variety of the angry face emoji such as devil face emoji and imp emoji were also common in gang member tweets. \n Question: What are the differences in the use of emojis between gang member and the rest of the Twitter population?", "output": "32.25% of gang members in our dataset have chained together the police and the pistol emoji, compared to just 1.14% of non-gang members only 1.71% of non-gang members have used the hundred points emoji and pistol emoji together in tweets while 53% of gang members have used them gang members have a penchant for using just a small set of emoji symbols that convey their anger and violent behavior", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We propose the following challenge to the community: We must develop formal definition and evaluation for faithfulness that allows us the freedom to say when a method is sufficiently faithful to be useful in practice.\n\nWe note two possible approaches to this end:\n\nAcross models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. Perhaps some models or tasks allow sufficiently faithful interpretation, even if the same is not true for others.\n\nFor example, the method may not be faithful for some question-answering task, but faithful for movie review sentiment, perhaps based on various syntactic and semantic attributes of those tasks.\n\nAcross input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves. If we are able to say with some degree of confidence whether a specific decision's explanation is faithful to the model, even if the interpretation method is not considered universally faithful, it can be used with respect to those specific areas or instances only. \n Question: What faithfulness criteria does they propose?", "output": "Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks. Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves.", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In this context, we decided to study the three main types of architectures which have demonstrated promising results over traditional systems: 1) Connectionist Temporal Classification (CTC) BIBREF5, BIBREF6 which uses Markov assumptions (i.e. conditional independence between predictions at each time step) to efficiently solve sequential problems by dynamic programming, 2) Attention-based methods BIBREF7, BIBREF8 which rely on an attention mechanism to perform non-monotonic alignment between acoustic frames and recognized acoustic units and 3) RNN-tranducer BIBREF0, BIBREF9, BIBREF10 which extends CTC by additionally modeling the dependencies between outputs at different steps using a prediction network analogous to a language model. \n Question: What are the existing end-to-end ASR approaches for the French language?", "output": "1) Connectionist Temporal Classification (CTC) 2) Attention-based methods 3) RNN-tranducer", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We apply the same data split in BIBREF31, BIBREF30, BIBREF6, which uses news (the union of bn and nw) as the training set, a half of bc as the development set and the remaining data as the test set.\n\nWe learn the model parameters using Adam BIBREF32. We apply dropout BIBREF33 to the hidden layers to reduce overfitting. The development set is used for tuning the model hyperparameters and for early stopping. We train 5 Bi-LSTM English RE models initiated with different random seeds, apply the 5 models on the target languages, and combine the outputs by selecting the relation type labels with the highest probabilities among the 5 models. \n Question: Do they train their own RE model?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "To further study the information encoded in the discourse embeddings, we perform t-SNE clustering BIBREF20 on them, using the best performing model CNN2-DE (global). \n Question: How are discourse embeddings analyzed?", "output": "They perform t-SNE clustering to analyze discourse embeddings", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Recently bhat-EtAl:2017:EACLshort provided a CS dataset for the evaluation of their parsing models which they trained on the Hindi and English Universal Dependency (UD) treebanks. We extend this dataset by annotating 1,448 more sentences. \n Question: How big is the provided treebank?", "output": "1448 sentences more than the dataset from Bhat et al., 2017", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We map each relation type $R(x,y)$ to at least one parametrized natural-language question $q_x$ whose answer is $y$ . \n Question: How is the input triple translated to a slot-filling task?", "output": "The relation R(x,y) is mapped onto a question q whose answer is y", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": " Since we are interested in the zero-shot capabilities of our representation, we trained our sentiment analysis model only on the english IMDB Large Movie Review dataset and tested it on the chinese ChnSentiCorp dataset and german SB-10K BIBREF24 , BIBREF25 . A natural language inference task consists of two sentences; a premise and a hypothesis which are either contradictions, entailments or neutral. Learning a NLI task takes a certain nuanced understanding of language. Therefore it is of interest whether or not UG-WGAN captures the necessary linguistic features.  \n Question: Did they experiment with tasks other than word problems in math?", "output": "They experimented with sentiment analysis and natural language inference task", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The English online magazine of ISIS was named Dabiq and first appeared on the dark web on July 2014 and continued publishing for 15 issues. This publication was followed by Rumiyah which produced 13 English language issues through September 2017. Looking through both Dabiq and Rumiyah, many issues of the magazines contain articles specifically addressing women, usually with \u201c to our sisters \u201d incorporated into the title. \n Question: Do they report results only on English data?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism. \n Question: What architecture does the decoder have?", "output": "LSTM", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Human Judgments\nFollowing BIBREF11 , BIBREF12 and the vast amount of previous work on semantic similarity, we ask nine undergraduate subjects to assess the similarity of 360 pairs of relations from a subset of Wikidata BIBREF8 that are chosen to cover from high to low levels of similarity. In our experiment, subjects were asked to rate an integer similarity score from 0 (no similarity) to 4 (perfectly the same) for each pair.  \n Question: How do they gather human judgements for similarity between relations?", "output": "By assessing similarity of 360 pairs of relations from a subset of Wikidata using an integer similarity score from 0 to 4", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "For instance, schema 23 from the WSC collection can be translated into both \u201cThe girls were bullying the boys so we [punished/rescued] them\u201d and \u201cThe boys were bullying the girls, so we [punished/rescued] them,\u201d thus avoiding any presupposition of whether girls are more likely to bully boys or vice versa. \n Question: What data do they look at?", "output": "WSC collection", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Suppose there are INLINEFORM0 mentions in documents on average, among these global models, NCEL not surprisingly has the lowest time complexity INLINEFORM1 since it only considers adjacent mentions, where INLINEFORM2 is the number of sub-GCN layers indicating the iterations until convergence. \n Question: Do they only use adjacent entity mentions or use more than that in some cases (next to adjacent)?", "output": "NCEL considers only adjacent mentions.", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Although such an approach has been used in different studies during feature engineering, the selection of word vectors and the number of clusters remain a trial-end-error procedure.  \n Question: Which other hyperparameters, other than number of clusters are typically evaluated in this type of research?", "output": "selection of word vectors", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We carried out experiments on four Chinese NLP tasks, including Emotion Classification (EC), Named Entity Recognition (NER), Sentence Pair Matching (SPM) and Natural Language Inference (NLI). \n Question: What benchmarks did they experiment on?", "output": "Emotion Classification (EC) Named Entity Recognition (NER) Sentence Pair Matching (SPM) Natural Language Inference (NLI)", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Probing Methodology and Modeling ::: Task Definition and Modeling ::: Baselines and Sanity Checks.\nWhen creating synthetic datasets, it is important to ensure that systematic biases, or annotation artifacts BIBREF41, are not introduced into the resulting probes and that the target datasets are sufficiently challenging (or good, in the sense of BIBREF42). To test for this, we use several of the MCQA baseline models first introduced in BIBREF0, which take inspiration from the LSTM-based models used in BIBREF43 for NLI and various partial-input baselines based on these models. \n Question: How do they control for annotation artificats?", "output": " we use several of the MCQA baseline models first introduced in BIBREF0", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We conduct experiments on WikiSQL BIBREF8 , which provides 87,726 annotated question-SQL pairs over 26,375 web tables. We do our study on SimpleQuestions BIBREF10 , which includes 108,442 simple questions, each of which is accompanied by a subject-relation-object triple. We conduct experiments on SequentialQA BIBREF9 which is derived from the WikiTableQuestions dataset BIBREF19 . \n Question: What datasets are used in this paper?", "output": "WikiSQL SimpleQuestions SequentialQA", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We use word alignments, similarly to other annotation projection work, to project the AMR alignments to the target languages. Our approach depends on an underlying assumption that we make: if a source word is word-aligned to a target word and it is AMR aligned with an AMR node, then the target word is also aligned to that AMR node. Word alignments were generated using fast_align BIBREF10 , while AMR alignments were generated with JAMR BIBREF11 . \n Question: How is annotation projection done when languages have different word order?", "output": "Word alignments are generated for parallel text, and aligned words are assumed to also share AMR node alignments.", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We show the statistics of the dataset we use in tab:statistics, and the construction procedures will be introduced in this section. In Wikidata BIBREF8 , facts can be described as (Head item/property, Property, Tail item/property).  We first choose the most common 202 relations and 120000 entities from Wikidata as our initial data. ReVerb BIBREF9 is a program that automatically identifies and extracts binary relationships from English sentences. We use the extractions from running ReVerb on Wikipedia. FB15K BIBREF3 is a subset of freebase. TACRED BIBREF10 is a large supervised relation extraction dataset obtained via crowdsourcing. We directly use these two dataset, no extra processing steps were applied. \n Question: Which knowledge bases do they use?", "output": "Wikidata ReVerb FB15K TACRED", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Adding word alignments in parallel sentences results in small, non significant improvements, even if there is some labeled data available in the source language. This difficulty in showing the usefulness of parallel corpora for SRI may be due to the current assumptions about role alignments, which mean that only a small percentage of roles are aligned. Further analyses reveals that annotating small amounts of data can easily outperform the performance gains obtained by adding large unlabeled dataset as well as adding parallel corpora. \n Question: Overall, does having parallel data improve semantic role induction across multiple languages?", "output": "No", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In this work, we build a novel regression model, based on linguistic, content, behavioral and topic features to detect Arabic Twitter bots to understand the impact of bots in spreading religious hatred in Arabic Twitter space.  \n Question: Do they propose a new model to better detect Arabic bots specifically?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Twitter data: We used the Twitter API to scrap tweets with hashtags. All non-English tweets were filtered out by the API. \n Question: Do they evaluate only on English datasets?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The main reason is that the datasets only contain a small portion of multi-aspect sentences with different polarities. The distraction of attention will not impact the sentiment prediction much in single-aspect sentences or multi-aspect sentences with the same polarities. \n Question: Is the model evaluated against the baseline also on single-aspect sentences?", "output": "No", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted.  We considered UD1.2 corpora for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.  \n Question: which datasets did they experiment with?", "output": "Universal Dependencies v1.2 treebanks for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German,\nIndonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Our baseline is TransE since that the score function of our models is based on TransE. \n Question: What baselines are used for comparison?", "output": "TransE", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In this work, we use the datasets released by BIBREF1 and HEOT dataset provided by BIBREF0 .  This was then followed by transliteration using the Xlit-Crowd conversion dictionary and translation of each word to English using Hindi to English dictionary.  \n Question: Do all the instances contain code-switching?", "output": "No", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In this way, an attention vector sequence is segmented into several subsequences and each subsequence represents the attention of one word. Then, motivated by the psycholinguistic finding that readers are likely to pay approximate attention to each character in one Chinese word, we devise an appropriate aggregation module to fuse the inner-word character attention. \n Question: How does the fusion method work?", "output": "ttention vector sequence is segmented into several subsequences and each subsequence represents the attention of one word we devise an appropriate aggregation module to fuse the inner-word character attention", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Cuneiform is an ancient writing system invented by the Sumerians for more than three millennia. \n Question: What is one of the first writing systems in the world?", "output": "Cuneiform", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In target data, the code is written in Python programming language. \n Question: What programming language is target language?", "output": "Python", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We used Amazon Mechanical Turk (MTurk) to collect new labels and explanations for SNLI-VE. 2,060 workers participated in the annotation effort, with an average of 1.98 assignments per worker and a standard deviation of 5.54. \n Question: How many annotators are used to write natural language explanations to SNLI-VE-2.0?", "output": "2,060 workers", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "For all the tasks in our experimental study, we use 36 millions English tweets collected between August and September 2017.  \n Question: Do they report results only on English datasets?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Our dataset is constructed via distant supervision from Twitter. \n Question: Where did they get the data for this project?", "output": "Twitter", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "For DQA four participants answered each question, therefore we took the average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values over the four evaluators as the result per question. The detailed answers by the participants and available online. To assess the correctness of the answers given both by participants in the DQA experiments, and by the QALD system, we use the classic information retrieval metrics of precision (P), recall (R), and F1. INLINEFORM0 measures the fraction of relevant (correct) answer (items) given versus all answers (answer items) given. \n Question: Do they test performance of their approaches using human judgements?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The corpus consists of a collection of 2,346 clinical notes (admission notes, progress notes, and discharge summaries), which amounts to 2,372,323 tokens in total (an average of 1,011 tokens per note). All the notes were written in English and extracted from the EHRs of 183 psychosis patients from McLean Psychiatric Hospital in Belmont, MA, all of whom had in their history at least one instance of 30-day readmission. \n Question: What is the dataset used?", "output": "EHRs of 183 psychosis patients from McLean Psychiatric Hospital in Belmont, MA", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "As shown in Table TABREF48, our two models show competitive results compared to the state-of-the-art on the Visual Dialog challenge 2018 (DL-61 was the winner of the Visual Dialog challenge 2018).  \n Question: What model was winner of the Visual Dialog challenge 2018?", "output": "DL-61", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Experimental Setup \n Question: What domains are detected in this paper?", "output": "Answer with content missing: (Experimental setup not properly rendered) In our experiments we used seven target domains: \u201cBusiness and Commerce\u201d (BUS), \u201cGovernment and Politics\u201d (GOV), \u201cPhysical and Mental Health\u201d (HEA), \u201cLaw and Order\u201d (LAW),\n\u201cLifestyle\u201d (LIF), \u201cMilitary\u201d (MIL), and \u201cGeneral Purpose\u201d (GEN). Exceptionally, GEN does\nnot have a natural root category.", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "For this purpose, we construct a corpus of 172,815 articles from Tagalog Wikipedia which we call WikiText-TL-39 BIBREF18. \n Question: What other datasets are used?", "output": "WikiText-TL-39", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "For training and testing we use the train-validation-test split of WikiTableQuestions BIBREF0 , a dataset containing 22,033 pairs of questions and answers based on 2,108 Wikipedia tables. This dataset is also used by our baselines, BIBREF0 , BIBREF3 . \n Question: Does the dataset they use differ from the one used by Pasupat and Liang, 2015?", "output": "No", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Car-speak is abstract language that pertains to a car's physical attribute(s). In this instance the physical attributes that the term \u201cfast\u201d pertains to could be the horsepower, or it could be the car's form factor (how the car looks). However, we do not know exactly which attributes the term \u201cfast\u201d refers to. \n Question: How does car speak pertains to a car's physical attributes?", "output": "we do not know exactly", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We found interesting patterns which are learned by our model and help understand these monolingual gains. For example, a recurring pattern is that words in English which are translated to the same word, or to semantically close words, in the target language end up closer together after our transformation. For example, in the case of English-Spanish the following pairs were among the pairs whose similarity increased the most by applying our transformation: cellphone-telephone, movie-film, book-manuscript or rhythm-cadence, which are either translated to the same word in Spanish (i.e., tel\u00e9fono and pel\u00edcula in the first two cases) or are already very close in the Spanish space. More generally, we found that word pairs which move together the most tend to be semantically very similar and belong to the same domain, e.g., car-bicycle, opera-cinema, or snow-ice.\n\n \n Question: Why does the model improve in monolingual spaces as well? ", "output": "because word pair similarity increases if the two words translate to similar parts of the cross-lingual embedding space", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We evaluate our method on IWSLT16 German-English (DE-EN) translation in both directions, WMT15 English-German (EN-DE) translation in both directions, and NIST Chinese-to-English (ZH$\\rightarrow $EN) translation.  \n Question: What corpora is used?", "output": "IWSLT16 WMT15 NIST", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We show the final results for opinion recommendation, comparing our proposed model with the following state-of-the-art baseline systems:\n\nRS-Average is the widely-adopted baseline (e.g., by Yelp.com), using the averaged review scores as the final score.\n\nRS-Linear estimates the rating score that a user would give by INLINEFORM0 BIBREF49 , where INLINEFORM1 and INLINEFORM2 are the the training deviations of the user INLINEFORM3 and the product INLINEFORM4 , respectively.\n\nRS-Item applies INLINEFORM0 NN to estimate the rating score BIBREF50 . We choose the cosine similarity between INLINEFORM1 to measure the distance between product.\n\nRS-MF is a state-of-the-art recommendation model, which uses matrix factorisation to predict rating score BIBREF8 , BIBREF41 , BIBREF25 .\n\nSum-Opinosis uses a graph-based framework to generate abstractive summarisation given redundant opinions BIBREF51 .\n\nSum-LSTM-Att is a state-of-the-art neural abstractive summariser, which uses an attentional neural model to consolidate information from multiple text sources, generating summaries using LSTM decoding BIBREF44 , BIBREF3 .\n\nAll the baseline models are single-task models, without considering rating and summarisation prediction jointly. The results are shown in Table TABREF46 .  \n Question: What are the baselines?", "output": "RS-Average  RS-Linear RS-Item RS-MF Sum-Opinosis Sum-LSTM-Att", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In contrast to existing bottlenecks, this work targets three different types of social networks (Formspring: a Q&A forum, Twitter: microblogging, and Wikipedia: collaborative knowledge repository) for three topics of cyberbullying (personal attack, racism, and sexism) without doing any explicit feature engineering by developing deep learning based models along with transfer learning. \n Question: What cyberbulling topics did they address?", "output": "personal attack, racism, and sexism", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Topology only embeddings: MMB BIBREF45 , DeepWalk BIBREF1 , LINE BIBREF33 , Node2vec BIBREF46 . ( INLINEFORM1 ) Joint embedding of topology & text: Naive combination, TADW BIBREF5 , CENE BIBREF6 , CANE BIBREF9 , WANE BIBREF10 , DMTE BIBREF34 .  \n Question: Which other embeddings do they compare against?", "output": "MMB DeepWalk LINE  Node2vec TADW CENE CANE WANE DMTE", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "For the image-aided model (W+C+V; upper row in Figure FIGREF19 ), we confirm that the modality attention successfully attenuates irrelevant signals (selfies, etc.) and amplifies relevant modality-based contexts in prediction of a given token. \n Question: Do they inspect their model to see if their model learned to associate image parts with words related to entities?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "What similarities and/or differences do these topics have with non-violent, non-Islamic religious material addressed specifically to women? As these questions suggest, to understand what, if anything, makes extremist appeals distinctive, we need a point of comparison in terms of the outreach efforts to women from a mainstream, non-violent religious group. For this purpose, we rely on an online Catholic women's forum. Comparison between Catholic material and the content of ISIS' online magazines allows for novel insight into the distinctiveness of extremist rhetoric when targeted towards the female population. To accomplish this task, we employ topic modeling and an unsupervised emotion detection method. \n Question: How are similarities and differences between the texts from violent and non-violent religious groups analyzed?", "output": "By using topic modeling and unsupervised emotion detection on ISIS materials and articles from Catholic women forum", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": " We used Convolutional Neural Network (CNN) network whose structure is similar to the one proposed by BIBREF29.  \n Question: What neural architectures are used?", "output": "Convolutional Neural Network (CNN)", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The current state-of-the-art approach BIBREF14 , BIBREF15 uses maximum entropy and CRF models with a combination of language model and hand-crafted features to predict if each character in the hashtag is the beginning of a new word. \n Question: What current state of the art method was used for comparison?", "output": "current state-of-the-art approach BIBREF14 , BIBREF15", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "For English dataset, we crawl 687 cybersecurity articles from a collection of advanced persistent threats (APT) reports which are published from 2008 to 2018.  \n Question: Where are the cybersecurity articles used in the model sourced from?", "output": " from a collection of advanced persistent threats (APT) reports which are published from 2008 to 2018", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The clinical notes we used for the experiment are provided by domain experts, consisting of 1,160 physician logs of Medical ICU admission requests at a tertiary care center affiliated to Mount Sanai. The enriched corpus contains 42,506 Wikipedia articles, each of which corresponds to one candidate, 6 research papers and 2 critical care medicine textbooks, besides our raw ICU data. \n Question: Which dataset do they use to build their model?", "output": "1,160 physician logs of Medical ICU admission requests 42,506 Wikipedia articles 6 research papers and 2 critical care medicine textbooks", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Thus, to limit the number of different inputs to the classifier, we wish to reduce the number of distinct word recognition outputs that an attacker can induce, not just the number of words on which the model is \u201cfooled\u201d. We denote this property of a model as its sensitivity. \n Question: What does the \"sensitivity\" quantity denote?", "output": "the number of distinct word recognition outputs that an attacker can induce", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The annotation process was a trial-and-error, with cycles composed of annotation, discussing confusing entities, updating the annotation guide schematic and going through the corpus section again to correct entities following guide changes. \n Question: Did they experiment with the corpus?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "As we only extracted references to other judicial decisions, we obtained 471,319 references from Supreme Court decisions, 167,237 references from Supreme Administrative Court decisions and 264,463 references from Constitutional Court Decisions. These are numbers of text spans identified as references prior the further processing described in Section SECREF3. \n Question: How big is the dataset?", "output": "903019 references", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Following work by doukhan2018open, we wanted to explore the corpora looking at the number of speakers of each gender category as well as their speech duration, considering both variables as good features to account for gender representation.  \n Question: What representations are presented by this paper?", "output": "the number of speakers of each gender category their speech duration", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "To test the robustness of the taggers against the OOV problem, we also conduct experiments using artificially constructed unnormalized text by corrupting words in the normal dev set. Again, the CNN tagger outperforms the two baselines by a very large margin. \n Question: How do they confirm their model working well on out-of-vocabulary problems?", "output": "conduct experiments using artificially constructed unnormalized text by corrupting words in the normal dev set", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "SVM: We define 3 sets of features to characterize each question. The first is a simple bag-of-words set of features over the question (SVM-BOW), the second is bag-of-words features of the question as well as length of the question in words (SVM-BOW + LEN), and lastly we extract bag-of-words features, length of the question in words as well as part-of-speech tags for the question (SVM-BOW + LEN + POS). This results in vectors of 200, 201 and 228 dimensions respectively, which are provided to an SVM with a linear kernel. No-Answer Baseline (NA) : Most of the questions we receive are difficult to answer in a legally-sound way on the basis of information present in the privacy policy. We establish a simple baseline to quantify the effect of identifying every question as unanswerable. Word Count Baseline : To quantify the effect of using simple lexical matching to answer the questions, we retrieve the top candidate policy sentences for each question using a word count baseline BIBREF53, which counts the number of question words that also appear in a sentence. We include the top 2, 3 and 5 candidates as baselines. Human Performance: We pick each reference answer provided by an annotator, and compute the F1 with respect to the remaining references, as described in section 4.2.1. Each reference answer is treated as the prediction, and the remaining n-1 answers are treated as the gold reference. The average of the maximum F1 across all reference answers is computed as the human baseline. \n Question: Were other baselines tested to compare with the neural baseline?", "output": "SVM No-Answer Baseline (NA)  Word Count Baseline Human Performance", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We first investigated overall detection of any NMT-Fake reviews (1,006 fake reviews and 994 real reviews). \n Question: How many reviews in total (both generated and true) do they evaluate on Amazon Mechanical Turk?", "output": "1,006 fake reviews and 994 real reviews", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "This method, which we refer to as progressive dynamic hurdles (PDH), allows models that are consistently performing well to train for more steps. It begins as ordinary tournament selection evolutionary architecture search with early stopping, with each child model training for a relatively small $s_0$ number of steps before being evaluated for fitness. However, after a predetermined number of child models, $m$ , have been evaluated, a hurdle, $h_0$ , is created by calculating the the mean fitness of the current population. For the next $m$ child models produced, models that achieve a fitness greater than $h_0$ after $s_0$ train steps are granted an additional $s_1$ steps of training and then are evaluated again to determine their final fitness. Once another $m$ models have been considered this way, another hurdle, $h_1$ , is constructed by calculating the mean fitness of all members of the current population that were trained for the maximum number of steps. For the next $m$ child models, training and evaluation continues in the same fashion, except models with fitness greater than $m$0 after $m$1 steps of training are granted an additional $m$2 number of train steps, before being evaluated for their final fitness. This process is repeated until a satisfactory number of maximum training steps is reached. \n Question: what is the proposed Progressive Dynamic Hurdles method?", "output": "allows models that are consistently performing well to train for more steps", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "To control the quality, we ensured that a single annotator annotates maximum 120 headlines (this protects the annotators from reading too many news headlines and from dominating the annotations). Secondly, we let only annotators who geographically reside in the U.S. contribute to the task.\n\nWe test the annotators on a set of $1,100$ test questions for the first phase (about 10% of the data) and 500 for the second phase. Annotators were required to pass 95%. \n Question: How is quality of annotation measured?", "output": "Annotators went through various phases to make sure their annotations did not deviate from the mean.", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "For a purpose of learning from limited annotated linguistic resources, our preliminary discovery shows that it is possible to build a geometric space projection between embedding spaces to help cross-lingual NE recognition. \n Question: What is their model?", "output": "cross-lingual NE recognition", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Meanwhile, attending colleges in the Northeast, West and South regions increases the possibility of posting about sexual harassment (positive coefficients), over the Midwest region.  \n Question: Which geographical regions correlate to the trend?", "output": "Northeast U.S., West U.S. and South U.S.", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by +1.86 in terms of F1 score on CTB5, +1.80 on CTB6 and +2.19 on UD1.4. \n Question: What are method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets?", "output": "+1.86 in terms of F1 score on CTB5 +1.80 on CTB6 +2.19 on UD1.4", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Our baseline decoder is a standard beam search decoder BIBREF5 with several straightforward performance optimizations \n Question: What baseline decoder do they use?", "output": "a standard beam search decoder BIBREF5 with several straightforward performance optimizations", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Strategy-based methods depart from the pre-training stage, seeking to take advantage of the pre-trained models during the process of target task learning. The approaches include fine-tuning schedules that elaborately design the control of learning rates for optimization, proxy tasks that leverage labeled data to help the pre-trained model better fit the target data distribution, and knowledge distillation approaches that ditch the paradigm of initialization with pre-trained parameters by adopting the pre-trained model as a teacher network. \n Question: How strategy-based methods handle obstacles in NLG?", "output": "fine-tuning schedules that elaborately design the control of learning rates for optimization proxy tasks that leverage labeled data to help the pre-trained model better fit the target data distribution knowledge distillation approaches that ditch the paradigm of initialization with pre-trained parameters by adopting the pre-trained model as a teacher network", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "All data were downloaded from Twitter in two separate batches using the \u201ctwint\" scraping tool BIBREF5 to sample historical tweets for several different search terms; queries always included either \u201cclimate change\" or \u201cglobal warming\", and further included disaster-specific search terms (e.g., \u201cbomb cyclone,\" \u201cblizzard,\" \u201csnowstorm,\" etc.).  \n Question: Do they report results only on English data?", "output": "Yes", "category": "Question Answering 1087.json"}]