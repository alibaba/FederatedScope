[{"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "One would expect that training with more data would improve the quality of the embeddings, but we found out with the results obtained with the C3 dataset, that only high-quality data helps.", "output": "What turn out to be more important high volume or high quality data?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "For the McGurk effect, we attempt an illusion for a language token (e.g. phoneme, word, sentence) $x$ by creating a video where an audio stream of $x$ is visually dubbed over by a person saying $x^{\\prime }\\ne x$ . The illusion $f(x^{\\prime },x)$ affects a listener if they perceive what is being said to be $y\\ne x$ if they watched the illusory video whereas they perceive $x$ if they had either listened to the audio stream without watching the video or had watched the original unaltered video, depending on specification.  A prototypical example is that the audio of the phoneme \u201cbaa,\u201d accompanied by a video of someone mouthing \u201cvaa\u201d, can be perceived as \u201cvaa\u201d or \u201cgaa\u201d (Figure 1 ).", "output": "What is the McGurk effect?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "To have an estimation about human performance in each metric, we iteratively treat every reference sentence in dev/test data as the prediction to be compared with all references (including itself). That is, if a model has the same reasoning ability with average performance of our crowd workers, its results should exceed this \u201chuman bound\u201d.", "output": "What measures were used for human evaluation?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Nonetheless, the main caveat of this basic pre-training is that the source encoder is trained to be used by an English decoder, while the target decoder is trained to use the outputs of an English encoder \u2014 not of a source encoder.", "output": "Is pivot language used in experiments English or some other language?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We seek a function controlled by gates that can mix states across timesteps, but which acts independently on each channel of the state vector. The simplest option, which BIBREF12 term \u201cdynamic average pooling\u201d, uses only a forget gate: DISPLAYFORM0", "output": "What pooling function is used?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We validate the performance of the proposed s2sL by providing the preliminary results obtained on two different tasks namely, Speech/Music discrimination and emotion classification. We considered the GTZAN Music-Speech dataset [17], consisting of 120 audio files (60 speech and 60 music), for task of classifying speech and music. Each audio file (of 2 seconds duration) is represented using a 13-dimensional mel-frequency cepstral coefficient (MFCC) vector, where each MFCC vector is the average of all the frame level (frame size of 30 msec and an overlap of 10 msec) MFCC vectors. It is to be noted that our main intention for this task is not better feature selection, but to demonstrate the effectiveness of our approach, in particular for low data scenarios. The standard Berlin speech emotion database (EMO-DB) [18] consisting of 535 utterances corresponding to 7 different emotions is considered for the task of emotion classification. ", "output": "Up to how many samples do they experiment with?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "TABREF5 displays the output behavior of models that we damaged to resemble the damage that causes aphasia. Because the output languages in all of our domains use tokens to represent meanings in many cases, it is expected that the analog to Wernicke's area is responsible for maintaining a high precision.", "output": "Do they perform a quantitative analysis of their model displaying knowledge distortions?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We use two datasets in this work: the training is done on the Fisher Corpus English Part 1 (LDC2004S13) BIBREF15 and testing on the Suicide Risk Assessment corpus BIBREF16 , along with Fisher.", "output": "Which dataset do they use to learn embeddings?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Dataset Quality Analysis ::: Inter-Annotator Agreement (IAA)\nTo estimate dataset consistency across different annotations, we measure F1 using our UA metric with 5 generators per predicate. Dataset Quality Analysis ::: Dataset Assessment and Comparison\nWe assess both our gold standard set and the recent Dense set against an integrated expert annotated sample of 100 predicates.  Dataset Quality Analysis ::: Agreement with PropBank Data\nIt is illuminating to observe the agreement between QA-SRL and PropBank (CoNLL-2009) annotations BIBREF7. ", "output": "How was quality measured?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Our training data consists of 2.09M sentence pairs extracted from LDC corpus. To test different approaches on Chinese-to-English translation task, we use NIST 2003(MT03) dataset as the validation set, and NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) datasets as our test sets. For English-to-Chinese translation task, we also use NIST 2003(MT03) dataset as the validation set, and NIST 2008(MT08) will be used as test set.", "output": "What dataset did they use?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "To adapt the FactorCell BIBREF4 for our purposes, we replace user embeddings with a low-dimensional image representation. Thus, we are able to modify each query completion to be personalized to a specific image representation.", "output": "How they complete a user query prefix conditioned upon an image?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Firstly, LastStateRNN is the classic RNN model, where the last state passes through an MLP and then the LR Layer estimates the corresponding probability. In contrast, in the AvgRNN model we consider the average vector of all states that come out of the cells. The AttentionRNN model is the one that it has been presented in BIBREF9. Moreover, we introduce the MultiAttentionRNN model for the harassment language detection, which instead of one attention, it includes four attentions, one for each category.", "output": "What baseline model is used?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We employ a multi-class Naive Bayes classifier as the second stage classification mechanism, for categorizing tweets appropriately, depending on the type of emergencies they indicate.", "output": "What classifier is used for emergency categorization?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We compare the results of our model to a baseline multi-task architecture inspired by yang2016multi. In our baseline model there are no explicit connections between tasks - the only shared parameters are in the hidden layer.", "output": "What is the baseline?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The first approach is based on hierarchical modeling BIBREF13 , which assumes that the group-specific embedding representations are tied through a global embedding.", "output": "What hierarchical modelling approach is used?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": ". This work can be extended to predict future events with one day in advance, where we will use the same method for feature selection in addition to to time series analysis of the historical patterns of the word-pairs.", "output": "Do the authors suggest any future extensions to this work?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Our transference model extends the original transformer model to multi-encoder based transformer architecture. The transformer architecture BIBREF12 is built solely upon such attention mechanisms completely replacing recurrence and convolutions. The transformer uses positional encoding to encode the input and output sequences, and computes both self- and cross-attention through so-called multi-head attentions, which are facilitated by parallelization. We use multi-head attention to jointly attend to information at different positions from different representation subspaces.", "output": "Which algorithm is used in the UDS-DFKI system?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "An extrinsic evaluation was carried out on the task of Open IE BIBREF7.", "output": "Is the semantic hierarchy representation used for any task?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In this regard, we observed that HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset. In terms of inception score (IS), which is the metric that was applied to majority models except DC-GAN, the results in Table TABREF48 show that StackGAN++ only showed slight improvement over its predecessor, StackGAN, for text-to-image synthesis. In addition, the results in Table TABREF48 also show that DM-GAN BIBREF53 has the best performance, followed by Obj-GAN BIBREF81. Notice that both DM-GAN and Obj-GAN are most recently developed methods in the field (both published in 2019), indicating that research in text to image synthesis is continuously improving the results for better visual perception and interception.", "output": "What is the conclusion of comparison of proposed solution?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "For both POS tagging and dependency parsing, we run experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank.", "output": "Do they evaluate only on English datasets?, What datasets do they evaluate on?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "For example, the original sentence `We went shop on Saturday' and the corrected version `We went shopping on Saturday' would produce the following pattern:\n\n(VVD shop_VV0 II, VVD shopping_VVG II)\n\nAfter collecting statistics from the background corpus, errors can be inserted into error-free text. The learned patterns are now reversed, looking for the correct side of the tuple in the input sentence. We only use patterns with frequency INLINEFORM0 , which yields a total of 35,625 patterns from our training data. ", "output": "What textual patterns are extracted?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Compared to using external models for confidence modeling, an advantage of the proposed method is that the base model does not change: the binary classification loss just provides additional supervision. For supervised open IE systems, the confidence score of an assertion is typically computed based on its extraction likelihood given by the model BIBREF3 , BIBREF5  We follow the evaluation metrics described by Stanovsky:2016:OIE2016: area under the precision-recall curve (AUC) and F1 score.", "output": "How does this compare to traditional calibration methods like Platt Scaling?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The annotation projection method, which we follow in this work, is one way to address this problem. It was introduced for POS tagging, base noun phrase bracketing, NER tagging, and inflectional morphological analysis BIBREF29 but it has also been used for dependency parsing BIBREF30 , role labeling BIBREF31 , BIBREF32 and semantic parsing BIBREF26 .", "output": "Do the authors test their annotation projection techniques on tasks other than AMR?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In addition to the traditional VAE structure, we introduces an extra context-aware latent variable in CWVAE to learn the event background knowledge. In the pretrain stage, CWVAE is trained on an auxiliary dataset (consists of three narrative story corpora and contains rich event background knowledge), to learn the event background information by using the context-aware latent variable. Subsequently, in the finetune stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target (e.g., intents, reactions, etc.).", "output": "How does the context-aware variational autoencoder learn event background information?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We conduct our experiments on $\\sim $ 8.7M annotated anonymised user utterances. They are annotated and derived from requests across 23 domains.", "output": "Over which datasets/corpora is this work evaluated?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": " In this work, we develop a technique to rapidly transfer an existing pre-trained model from English to other languages in an energy efficient way BIBREF8. As the first step, we focus on building a bilingual language model (LM) of English and a target language. Starting from a pre-trained English LM, we learn the target language specific parameters (i.e., word embeddings), while keeping the encoder layers of the pre-trained English LM fixed. We then fine-tune both English and target model to obtain the bilingual LM. ", "output": "How is the model transferred to other languages?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin.  Experimental results on real-world dataset demonstrate that our proposed model competes favorably with strong baseline models in all three specific tasks.", "output": "What are the specific tasks being unified?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In this chapter, we apply an ensemble of deep learning and linguistics to tackle both the problem of aspect extraction and subjectivity detection.", "output": "How are aspects identified in aspect extraction?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Although our approach strongly outperforms random baselines, the relatively low F1 scores indicate that predicting which word is echoed in explanations is a very challenging task. It follows that we are only able to derive a limited understanding of how people choose to echo words in explanations. The extent to which explanation construction is fundamentally random BIBREF47, or whether there exist other unidentified patterns, is of course an open question. We hope that our study and the resources that we release encourage further work in understanding the pragmatics of explanations.", "output": "Do authors provide any explanation for intriguing patterns of word being echoed?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We consider two different models for each language pair: the Baseline and the Document model. We evaluate them on 3 test sets and report BLEU and TER scores. All experiments are run 8 times with different seeds, we report averaged results and p-values for each experiment.", "output": "What evaluation metrics did they use?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Co-Reference Resolution: To support multi-turn interactions, it is sometimes necessary to use co-reference resolution techniques for effective retrieval. In Macaw, we identify all the co-references from the last request of user to the conversation history. The same co-reference resolution outputs can be used for different query generation components. This can be a generic or action-specific component.\n\nQuery Generation: This component generates a query based on the past user-system interactions. The query generation component may take advantage of co-reference resolution for query expansion or re-writing.\n\nRetrieval Model: This is the core ranking component that retrieves documents or passages from a large collection. Macaw can retrieve documents from an arbitrary document collection using the Indri python interface BIBREF9, BIBREF10. We also provide the support for web search using the Bing Web Search API. Macaw also allows multi-stage document re-ranking.\n\nResult Generation: The retrieved documents can be too long to be presented using some interfaces. Result generation is basically a post-processing step ran on the retrieved result list. In case of question answering, it can employ answer selection or generation techniques, such as machine reading comprehension models. For example, Macaw features the DrQA model BIBREF11 for question answering.", "output": "What functionality does Macaw provide?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Using our annotation software, we automatically extracted landmarks of 2000 images from the UOttawa database BIBREF14 had been annotated for image segmentation tasks. ", "output": "How big are datasets used in experiments?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In our experiment, we use a conversational Mandarin-English code-switching speech corpus called SEAME Phase II (South East Asia Mandarin-English). ", "output": "What languages are explored in this paper?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Abstract meaning representation BIBREF0 , or AMR for short, allows us to do that with the inclusion of most of the shallow-semantic natural language processing (NLP) tasks that are usually addressed separately, such as named entity recognition, semantic role labeling and co-reference resolution.", "output": "Which subtasks do they evaluate on?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "To evaluate the performance of our approach, we used a subset of the SNIPS BIBREF12 dataset, which is readily available in RASA nlu format.  We use accuracy of intent and entity recognition as our task and metric. ", "output": "How are their changes evaluated?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We used the relation classification dataset of the SemEval 2010 task 8 BIBREF8 . It consists of sentences which have been manually labeled with 19 relations (9 directed relations and one artificial class Other). 8,000 sentences have been distributed as training set and 2,717 sentences served as test set.", "output": "Which dataset do they train their models on?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Validated transcripts were sent to professional translators. In order to control the quality of the professional translations, we applied various sanity checks to the translations BIBREF11. We also sanity check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and confirm they are totally disjoint.", "output": "How is the quality of the data empirically evaluated? ", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We adopt a DNN-based acoustic model BIBREF0 with 11 hidden layers and the alignment used to train the model is derived from a HMM-GMM model trained with SAT criterion.", "output": "What are the deep learning architectures used in the task?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Yet, it is possible that the precise interpretability scores that are measured here are biased by the dataset used. However, it should be noted that the change in the interpretability scores for different word coverages might be effected by non-ideal subsampling of category words. Although our word sampling method, based on words' distances to category centers, is expected to generate categories that are represented better compared to random sampling of category words, category representations might be suboptimal compared to human designed categories.", "output": "What are the weaknesses of their proposed interpretability quantification method?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Experiment 1 directly tested the hypothesis that speakers increase their specificity in contexts with asymmetry in visual access. We found that speakers are not only context-sensitive in choosing referring expressions that distinguish target from distractors in a shared context, but are occlusion-sensitive, adaptively compensating for uncertainty. These results strongly suggest that the speaker's informativity influences listener accuracy. In support of this hypothesis, we found a strong negative correlation between informativity and error rates across items and conditions: listeners make fewer errors when utterances are a better fit for the target relative to the distractor ( $\\rho = -0.81$ , bootstrapped 95% CI $= [-0.9, -0.7]$ ; Fig. 6 B). This result suggests that listener behavior is driven by an expectation of speaker informativity: listeners interpret utterances proportionally to how well they fit objects in context. Our Rational Speech Act (RSA) formalization of cooperative reasoning in this context predicts that speakers (directors) naturally increase the informativity of their referring expressions to hedge against the increased risk of misunderstanding; Exp. 1 presents direct evidence in support of this hypothesis. Exp. 2 is consistent with this hypothesis; when directors used underinformative scripted instructions (taken from prior work), listeners made significantly more errors than when speakers were allowed to provide referring expressions at their natural level of informativity, and speaker informativeness strongly modulated listener error rates.", "output": "Did participants behave unexpectedly?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The dataset comprises a total of 353 conversations from 40 speakers (11 nurses, 16 patients, and 13 caregivers) with consent to the use of anonymized data for research. In Section SECREF16 , we build templates and expression pools using linguistic analysis followed by manual verification.", "output": "How big is their created dataset?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We use the publicly available dataset KVRET BIBREF5 in our experiments. There are 2,425 dialogues for training, 302 for validation and 302 for testing, as shown in the upper half of Table TABREF12.", "output": "What is the size of the dataset?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Among all single models, LFT performs the best, followed by MinAvgOut. RL is also comparable with previous state-of-the-art models VHRED (attn) and Reranking-RL. We think that this is because LFT exerts no force in pulling the model predictions away from the ground-truth tokens, but rather just makes itself aware of how dull each response is. Consequently, its responses appear more relevant than the other two approaches. Moreover, the hybrid model (last row) outperforms all other models by a large margin. ", "output": "Which one of the four proposed models performed best?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The main questions this paper attempts to answer are: Does compressing BERT impede it's ability to transfer to new tasks? And does fine-tuning make BERT more or less compressible?\n\nTo explore these questions, we compressed English BERT using magnitude weight pruning BIBREF8 and observed the results on transfer learning to the General Language Understanding Evaluation (GLUE) benchmark BIBREF9, a diverse set of natural language understanding tasks including sentiment analysis, NLI, and textual similarity evaluation. We chose magnitude weight pruning, which compresses models by removing weights close to 0, because it is one of the most fine-grained and effective compression methods and because there are many interesting ways to view pruning, which we explore in the next section. Our findings are as follows: Low levels of pruning (30-40%) do not increase pre-training loss or affect transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks.", "output": "How they observe that fine-tuning BERT on a specific task does not improve its prunability?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We set up the baselines and proposed models as follows: LSTM with text embedding: CNNs and LSTMs are widely used to encode textual contents for sentiment analysis in BIBREF45 , BIBREF46 and many online tutorials. Here we select the standard LSTM with pre-trained word embedding as input, and add one fully-connected layer with sigmoid activation top of the LSTM encoder (same as all other models), denoted as T-LSTM. LSTM with emoji embedding: We consider the emoji as one special word and input both pre-trained text and emoji embeddings into the same LSTM network, namely E-LSTM. Similarly, we concatenate the pre-trained bi-sense emoji embedding as one special word to feed into the LSTM network. This model is called BiE-LSTM. Attention-based LSTM with emojis:We also use the word-emoji embedding to calculate the emoji-word attention following Equation EQREF20 and EQREF21 , and the only difference is that we replace the attention-derived senti-emoji embedding with the pre-trained word-emoji embedding by fasttext, denoted as ATT-E-LSTM.", "output": "What is the baseline for experiments?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": " Experimentally, on three benchmark datasets for machine translation \u2013 WMT2014, WMT2016 and IWSLT-2014, FlowSeq achieves comparable performance with state-of-the-art non-autoregressive models, and almost constant decoding time w.r.t. the sequence length compared to a typical left-to-right Transformer model, which is super-linear.", "output": "What are three neural machine translation (NMT) benchmark datasets used for evaluation?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We extract user's age by applying regular expression patterns to profile descriptions (such as \"17 years old, self-harm, anxiety, depression\") BIBREF41 . We compile \"age prefixes\" and \"age suffixes\", and use three age-extraction rules: 1. I am X years old 2. Born in X 3. X years old, where X is a \"date\" or age (e.g., 1994). We selected a subset of 1464 users INLINEFORM0 from INLINEFORM1 who disclose their gender in their profile description.", "output": "Where does the information on individual-level demographics come from?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Following BIBREF21, we sample $K$ captions for each image when applying REINFORCE: ${\\hat{c}}_1 \\ldots {\\hat{c}}_K$, ${\\hat{c}}_k \\sim p_{\\theta }(c|I)$,\n\nThe baseline for each sampled caption is defined as the average reward of the rest samples.", "output": "What baseline function is used in REINFORCE algorithm?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Method ::: Passage Ranking Model\nThe key component of our framework is the Ranker model, which is provided with a question $q$ and $K$ passages $\\mathcal {P} = \\lbrace p_1, p_2 ... p_K\\rbrace $ from a pool of candidates, and outputs a chain of selected passages. Method ::: Cooperative Reasoner\nTo alleviate the noise in the distant supervision signal $\\mathcal {C}$, in addition to the conditional selection, we further propose a cooperative Reasoner model, also implemented with the MatchLSTM architecture (see Appendix SECREF6), to predict the linking entity from the selected passages.", "output": "What are two models' architectures in proposed solution?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8. Table presents the results for bilingual UWS and multilingual leveraging. ", "output": "Does the paper report the accuracy of the model?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": " As well as refining the annotation guidelines, the development process trained annotators who were not security experts. The development and test sets for Darkode and Hack Forums were annotated by additional team members (five for Darkode, one for Hack Forums), and then every disagreement was discussed and resolved to produce a final annotation. The authors, who are researchers in either NLP or computer security, did all of the annotation.", "output": "Who annotated the data?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We also measure the usage of words related to people's core values as reported by Boyd et al. boyd2015. The sets of words, or themes, were excavated using the Meaning Extraction Method (MEM) BIBREF10 .", "output": "How do they obtain psychological dimensions of people?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "As the baseline model (BASE-4L) for IWSLT14 German-English and Turkish-English, we use a 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256 by default.", "output": "what are the baselines?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The baseline is a hierarchical phrase-based system BIBREF29 with a 4-gram language model, with feature weights tuned using MIRA BIBREF30 . Therefore, an appropriate additional baseline would be to mark translation rules with these indicator functions but without the scores, akin to identifying rules with phrases in them (Baseline + SegOn).", "output": "Which translation systems do they compare against?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We took inspiration from these works to design our experiments to solve the CSKS task.", "output": "What problem do they apply transfer learning to?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Currently, the following WSD models induced from a text corpus are available: Word senses based on cluster word features. This model uses the cluster words from the induced word sense inventory as sparse features that represent the sense.\n\nWord senses based on context word features. This representation is based on a sum of word vectors of all cluster words in the induced sense inventory weighted by distributional similarity scores.\n\nSuper senses based on cluster word features. To build this model, induced word senses are first globally clustered using the Chinese Whispers graph clustering algorithm BIBREF9 . The edges in this sense graph are established by disambiguation of the related words BIBREF11 , BIBREF12 . The resulting clusters represent semantic classes grouping words sharing a common hypernym, e.g. \u201canimal\u201d. This set of semantic classes is used as an automatically learned inventory of super senses: There is only one global sense inventory shared among all words in contrast to the two previous traditional \u201cper word\u201d models. Each semantic class is labeled with hypernyms. This model uses words belonging to the semantic class as features.\n\nSuper senses based on context word features. This model relies on the same semantic classes as the previous one but, instead, sense representations are obtained by averaging vectors of words sharing the same class.", "output": "Do they use a neural model for their task?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In this section, we propose new simple disentanglement models that perform better than prior methods, and re-examine prior work.", "output": "Did they experiment with the corpus?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "String kernels represent a way of using information at the character level by measuring the similarity of strings through character n-grams.", "output": "What is a string kernel?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We created this CORD-19-NER dataset with comprehensive named entity annotation on the CORD-19 corpus (2020-03-13).  The corpus is generated from the 29,500 documents in the CORD-19 corpus (2020-03-13). ", "output": "What is the size of this dataset?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We define the STransE score function $\\mathcal {R}$1 as follows:\n\n$ f_r(h, t) & = & \\Vert \\textbf {W}_{r,1}\\textbf {h} + \\textbf {r} - \\textbf {W}_{r,2}\\textbf {t}\\Vert _{\\ell _{1/2}} $\n\nusing either the $\\ell _1$ or the $\\ell _2$ -norm (the choice is made using validation data; in our experiments we found that the $\\ell _1$ norm gave slightly better results).", "output": "What scoring function does the model use to score triples?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The current state-of-the-art approach BIBREF14 , BIBREF15 uses maximum entropy and CRF models with a combination of language model and hand-crafted features to predict if each character in the hashtag is the beginning of a new word.", "output": "What current state of the art method was used for comparison?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Specifically, we group the models based on the objective function it optimizes. We believe this work can aid the understanding of the existing literature.  We believe that the performance of these models is highly dependent on the objective function it optimizes \u2013 predicting adjacent word (within-tweet relationships), adjacent tweet (inter-tweet relationships), the tweet itself (autoencoder), modeling from structured resources like paraphrase databases and weak supervision. ", "output": "How do they encourage understanding of literature as part of their objective function?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The reviews we used have been extracted from TripAdvisor and originally proposed in BIBREF10 , BIBREF11 . ", "output": "Where are the hotel reviews from?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Irony Classifier: We implement a CNN classifier trained with our irony dataset. Sentiment Classifier for Irony: We first implement a one-layer LSTM network to classify ironic sentences in our dataset into positive and negative ironies. Sentiment Classifier for Non-irony: Similar to the training process of the sentiment classifier for irony, we first implement a one-layer LSTM network trained with the dataset for the sentiment analysis of common twitters to classify the non-ironies into positive and negative non-ironies. In this section, we describe some additional experiments on the transformation from ironic sentences to non-ironic sentences.", "output": "What experiments are conducted?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We have observed that the number of epochs is an important parameter and its increase leads to results that rank our two worst models almost equal, or even better than others.", "output": "How does different parameter settings impact the performance and semantic capacity of resulting model?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Amazon Reviews Dataset BIBREF24 is a large dataset with millions of reviews from different product categories. For our experiments, we consider a subset of 20000 reviews from the domains Cell Phones and Accessories(C), Clothing and Shoes(S), Home and Kitchen(H) and Tools and Home Improvement(T). Out of 20000 reviews, 10000 are positive and 10000 are negative. We use 12800 reviews for training, 3200 reviews for validation and 4000 reviews for testing from each domain.", "output": "For the purposes of this paper, how is something determined to be domain specific knowledge?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Among all available parameters to tune the D2V algorithm released by Gensim, six of them were selected for optimisation BIBREF14. The window_size parameter affects the size of the sliding window used to parse texts. The alpha parameter represents the learning rate of the network. The sample setting allows the model to reduce the importance given to high-frequency words. The dm parameter defines the training used architecture (PV-DM or PV-DBOW). ", "output": "What Doc2Vec architectures other than PV-DBOW have been tried?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "terms extracted from Yahoo! Answers tend to be more related, in terms of the number of correlated terms, to attributes related to religion or ethnicity compared to terms from Twitter. However, for two particular attributes (i.e., Price and Buddhist), the number of correlated terms from Twitter is higher than the ones from Yahoo! Answers . ", "output": "On Twitter, do the demographic attributes and answers show more correlations than on Yahoo! Answers?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "For the two single sentence tasks\u2014the syntax-oriented CoLA task and the SST sentiment task\u2014we find somewhat deteriorated performance.", "output": "Does the additional training on supervised tasks hurt performance in some tasks?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "A part of the ILCI English-Hindi Tourism parallel corpus (1500 sentences) was used for training the classifiers. ", "output": "What is the size of the parallel corpus used to train the model constraints?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "the facts that are relevant for answering a particular question) are labeled during training.", "output": "What does supporting fact supervision mean?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We use the MedWeb (\u201cMedical Natural Language Processing for Web Document\u201d) dataset BIBREF4 that was provided as part of a subtask at the NTCIR-13 Conference BIBREF5. The data is summarised in Table TABREF1. There are a total of 2,560 pseudo-tweets in three different languages: Japanese (ja), English (en) and Chinese (zh).", "output": "How big is dataset used for fine-tuning model for detection of red flag medical symptoms in individual statements?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Adding word alignments in parallel sentences results in small, non significant improvements, even if there is some labeled data available in the source language. This difficulty in showing the usefulness of parallel corpora for SRI may be due to the current assumptions about role alignments, which mean that only a small percentage of roles are aligned. Further analyses reveals that annotating small amounts of data can easily outperform the performance gains obtained by adding large unlabeled dataset as well as adding parallel corpora.", "output": "Overall, does having parallel data improve semantic role induction across multiple languages?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We recruited 102 pairs of participants from Amazon Mechanical Turk and randomly assigned speaker and listener roles", "output": "Was this experiment done in a lab?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We also explored clustering 12742 STRENGTH sentences directly using CLUTO BIBREF19 and Carrot2 Lingo BIBREF20 clustering algorithms. ", "output": "What clustering algorithms were used?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Here I would use Informap algorithm BIBREF12. To make a comparison to this method, I am using CCM and SCA for distance measurement in this experiment, too. UPGMA algorithm would be used accordingly in these two cases.", "output": "Is the proposed method compared to previous methods?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Thus we consider the task of answer generation for TweetQA and we use several standard metrics for natural language generation to evaluate QA systems on our dataset, namely we consider BLEU-1 BIBREF16 , Meteor BIBREF17 and Rouge-L BIBREF18 in this paper.", "output": "What evaluation metrics do they use?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The first challenge is to acquire very large Vietnamese corpus and to use them in building a classifier, which could further improve accuracy.  The second challenge is design and development of big data warehouse and analytic framework for Vietnamese documents, which corresponds to the rapid and continuous growth of gigantic volume of articles and/or documents from Web 2.0 applications, such as, Facebook, Twitter, and so on. The final challenge relates to building a system, which is able to incrementally learn new corpora and interactively process feedback.", "output": "Why challenges does word segmentation in Vietnamese pose?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We train both of the systems on the WMT15 German-to-English training data, see Table TABREF18 for some statistics. For this purpose, we use manual alignments provided by RWTH German-English dataset as the hard alignments.", "output": "What datasets are used?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Following the previous works, we report the precision ($P.$), recall ($R.$) and $F_1$ scores for target recognition and targeted sentiment. ", "output": "How is the effectiveness of the model evaluated?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "For all the classifiers, our feature combination outperforms the baselines (considering only unigram features) as well as BIBREF3 , with the MILR classifier getting an F-score improvement of 3.7% and Kappa difference of 0.08. We also achieve an improvement of 2% over the baseline, using SVM classifier, when we employ our feature set. We also observe that the gaze features alone, also capture the differences between sarcasm and non-sarcasm classes with a high-precision but a low recall.", "output": "What other evaluation metrics are looked at?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The SG model achieved a high average similarity score of 0.650 followed by CBoW with a 0.632 average similarity score. The GloVe also achieved a considerable average score of 0.591 respectively. However, the average similarity score of SdfastText is 0.388 and the word pair Microsoft-Bill Gates is not available in the vocabulary of SdfastText. Moreover, the average semantic relatedness similarity score between countries and their capitals is shown in Table TABREF78 with English translation, where SG also yields the best average score of 0.663 followed by CBoW with 0.611 similarity score. The GloVe also yields better semantic relatedness of 0.576 and the SdfastText yield an average score of 0.391.", "output": "How does proposed word embeddings compare to Sindhi fastText word representations?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "One distinguishing feature of educational topics is their breadth of sub-topics and points of view, as they attract researchers, practitioners, parents, students, or policy-makers. We assume that this diversity leads to the linguistic variability of the education topics and thus represents a challenge for NLP.", "output": "What challenges do different registers and domains pose to this task?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "o evaluate the quality of the generated sequences regarding both precision and recall, the evaluation metrics include BLEU and ROUGE (1, 2, L) scores with multiple references BIBREF22 .", "output": "What evaluation metrics are used?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We use the additional INLINEFORM0 training articles labeled by publisher as an unsupervised data set to further train the BERT model. We first investigate the impact of pre-training on BERT-BASE's performance.  On the same computer, fine tuning the model on the small training set took only about 35 minutes for sequence length 100. ", "output": "How are the two different models trained?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Chowdhury BIBREF14 and Thomas et al. BIBREF11 proposed methods that use linguistic phenomenons and two-stage SVM to classify DDIs. FBK-irst BIBREF10 is a follow-on work which applies kernel method to the existing model and outperforms it. Neural network based approaches have been proposed by several works. Liu et al. BIBREF9 employ CNN for DDI extraction for the first time which outperforms the traditional machine learning based methods.  Sahu et al. BIBREF12 proposed LSTM based DDI extraction approach and outperforms CNN based approach, since LSTM handles sentence as a sequence instead of slide windows.", "output": "What are the existing methods mentioned in the paper?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The baseline classifier uses a linear Support Vector Machine BIBREF7 , which is suited for a high number of features. ", "output": "What baseline is used?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The corpus used for sentiment analysis is the IMDb dataset of movie reviews by BIBREF11 while that for NER is Groningen Meaning Bank (GMB) by BIBREF12, containing 47,959 sentence samples. ", "output": "What Named Entity Recognition dataset is used?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We use the pre-trained uncased BERT$_\\mathrm {BASE}$ model for fine-tuning, because we find that BERT$_\\mathrm {LARGE}$ model performs slightly worse than BERT$_\\mathrm {BASE}$ in this task. ", "output": "Do they use large or small BERT?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am.", "output": "what is the source of the news sentences?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8. Table presents the results for bilingual UWS and multilingual leveraging. For the former, we reach our best result by using as aligned information the French, the original aligned language for this dataset. Languages closely related to French (Spanish and Portuguese) ranked better, while our worst result used German. English also performs notably well in our experiments. We believe this is due to the statistics features of the resulting text. We observe in Table that the English portion of the dataset contains the smallest vocabulary among all languages. Since we train our systems in very low-resource settings, vocabulary-related features can impact greatly the system's capacity to language-model, and consequently the final quality of the produced alignments. Even in high-resource settings, it was already attested that some languages are more difficult to model than others BIBREF9.", "output": "Is the model evaluated against any baseline?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence.  The baseline for the FLC task generates spans and selects one of the 18 techniques randomly. ", "output": "What was the baseline for this task?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "While it is obvious that our embeddings can be used as features for new predictive models, it is also very easy to incorporate our learned Dolores embeddings into existing predictive models on knowledge graphs. The only requirement is that the model accepts as input, an embedding layer (for entities and relations). If a model fulfills this requirement (which a large number of neural models on knowledge graphs do), we can just use Dolores embeddings as a drop-in replacement. We just initialize the corresponding embedding layer with Dolores embeddings. In our evaluation below, we show how to improve several state-of-the-art models on various tasks simply by incorporating Dolores as a drop-in replacement to the original embedding layer.", "output": "How are meaningful chains in the graph selected?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each.\n\nWord are white-space and punctuation separated, and some spelling errors are corrected (1.33% of the total words) to have very clean test cases. Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization as shown in Figure FIGREF2 .", "output": "How was the dataset annotated?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "However, adversarial misspellings constitute a longstanding real-world problem. Spammers continually bombard email servers, subtly misspelling words in efforts to evade spam detection while preserving the emails' intended meaning BIBREF1 , BIBREF2 .", "output": "Why is the adversarial setting appropriate for misspelling recognition?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We used two parallel corpora: Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0 and NTCIR PatentMT Parallel Corpus BIBREF1 .", "output": "What parallel corpus did they use?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Sentiment Classification\nWe first conduct a multi-task experiment on sentiment classification.\n\nWe use 16 different datasets from several popular review corpora used in BIBREF20 . These datasets consist of 14 product review datasets and two movie review datasets.\n\nAll the datasets in each task are partitioned randomly into training set, development set and testing set with the proportion of 70%, 10% and 20% respectively. Transferability of Shared Sentence Representation\nWith attention mechanism, the shared sentence encoder in our proposed models can generate more generic task-invariant representations, which can be considered as off-the-shelf knowledge and then be used for unseen new tasks. Introducing Sequence Labeling as Auxiliary Task\nA good sentence representation should include its linguistic information. Therefore, we incorporate sequence labeling task (such as POS Tagging and Chunking) as an auxiliary task into the multi-task learning framework, which is trained jointly with the primary tasks (the above 16 tasks of sentiment classification).", "output": "What tasks did they experiment with?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": " We show that using external knowledge outside the tweet text (from landing pages of URLs) and user features can significantly improve performance. ", "output": "What external sources of information are used?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Car-speak is abstract language that pertains to a car's physical attribute(s). In this instance the physical attributes that the term \u201cfast\u201d pertains to could be the horsepower, or it could be the car's form factor (how the car looks). However, we do not know exactly which attributes the term \u201cfast\u201d refers to. We train a series of classifiers in order to classify car-speak. We train three classifiers on the review vectors that we prepared in Section SECREF8. The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13.", "output": "Is car-speak language collection of abstract features that classifier is later trained on?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "BIBREF17: The Lemming model is a log-linear model that performs joint morphological tagging and lemmatization. ", "output": "What were the non-neural baselines used for the task?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We carried out our experiments on the Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2, from which morphosyntactically annotated corpora can be trivially extracted.  We considered UD1.2 corpora for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish. ", "output": "which datasets did they experiment with?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "For evaluation metrics, we used BLEU BIBREF8 and RIBES BIBREF9 to measure translation accuracy, and token-level delay to measure latency. We used Kytea BIBREF10 as a tokenize method for evaluations of Japanese translation accuracy.", "output": "Which metrics do they use to evaluate simultaneous translation?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "GE-FL reduces the heavy load of instance annotation and performs well when we provide prior knowledge with no bias. In our experiments, we observe that comparable numbers of labeled features for each class have to be supplied. We randomly select $t \\in [1, 20]$ features from the feature pool for one class, and only one feature for the other. Our methods are also evaluated on datasets with different unbalanced class distributions. We manually construct several movie datasets with class distributions of 1:2, 1:3, 1:4 by randomly removing 50%, 67%, 75% positive documents. Incorporating KL divergence is robust enough to control unbalance both in the dataset and in labeled features while the other three methods are not so competitive.", "output": "How do they define robustness of a model?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We conduct experiments on the SQuAD dataset BIBREF3.", "output": "On what datasets are experiments performed?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "For the 2019 Workshop on Neural Generation of Text (WNGT) Efficiency shared task BIBREF0, the Notre Dame Natural Language Processing (NDNLP) group looked at a method of inducing sparsity in parameters called auto-sizing in order to reduce the number of parameters in the Transformer at the cost of a relatively minimal drop in performance.", "output": "What is WNGT 2019 shared task?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Non-contextual properties of a word. These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.", "output": "What non-contextual properties do they refer to?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We are using the disaster data from BIBREF5. It contains various dataset including the CrisiLexT6 dataset which contains six crisis events related to English tweets in 2012 and 2013, labeled by relatedness (on-topic and off-topic) of respective crisis. Each crisis event tweets contain almost 10,000 labeled tweets but we are only focused on flood-related tweets thus, we experimented with only two flood event i.e. Queensland flood in Queensland, Australia and Alberta flood in Alberta, Canada and relabeled all on-topic tweets as Related and Off-topic as Unrelated for implicit class labels understanding in this case.", "output": "What dataset did they use?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "To test whether topic models can be used for dating poetry or attributing authorship, we perform supervised classification experiments with Random Forest Ensemble classifiers. ", "output": "What is the algorithm used for the classification tasks?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Next, we analyzed all tweets labeled as offensive to better understand how Arabic speakers use offensive language. Here is a breakdown of usage:\n\nDirect name calling: The most frequent attack is to call a person an animal name, and the most used animals were \u0643\u0644\u0628> (\u201cklb\u201d \u2013 \u201cdog\u201d), \u062d\u0645\u0627\u0631> (\u201cHmAr\u201d \u2013 \u201cdonkey\u201d), and \u0628\u0647\u064a\u0645> (\u201cbhym\u201d \u2013 \u201cbeast\u201d). The second most common was insulting mental abilities using words such as \u063a\u0628\u064a> (\u201cgby\u201d \u2013 \u201cstupid\u201d) and \u0639\u0628\u064a\u0637> (\u201cEbyT\u201d \u2013\u201cidiot\u201d). Some culture-specific differences should be considered. Not all animal names are used as insults. For example, animals such as \u0623\u0633\u062f> (\u201cAsd\u201d \u2013 \u201clion\u201d), \u0635\u0642\u0631> (\u201cSqr\u201d \u2013 \u201cfalcon\u201d), and \u063a\u0632\u0627\u0644> (\u201cgzAl\u201d \u2013 \u201cgazelle\u201d) are typically used for praise. For other insults, people use: some bird names such as \u062f\u062c\u0627\u062c\u0629> (\u201cdjAjp\u201d \u2013 \u201cchicken\u201d), \u0628\u0648\u0645\u0629> (\u201cbwmp\u201d \u2013 \u201cowl\u201d), and \u063a\u0631\u0627\u0628> (\u201cgrAb\u201d \u2013 \u201ccrow\u201d); insects such as \u0630\u0628\u0627\u0628\u0629> (\u201c*bAbp\u201d \u2013 \u201cfly\u201d), \u0635\u0631\u0635\u0648\u0631> (\u201cSrSwr\u201d \u2013 \u201ccockroach\u201d), and \u062d\u0634\u0631\u0629> (\u201cH$rp\u201d \u2013 \u201cinsect\u201d); microorganisms such as \u062c\u0631\u062b\u0648\u0645\u0629> (\u201cjrvwmp\u201d \u2013 \u201cmicrobe\u201d) and \u0637\u062d\u0627\u0644\u0628> (\u201cTHAlb\u201d \u2013 \u201calgae\u201d); inanimate objects such as \u062c\u0632\u0645\u0629> (\u201cjzmp\u201d \u2013 \u201cshoes\u201d) and \u0633\u0637\u0644> (\u201csTl\u201d \u2013 \u201cbucket\u201d) among other usages.\n\nSimile and metaphor: Users use simile and metaphor were they would compare a person to: an animal as in \u0632\u064a \u0627\u0644\u062b\u0648\u0631> (\u201czy Alvwr\u201d \u2013 \u201clike a bull\u201d), \u0633\u0645\u0639\u0646\u064a \u0646\u0647\u064a\u0642\u0643> (\u201csmEny nhyqk\u201d \u2013 \u201clet me hear your braying\u201d), and \u0647\u0632 \u062f\u064a\u0644\u0643> (\u201chz dylk\u201d \u2013 \u201cwag your tail\u201d); a person with mental or physical disability such as \u0645\u0646\u063a\u0648\u0644\u064a> (\u201cmngwly\u201d \u2013 \u201cMongolian (down-syndrome)\u201d), \u0645\u0639\u0648\u0642> (\u201cmEwq\u201d \u2013 \u201cdisabled\u201d), and \u0642\u0632\u0645> (\u201cqzm\u201d \u2013 \u201cdwarf\u201d); and to the opposite gender such as \u062c\u064a\u0634 \u0646\u0648\u0627\u0644> (\u201cjy$ nwAl\u201d \u2013 \u201cNawal's army (Nawal is female name)\u201d) and \u0646\u0627\u062f\u064a \u0632\u064a\u0632\u064a> (\u201cnAdy zyzy\u201d \u2013 \u201cZizi's club (Zizi is a female pet name)\u201d).\n\nIndirect speech: This type of offensive language includes: sarcasm such as \u0623\u0630\u0643\u0649 \u0625\u062e\u0648\u0627\u062a\u0643> (\u201cA*kY AxwAtk\u201d \u2013 \u201csmartest one of your siblings\u201d) and \u0641\u064a\u0644\u0633\u0648\u0641 \u0627\u0644\u062d\u0645\u064a\u0631> (\u201cfylswf AlHmyr\u201d \u2013 \u201cthe donkeys' philosopher\u201d); questions such as \u0627\u064a\u0647 \u0643\u0644 \u0627\u0644\u063a\u0628\u0627\u0621 \u062f\u0647> (\u201cAyh kl AlgbA dh\u201d \u2013 \u201cwhat is all this stupidity\u201d); and indirect speech such as \u0627\u0644\u0646\u0642\u0627\u0634 \u0645\u0639 \u0627\u0644\u0628\u0647\u0627\u064a\u0645 \u063a\u064a\u0631 \u0645\u062b\u0645\u0631> (\u201cAlnqA$ mE AlbhAym gyr mvmr\u201d \u2013 \u201cno use talking to cattle\u201d).\n\nWishing Evil: This entails wishing death or major harm to befall someone such as \u0631\u0628\u0646\u0627 \u064a\u0627\u062e\u062f\u0643> (\u201crbnA yAxdk\u201d \u2013 \u201cMay God take (kill) you\u201d), \u0627\u0644\u0644\u0647 \u064a\u0644\u0639\u0646\u0643> (\u201cAllh ylEnk\u201d \u2013 \u201cmay Allah/God curse you\u201d), and \u0631\u0648\u062d \u0641\u064a \u062f\u0627\u0647\u064a\u0629> (\u201crwH fy dAhyp\u201d \u2013 equivalent to \u201cgo to hell\u201d).\n\nName alteration: One common way to insult others is to change a letter or two in their names to produce new offensive words that rhyme with the original names. Some examples of such include changing \u0627\u0644\u062c\u0632\u064a\u0631\u0629> (\u201cAljzyrp\u201d \u2013 \u201cAljazeera (channel)\u201d) to \u0627\u0644\u062e\u0646\u0632\u064a\u0631\u0629> (\u201cAlxnzyrp\u201d \u2013 \u201cthe pig\u201d) and \u062e\u0644\u0641\u0627\u0646> (\u201cxlfAn\u201d \u2013 \u201cKhalfan (person name)\u201d) to \u062e\u0631\u0641\u0627\u0646> (\u201cxrfAn\u201d \u2013 \u201ccrazed\u201d).\n\nSocietal stratification: Some insults are associated with: certain jobs such as \u0628\u0648\u0627\u0628> (\u201cbwAb\u201d \u2013 \u201cdoorman\u201d) or \u062e\u0627\u062f\u0645> (\u201cxAdm\u201d \u2013 \u201cservant\u201d); and specific societal components such \u0628\u062f\u0648\u064a> (\u201cbdwy\u201d \u2013 \u201cbedouin\u201d) and \u0641\u0644\u0627\u062d> (\u201cflAH\u201d \u2013 \u201cfarmer\u201d).\n\nImmoral behavior: These insults are associated with negative moral traits or behaviors such as \u062d\u0642\u064a\u0631> (\u201cHqyr\u201d \u2013 \u201cvile\u201d), \u062e\u0627\u064a\u0646> (\u201cxAyn\u201d \u2013 \u201ctraitor\u201d), and \u0645\u0646\u0627\u0641\u0642> (\u201cmnAfq\u201d \u2013 \u201chypocrite\u201d).\n\nSexually related: They include expressions such as \u062e\u0648\u0644> (\u201cxwl\u201d \u2013 \u201cgay\u201d), \u0648\u0633\u062e\u0629> (\u201cwsxp\u201d \u2013 \u201cprostitute\u201d), and \u0639\u0631\u0635> (\u201cErS\u201d \u2013 \u201cpimp\u201d).", "output": "What are the distinctive characteristics of how Arabic speakers use offensive language?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We compare FSDM with four baseline methods and two ablations.\n\nNDM BIBREF7 proposes a modular end-to-end trainable network. It applies de-lexicalization on user utterances and responses.\n\nLIDM BIBREF9 improves over NDM by employing a discrete latent variable to learn underlying dialogue acts. This allows the system to be refined by reinforcement learning.\n\nKVRN BIBREF13 adopts a copy-augmented Seq2Seq model for agent response generation and uses an attention mechanism on the KB. It does not perform belief state tracking.\n\nTSCP/RL BIBREF10 is a two-stage CopyNet which consists of one encoder and two copy-mechanism-augmented decoders for belief state and response generation.", "output": "What baselines have been used in this work?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": ". A highly dynamic community constantly shifts interests from one time window to another, and these temporal variations are reflected in its use of volatile language. Formally, we define the dynamicity of a community INLINEFORM0 as the average volatility of all utterances in INLINEFORM1 . ", "output": "How do the authors measure how temporally dynamic a community is?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "To demonstrate our method in a black-box setting, we focus our experiments on Google's machine translation system (GMT), accessed through its Cloud API.", "output": "Which neural machine translation system is used?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Slot filling models are a useful method for simple natural language understanding tasks, where information can be extracted from a sentence and used to perform some structured action As candidate tasks, we consider the actions that a user might perform via apps on their phone. Crowd-sourced data was collected simulating common use cases for four different apps: United Airlines, Airbnb, Greyhound bus service and OpenTable. The corresponding actions are booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant.", "output": "What tasks are they experimenting with in this paper?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Deep convolutional neural networks (CNNs) with 2D convolutions and small kernels BIBREF1, have achieved state-of-the-art results for several speech recognition tasks BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6. r Models: Our baseline CNN model BIBREF21 consists of 15 convolutional and one fully-connected layer.", "output": "Is model compared against state of the art models on these datasets?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .\n\nSanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.\n\nHealth Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 . Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus.  For Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). ", "output": "Are results reported only on English datasets?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In our dataset, we store the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet. We created a list of the most common Arabic keywords associated with COVID-19. Using Twitter\u2019s streaming API, we searched for any tweet containing the keyword(s) in the text of the tweet. Table TABREF1 shows the list of keywords used along with the starting date of tracking each keyword. Furthermore, Table TABREF2 shows the list of hashtags we have been tracking along with the number of tweets collected from each hashtag. ", "output": "What additional information is found in the dataset?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Attention-base translation model We use the system of BIBREF6 , a convolutional sequence to sequence model.", "output": "Which translation system do they use to translate to English?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In this section, we empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method) in ROOT. We show that AD can drastically improve accuracy and performance of derivative evaluation, compared to ND.", "output": "How is correctness of automatic derivation proved?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "  In a second step, we will train a neural monolingual translation system, that translates from the output of the PBMT system INLINEFORM0 to a better target sentence INLINEFORM1 .", "output": "Do they train the NMT model on PBMT outputs?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We use the existing large-scale antonym and synonym pairs previously used by Nguyen:16. Originally, the data pairs were collected from WordNet BIBREF9 and Wordnik.", "output": "What dataset do they use to evaluate their method?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In this experiment set, our BERT implementation is compared to several systems that participated in the MEDDOCAN challenge: a CRF classifier BIBREF18, a spaCy entity recogniser BIBREF18, and NLNDE BIBREF12, the winner of the shared task and current state of the art for sensitive information detection and classification in Spanish clinical text.  However, attending to the obtained results, BERT remains only 0.3 F1-score points behind, and would have achieved the second position among all the MEDDOCAN shared task competitors. Taking into account that only 3% of the gold labels remain incorrectly annotated, the task can be considered almost solved, and it is not clear if the differences among the systems are actually significant, or whether they stem from minor variations in initialisation or a long-tail of minor labelling inconsistencies.", "output": "Does BERT reach the best performance among all the algorithms compared?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Differently to textual data, our goal in this paper is to explore the large amount of categorical data that is often collected in travel surveys. This includes trip purpose, education level, or family type. We also consider other variables that are not necessarily of categorical nature, but typically end up as dummy encoding, due to segmentation, such as age, income, or even origin/destination pair.", "output": "How do they model travel behavior?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Generic Sarcasm. We first examine the different patterns learned on the Gen dataset.  We observe that the not-sarcastic patterns appear to capture technical and scientific language, while the sarcastic patterns tend to capture subjective language that is not topic-specific. We observe an abundance of adjective and adverb patterns for the sarcastic class, although we do not use adjective and adverb patterns in our regex retrieval method. Rhetorical Questions. We notice that while the not-sarcastic patterns generated for RQs are similar to the topic-specific not-sarcastic patterns we find in the general dataset, there are some interesting features of the sarcastic patterns that are more unique to the RQs.\n\nMany of our sarcastic questions focus specifically on attacks on the mental abilities of the addressee. This generalization is made clear when we extract and analyze the verb, subject, and object arguments using the Stanford dependency parser BIBREF32 for the questions in the RQ dataset. Hyperbole. One common pattern for hyperbole involves adverbs and adjectives, as noted above. We did not use this pattern to retrieve hyperbole, but because each hyperbolic sarcastic utterance contains multiple cues, we learn an expanded class of patterns for hyperbole.  We learn a number of verbal patterns that we had not previously associated with hyperbole, as shown in Table TABREF34 . Interestingly, many of these instantiate the observations of CanoMora2009 on hyperbole and its related semantic fields: creating contrast by exclusion, e.g. no limit and no way, or by expanding a predicated class, e.g. everyone knows. Many of them are also contrastive. ", "output": "What are the linguistic differences between each class?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We adopt several standard metrics widely used in existing works to measure the performance of dialogue generation models, including BLEU BIBREF24, embedding-based metrics (Average, Extrema, Greedy and Coherence) BIBREF25, BIBREF26, entropy-based metrics (Ent-{1,2}) BIBREF0 and distinct metrics (Dist-{1,2,3} and Intra-{1,2,3}) BIBREF1, BIBREF6.", "output": "What automatic evaluation metrics are used?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "These models define parametrized similarity scoring functions $: Q\\times T\\rightarrow \\mathbb {R}$ , where $Q$ is the set of natural language questions and $T$ is the set of paraphrases of logical forms.", "output": "Does a neural scoring function take both the question and the logical form as inputs?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We compare the predictions of logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features.", "output": "What predictive model do they build?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Dialogue act (DA) characterizes the type of a speaker's intention in the course of producing an utterance and is approximately equivalent to the illocutionary act of BIBREF0 or the speech act of BIBREF1. The recognition of DA is essential for modeling and automatically detecting discourse structure, especially in developing a human-machine dialogue system. It is natural to predict the Answer acts following an utterance of type Question, and then match the Question utterance to each QA-pair in the knowledge base. The predicted DA can also guide the response generation process BIBREF2. For instance, system generates a Greeting type response to former Greeting type utterance. DA recognition is aimed to assign a label to each utterance in a conversation. It can be formulated as a supervised classification problem. There are two trends to solve this problem: 1) as a sequence labeling problem, it will predict the labels for all utterances in the whole dialogue history BIBREF13, BIBREF14, BIBREF9; 2) as a sentence classification problem, it will treat utterance independently without any context history BIBREF5, BIBREF15. ", "output": "What is dialogue act recognition?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "They used 20-million-words data randomly sampled from the raw text released by the CoNLL 2017 Shared Task - Automatically Annotated Raw Texts and Word Embeddings BIBREF8, which is a combination of Wikipedia dump and common crawl.  For example, we compared the Latvian model by ELMoForManyLangs with a model we trained on a complete (wikidump + common crawl) Latvian corpus, which has about 280 million tokens.", "output": "How larger are the training sets of these versions of ELMo compared to the previous ones?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "To implement the first tier of our framework and detect moral relevance, we complement our morally relevant seed words with a corresponding set of seed words approximating moral irrelevance based on the notion of valence, i.e., the degree of pleasantness or unpleasantness of a stimulus. We refer to the emotional valence ratings collected by BIBREF28 for approximately 14,000 English words, and choose the words with most neutral valence rating that do not occur in the MFD as our set of morally irrelevant seed words, for an equal total number of morally relevant and morally irrelevant words.", "output": "How do they quantify moral relevance?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Specifically, PlEWi supplied 550,755 [error, correction] pairs, from which 298,715 were unique.", "output": "How is PIEWi annotated?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "To test this hypothesis we first trained UG-WGAN in English, Chinese and German following the procedure described in Section \"UG-WGAN\" . For this experiment we trained UG-WGAN on the English and Russian language following the procedure described in Section \"UG-WGAN\" .  To do so we trained 6 UG-WGAN models on the following languages: English, Russian, Arabic, Chinese, German, Spanish, French.", "output": "What are the languages they consider in this paper?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The final annotation categories for the dataset are: Joy, Sadness, Anger, Fear, Anticipation, Surprise, Love, Disgust, Neutral", "output": "How many emotions do they look at?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "First preference is given to the labels that are perfectly matching in all the neural annotators. In Table TABREF11, we can see that both datasets have about 40% of exactly matching labels over all models (AM). Then priority is given to the context-based models to check if the label in all context models is matching perfectly. In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models. Then, we allow labels to rely on these at least two context models. As a result, about 47% of the labels are taken based on the context models (CM). When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. There are about 3% in IEMOCAP and 5% in MELD (BM).\n\nFinally, when none the above conditions are fulfilled, we leave out the label with an unknown category. This unknown category of the dialogue act is labeled with `xx' in the final annotations, and they are about 7% in IEMOCAP and 11% in MELD (NM).", "output": "How does the ensemble annotator extract the final label?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Traditional text-to-speech (TTS) systems are composed of complex pipelines BIBREF0 , these often include accoustic frontends, duration model, acoustic prediction model and vocoder models. Neural text-to-speech systems have garnered large research interest in the past 2 years. The first to fully explore this avenue of research was Google's tacotron BIBREF1 system The architecture of our model utilizes RNN-based Seq2Seq model for generating mel spectrogram from text. The architecture is similar to that of Tacotron 2 BIBREF4 .  Direct comparison of model parameters between ours and the open-source tacotron 2, our model contains 4.5 million parameters, whereas the Tacotron 2 contains around 13 million parameters with default setting", "output": "How do they measure the size of models?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "To further demonstrate the effectiveness of the additional task-specific BiRNN layers in our architecture, we conducted an ablation study using the CoNLL04 dataset. We trained and evaluated in the same manner described above, using the same hyperparameters, with the following exceptions:\n\nWe used either (i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero task-specific BiRNN layers of any kind.\n\nWe increased the number of shared BiRNN layers to keep the total number of model parameters consistent with the number of parameters in the baseline model.\n\nWe average the results for each set of hyperparameter across three trials with random weight initializations.", "output": "What were the variables in the ablation study?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We use the text corpora (S) from BIBREF6 aristo2016:combining to build our tuple KB.  Specifically, for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S. We take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$ to create the tuple KB (T). The Sentence corpus (S) consists of domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining. ", "output": "What corpus was the source of the OpenIE extractions?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Let all word vectors be normalized and $W$ be the word matrix. Inspired by BIBREF21 , where vector space models are used for evaluating topic coherence, we suggest to estimate the interpretability of $k$ th component as $ \\operatorname{interp}_k W = \\sum _{i,j=1}^N W_{i,k} W_{j,k} \\left(W_i \\cdot W_j \\right). $ ", "output": "How do they evaluate interpretability in this paper?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Our official scores (column Ens Test in Table TABREF19 ) have placed us second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard.", "output": "What were the scores of their system?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Modules: Modules are the basic building blocks of different models. In LeafNATS, we provide ready-to-use modules for constructing recurrent neural network (RNN)-based sequence-to-sequence (Seq2Seq) models for NATS, e.g., pointer-generator network BIBREF1 . These modules include embedder, RNN encoder, attention BIBREF24 , temporal attention BIBREF6 , attention on decoder BIBREF2 and others. We also use these basic modules to assemble a pointer-generator decoder module and the corresponding beam search algorithms. The embedder can also be used to realize the embedding-weights sharing mechanism BIBREF2 .", "output": "What models are included in the toolkit?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The dataset consists of queries and the corresponding image search results. Each token in each query is given a language tag based on the user-set home language of the user making the search on Google Images.", "output": "Do the images have multilingual annotations or monolingual ones?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "For every input data point and possible target class, LRP delivers one scalar relevance value per input variable, hereby indicating whether the corresponding part of the input is contributing for or against a specific classifier decision, or if this input variable is rather uninvolved and irrelevant to the classification task at all.", "output": "Does the LRP method work in settings that contextualize the words with respect to one another?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We use an off-the-shelf parser, in this case Stanford CoreNLP BIBREF11 , to create binary constituency parses.", "output": "How do they obtain parsed source sentences?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In the future, we aim to use the corpus for annotation projects such as parts-of-speech tagging, named entity recognition. Furthermore, the generated word embeddings will be utilized for the automatic construction of Sindhi WordNet.", "output": "Are trained word embeddings used for any other NLP task?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Therefore, we implement reinforcement learning and elaborately design two rewards to describe the irony accuracy and sentiment preservation, respectively.", "output": "What is the combination of rewards for reinforcement learning?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The informal setting/environment of social media often encourage multilingual speakers to switch back and forth between languages when speaking or writing. These all resulted in code-mixing and code-switching. Code-mixing refers to the use of linguistic units from different languages in a single utterance or sentence, whereas code-switching refers to the co-occurrence of speech extracts belonging to two different grammatical systemsBIBREF3. This language interchange makes the grammar more complex and thus it becomes tough to handle it by traditional algorithms. Thus the presence of high percentage of code-mixed content in social media text has increased the complexity of the aggression detection task. For example, the dataset provided by the organizers of TRAC-2018 BIBREF0, BIBREF2 is actually a code-mixed dataset.", "output": "What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "To verify our assumption that target encoding and orthogonal regularization help to boost the diversity of generated sequences, we use two metrics, one quantitative and one qualitative, to measure diversity of generation. First, we simply calculate the average unique predictions produced by both INLINEFORM0 and INLINEFORM1 in experiments shown in Section SECREF36 .  In this example there are 29 ground truth phrases. Neither of the models is able to generate all of the keyphrases, but it is obvious that the predictions from INLINEFORM0 all start with \u201ctest\u201d, while predictions from INLINEFORM1 are diverse.", "output": "How is keyphrase diversity measured?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Experiments were conducted using the AMI IHM meeting corpus BIBREF18 to evaluated the speech recognition performance of various language models. ", "output": "Which dataset do they use?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We collected a simplified dataset from Simple English Wikipedia that are freely available, which has been previously used for many text simplification methods BIBREF0 , BIBREF10 , BIBREF3 .", "output": "what language does this paper focus on?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Overall, this confirms our initial intuition that combining the best performing popularity-based approach with the best similarity-based approach should result in the highest accuracy (i.e., INLINEFORM7 for INLINEFORM8 ).", "output": "which algorithm was the highest performer?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We evaluate our proposed approach for joint sentiment and emotion analysis on the benchmark dataset of SemEval 2016 Task 6 BIBREF7 and Stance Sentiment Emotion Corpus (SSEC) BIBREF15.", "output": "What are the datasets used for training?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Compared with the BERT-BASE model, BERT-SPC significantly improves the accuracy and F1 score of aspect polarity classification.", "output": "How much better is performance of the proposed model compared to the state of the art in these various experiments?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We started from an initial corpus that did not have revisions, and kept adding new documents and revising existing documents. The revision operations we used were deletion, addition and replacement of words, sentences, paragraphs, section names and document titles. The addition of words, ..., section names, and new documents were pulled from the Wikipedia abstracts. We generated six data sets using different random seeds, and each data set contained six corpora (Corpus 0 - 5).", "output": "What are simulated datasets collected?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We evaluate the effectiveness of our LAN model on two typical knowledge graph completion tasks, i.e., link prediction and triplet classification.", "output": "Which knowledge graph completion tasks do they experiment with?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The results show that NCEL consistently outperforms various baselines with a favorable generalization ability.", "output": "How effective is their NCEL approach overall?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Various automated evaluation approaches are proposed to facilitate the development and evaluation of NLG models. We summarize these evaluation approaches below. Text Overlap Metrics, including BLEU BIBREF5, METEOR BIBREF6 and ROUGE BIBREF7, are the most popular metrics employed in the evaluation of NLG models. Perplexity is commonly used to evaluate the quality of a language model. Parameterized Metrics learn a parameterized model to evaluate generated text.", "output": "What previous automated evalution approaches authors mention?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": " Finally, when investigating the relatedness between European vs. non European languages (cf. (En/Fr)$\\rightarrow $Ar), we obtain similar results than those obtained in the monolingual experiment (macro F-score 62.4 vs. 68.0) and best results are achieved by Ar $\\rightarrow $(En/Fr). This shows that there are pragmatic devices in common between both sides and, in a similar way, similar text-based patterns in the narrative way of the ironic tweets.", "output": "Do the authors identify any cultural differences in irony use?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The joke corpus in this dataset contains thousands of unique jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc).", "output": "Where did the real production data come from?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Our conversational agent uses two architectures to simulate a specialized reminiscence therapist. The block in charge of generating questions is based on the work Show, Attend and Tell BIBREF13. This work generates descriptions from pictures, also known as image captioning.", "output": "Is machine learning system underneath similar to image caption ML systems?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In the sixth international workshop on Vietnamese Language and Speech Processing (VLSP 2019), the Hate Speech Detection (HSD) task is proposed as one of the shared-tasks to handle the problem related to controlling content in SNSs. The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type.", "output": "Is the data all in Vietnamese?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Our encoder-decoder framework employs separate encoding for different speakers in the dialog. User utterances $x_t^{usr}$ and system utterances $x_t^{sys}$ are fed into a user encoder and a system encoder separately to obtain encoder hidden states $h_{i}^{usr}$ and $h_{i}^{sys}$ . We integrate semantic slot scaffold by performing delexicalization on original dialogs. Delexicalization is a common pre-processing step in dialog modeling. We integrate dialog domain scaffold through a multi-task framework. Dialog domain indicates different conversation task content, for example, booking hotel, restaurant and taxi in MultiWOZ dataset.", "output": "How does SPNet utilize additional speaker role, semantic slot and dialog domain annotations?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We use the following three evaluation metrics:\n\nPhoneme Error Rate (PER) is the Levenshtein distance between the predicted phoneme sequences and the gold standard phoneme sequences, divided by the length of the gold standard phoneme sequences.\n\nWord Error Rate (WER) is the percentage of words in which the predicted phoneme sequence does not exactly match the gold standard phoneme sequence.\n\nWord Error Rate 100 (WER 100) is the percentage of words in the test set for which the correct guess is not in the first 100 guesses of the system.", "output": "what evaluation metrics were used?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "CNN can also be employed on the sarcasm datasets in order to identify sarcastic and non-sarcastic tweets. We term the features extracted from this network baseline features, the method as baseline method and the CNN architecture used in this baseline method as baseline CNN. Since the fully-connected layer has 100 neurons, we have 100 baseline features in our experiment. ", "output": "What are the network's baseline features?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "As the first step, we build three baseline LID systems, one based on the i-vector model, and the other two based on LSTM-RNN, using the speech data of two languages from Babel: Assamese and Georgian (AG). The two RNN LID baselines are: a standard RNN LID system (AG-RNN-LID) that discriminates between the two languages in its output, and a multi-task system (AG-RNN-MLT) that was trained to discriminate between the two languages as well as the phones.", "output": "Which is the baseline model?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "After removing 135 questions during the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%). ", "output": "what is the size of their dataset?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We perform experiments on document-level variants of the SQuAD dataset BIBREF1 .", "output": "What datasets have this method been evaluated on?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The stories from each scenario were distributed among four different annotators. ", "output": "How many subjects have been used to create the annotations?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We train language models on two languages: One model is estimated on the German newscrawl distributed by WMT'18 comprising 260M sentences or 6B tokens. Another model is trained on the English newscrawl data comprising 193M sentences or 5B tokens. We learn a joint Byte-Pair-Encoding (BPE; Sennrich et al., 2016) vocabulary of 37K types on the German and English newscrawl and train the language models with this vocabulary. We consider two benchmarks: Most experiments are run on the WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 English-Turkish (en-tr) news task. For WMT'18 English-German, the training corpus consists of all available bitext excluding the ParaCrawl corpus and we remove sentences longer than 250 tokens as well as sentence-pairs with a source/target length ratio exceeding 1.5. This results in 5.18M sentence pairs. We tokenize all data with the Moses tokenizer BIBREF8 and apply the BPE vocabulary learned on the monolingual corpora.", "output": "What dataset do they use?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "These actions consist of the following components:\n\n[leftmargin=*]\n\nCo-Reference Resolution: To support multi-turn interactions, it is sometimes necessary to use co-reference resolution techniques for effective retrieval. Query Generation: This component generates a query based on the past user-system interactions. Retrieval Model: This is the core ranking component that retrieves documents or passages from a large collection. Result Generation: The retrieved documents can be too long to be presented using some interfaces.", "output": "What are the different modules in Macaw?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "For our study, we used the number of retweets to single-out those that went viral within our sample. Tweets within that subset (viral tweets hereafter) are varied and relate to different topics. We consider that a tweet contains fake news if its text falls within any of the following categories described by Rubin et al. BIBREF7 (see next section for the details of such categories): serious fabrication, large-scale hoaxes, jokes taken at face value, slanted reporting of real facts and stories where the truth is contentious. The dataset BIBREF8 , manually labelled by an expert, has been publicly released and is available to researchers and interested parties.", "output": "How did they determine fake news tweets?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We end up with two salient roles called Anchors and Punctual speakers:\n\nthe Anchor speakers (A) are above the threshold of 1% for both criteria, meaning they are intervening often and for a long time thus holding an important place in interaction;\n\nthe Punctual speakers (PS) on the contrary are below the threshold of 1% for both the total number of turns and the total speech time.", "output": "How many categories do authors define for speaker role?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The BLEU metric is adopted to evaluate the model performance during evaluation.", "output": "What evaluation metric is used?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The method is evaluated on the BERTbase model, which has 12 layers, 12 self-attention heads with a hidden size of 768.\n\nTo accelerate the training speed, two-phase training BIBREF1 is adopted. The first phase uses a maximal sentence length of 128, and 512 for the second phase. The numbers of training steps of two phases are 50K and 40K for the BERTBase model. We used AdamW BIBREF13 optimizer with a learning rate of 1e-4, a $\\beta _1$ of 0.9, a $\\beta _2$ of 0.999 and a L2 weight decay rate of $0.01$. The first 10% of the total steps are used for learning rate warming up, followed by the linear decay schema. We used a dropout probability of 0.1 on all layers. The data used for pre-training is the same as BERT, i.e., English Wikipedia (2500M words) and BookCorpus (800M words) BIBREF14.", "output": "Do they train their model starting from a checkpoint?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "There are almost 1.1 million sentences in the collection. There are 119 different relation types (unique predicates), having from just a few relations to a few million relations.", "output": "How big is their dataset?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": " The dataset was split timewise into training/validation/test sets, and hyperparameters were optimized to maximize the AUC-ROC on the validation set. ", "output": "What evaluation metrics were used?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": " We performed the annotation with freely available tools for the Portuguese language.", "output": "Are the annotations automatic or manually created?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": " The overall performance of BERT was significantly better than the other models, having the lowest average difference in accuracy of 22.5 points.", "output": "Which model generalized the best?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "E2E NLG challenge Dataset: The training set of the E2E challenge dataset which consists of 42K samples was partitioned into a 10K paired and 32K unpaired datasets by a random process.  The Wikipedia Company Dataset: The Wikipedia company dataset presented in Section SECREF18 was filtered to contain only companies having abstracts of at least 7 words and at most 105 words. The dataset was then divided into: a training set (35K), a development set (4.3K) and a test set (4.3K). The training set was also partitioned in order to obtain the paired and unpaired datasets. ", "output": "What non-annotated datasets are considered?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We propose a new unsupervised model, Sent2Vec, for learning universal sentence embeddings Formally, we learn a source (or context) embedding INLINEFORM0 and target embedding INLINEFORM1 for each word INLINEFORM2 in the vocabulary, with embedding dimension INLINEFORM3 and INLINEFORM4 as in ( EQREF6 ). The sentence embedding is defined as the average of the source word embeddings of its constituent words, as in ( EQREF8 ). We augment this model furthermore by also learning source embeddings for not only unigrams but also n-grams present in each sentence, and averaging the n-gram embeddings along with the words, i.e., the sentence embedding INLINEFORM5 for INLINEFORM6 is modeled as DISPLAYFORM0", "output": "How do the n-gram features incorporate compositionality?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We initialized the embeddings of these words with 300 dimensional Glove embeddings BIBREF31 . ", "output": "Do they use pretrained embeddings?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": " We use the $\\textsc {BERT}_{\\textsc {BASE}}$ ensemble from BIBREF3 as the single-hop QA model. ", "output": "What off-the-shelf QA model was used to answer sub-questions?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "To ensure the universality and general applicability of the curriculum, we perform an in-depth investigation on three publicly available conversation corpora, PersonaChat BIBREF12, DailyDialog BIBREF13 and OpenSubtitles BIBREF7, consisting of 140 248, 66 594 and 358 668 real-life conversation samples, respectively.", "output": "What three publicly available coropora are used?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13.  From the corpus of about 100 million sentences, we obtained 1.4 millions event pairs for AL, 41 millions for CA, and 6 millions for CO.", "output": "How large is raw corpus used for training?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The task of attribute embedding lays is embedding every value in attribute triples into a continuous vector space while preserving the semantic information. To capture both high-order structural information of KGs, we used an attention-based embedding propagation method. The final embedding of entities, relations and values are feed into two different deep neural network for two different tasks including link predication and entity classification.", "output": "How does KANE capture both high-order structural and attribute information of KGs in an efficient, explicit and unified manner?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The third person plural pronoun `they' has no gender in English and most other languages (unlike the third person singular pronouns `he', `she', and `it').  If these sentences are translated into French, then `they' in the first sentence should be translated `elles', as referring to Jane and Susan, and `they' in the second sentence should be translated `ils', as referring to Fred and George. A couple of examples: The word `sie' in German serves as both the formal second person prounoun (always capitalized), the third person feminine singular, and the third person plural. ", "output": "What language do they explore?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The D2V model has been rated 80 times as \"bad relevance\" while the pmra returned only 24 times badly relevant documents. ", "output": "How better are results for pmra algorithm  than Doc2Vec in human evaluation? ", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We compare our model with other strong competitive methods on the SQuAD leaderboard and TriviaQA leaderboard.", "output": "What other solutions do they compare to?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We evaluate the quality of the document embeddings learned by MPAD on 10 document classification datasets, covering the topic identification, coarse and fine sentiment analysis and opinion mining, and subjectivity detection tasks. We briefly introduce the datasets next. Their statistics are reported in Table TABREF21.\n\n(1) Reuters. This dataset contains stories collected from the Reuters news agency in 1987. Following common practice, we used the ModApte split and considered only the 10 classes with the highest number of positive training examples. We also removed documents belonging to more than one class and then classes left with no document (2 classes).\n\n(2) BBCSport BIBREF30 contains documents from the BBC Sport website corresponding to 2004-2005 sports news articles.\n\n(3) Polarity BIBREF31 features positive and negative labeled snippets from Rotten Tomatoes.\n\n(4) Subjectivity BIBREF32 contains movie review snippets from Rotten Tomatoes (subjective sentences), and Internet Movie Database plot summaries (objective sentences).\n\n(5) MPQA BIBREF33 is made of positive and negative phrases, annotated as part of the summer 2002 NRRC Workshop on Multi-Perspective Question Answering.\n\n(6) IMDB BIBREF34 is a collection of highly polarized movie reviews from IMDB (positive and negative). There are at most 30 reviews for each movie.\n\n(7) TREC BIBREF35 consists of questions that are classified into 6 different categories.\n\n(8) SST-1 BIBREF36 contains the same snippets as Polarity. The authors used the Stanford Parser to parse the snippets and split them into multiple sentences. They then used Amazon Mechanical Turk to annotate the resulting phrases according to their polarity (very negative, negative, neutral, positive, very positive).\n\n(9) SST-2 BIBREF36 is the same as SST-1 but with neutral reviews removed and snippets classified as positive or negative.\n\n(10) Yelp2013 BIBREF26 features reviews obtained from the 2013 Yelp Dataset Challenge.", "output": "Which datasets are used?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions.\n\nResults showed that users who, on average, produced utterances with more words gave significantly higher ratings ($\\beta $=0.01, SE=0.002, t=4.79, p$<$0.001)(see Figure 2) and engaged with Gunrock for significantly greater number of turns ($\\beta $=1.85, SE=0.05, t=35.58, p$<$0.001) (see Figure 2). These results can be interpreted as evidence for Gunrock's ability to handle complex sentences, where users are not constrained to simple responses to be understood and feel engaged in the conversation \u2013 and evidence that individuals are more satisfied with the conversation when they take a more active role, rather than the system dominating the dialog. On the other hand, another interpretation is that users who are more talkative may enjoy talking to the bot in general, and thus give higher ratings in tandem with higher average word counts.", "output": "How do they correlate user backstory queries to user satisfaction?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We used the Twitter API to gather real-time tweets from September 2018 until February 2019, selecting the ones containing any of the 51 Hatebase terms that are more common in hate speech tweets, as studied in BIBREF9.", "output": "How is data collected, manual collection or Twitter api?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "SCA BIBREF5 softly augments a randomly chosen word in a sentence by its contextual mixture of multiple related words, i.e., replacing the one-hot representation of a word by a distribution provided by a language model over the vocabulary.", "output": "How does soft contextual data augmentation work?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Input of the model is the concatenation of word embedding and another embedding indicating whether this word is predicate: $ \\mathbf {x}_t = [\\mathbf {W}_{\\text{emb}}(w_t), \\mathbf {W}_{\\text{mask}}(w_t = v)]. $", "output": "What's the input representation of OpenIE tuples into the model?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In this paper, in spite of previous statements, we present a system that uses rule-based and dictionary-based methods combined (in a way we prefer to call resource-based). ", "output": "What does their system consist of?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Our data has been developed by crawling and pre-processing an OSG web forum. The forum has a great variety of different groups such as depression, anxiety, stress, relationship, cancer, sexually transmitted diseases, etc.", "output": "How did they obtain the OSG dataset?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese.", "output": "Is Arabic one of the 11 languages in CoVost?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The main contribution of our work is thus a new parallel data-to-text NLG corpus that (1) is more conversational, rather than information seeking or question answering, and thus more suitable for an open-domain dialogue system, (2) represents a new, unexplored domain which, however, has excellent potential for application in conversational agents, and (3) has high-quality, manually cleaned human-produced utterances.", "output": "How the authors made sure that corpus is clean despite being crowdsourced?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "They can answer each question with either `yes', `rather yes', `rather no', or `no'. They can supplement each answer with a comment of at most 500 characters.", "output": "What annotations are present in dataset?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Note that ESIM models have two LSTM layers, the first (input) LSTM performs the input encoding and the second (inference) LSTM generates the representation for inference.", "output": "How many layers are there in their model?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Exemplars aim to provide appropriate context. To better understand the context, we experimented by analysing the questions generated through an exemplar. We observed that indeed a supporting exemplar could identify relevant tags (cows in Figure FIGREF3 ) for generating questions.\n\nWe improve use of exemplars by using a triplet network. This network ensures that the joint image-caption embedding for the supporting exemplar are closer to that of the target image-caption and vice-versa.", "output": "How do the authors define exemplars?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Incorporating Document Features ::: Learning Phase ::: Feature Extraction ::: Document-unaware Features\nOrdinal position: It is shown that inclusion of sentence, in summary, is relevant to its position in the document or even in a paragraph. Intuitively, sentences at the beginning or the end of a text are more likely to be included in the summary. Depending on how it is defined, this feature might be document-unaware or not. For example, in BIBREF29 and BIBREF37 it is defined as $\\frac{5}{5}$ for the first sentence, $\\frac{4}{5}$ for the second, and so on to $\\frac{1}{5}$ for fifth and zero for remaining sentences. In another research conducted by Wong et al. BIBREF9, it is defined as $\\frac{1}{sentence\\ number}$. With such a definition, we may have several sentences, for example, with position=$\\frac{1}{5}$ in the training set, these may not have the same sense of position. While a sentence position=$\\frac{1}{5}$ means \u201camong the firsts\u201d in a document with 40 sentences, it has a totally different meaning of \u201cin the middle\u201d, in another document containing 10 sentences. Thus, a useful feature formula should involve differences of documents which may change the meaning of information within it. In our experiments, we used the definition of BIBREF9. A document-aware version of position will be introduced in (SECREF6).\n\nLength of sentence: the intuition behind this feature is that sentences of too long or too short length are less likely to be included in the summary. Like sentence position, this feature is also subject to the wrong definition that makes it document-unaware. For example, in BIBREF9 it is defined as a number of words in a sentence. Such a definition does not take into account that a sentence with, say 15 words may be considered long if all other sentences of document have fewer words. Another sentence with the same number of words may be regarded as short, because other sentences in that document have more than 15 words. This might occur due to different writing styles. However, we included this in our experiments to compare its effect with that of its document-aware counterpart, which will be listed in (SECREF6).\n\nThe Ratio of Nouns: is defined in BIBREF30 as the number of nouns divided by total number of words in the sentence, after stop-words are removed. Three other features, Ratio of Verbs, Ratio of Adjectives, and Ratio of Adverbs are defined in the same manner and proved to have a positive effect on ranking performance. From our perspective, however, a sentence with a ratio of nouns =0.5, for example, in a document containing many nouns, must be discriminated in the training set from another sentence with the same ratio of nouns, that appeared in another document having fewer nouns. This feature does not represent how many nouns are there in the document, which is important in sentence ranking. The same discussion goes on to justify the need to consider the number of verbs, adjectives, and adverbs in the document. The impact of these features is examined in our experiments and compared to that of their document-aware counterparts.\n\nThe Ratio of Numerical entities: assuming that sentences containing more numerical data are probably giving us more information, this feature may help us in ranking BIBREF31, BIBREF32. For calculation, we count the occurrences of numbers and digits proportional to the length of sentence. This feature must be less weighted if almost all sentences of a document have numerical data. However, it does not count numbers and digits in other sentences of the document.\n\nCue Words: if a sentence contains special phrases such as \u201cin conclusion\u201d, \u201coverall\u201d, \u201cto summarize\u201d, \u201cin a nutshell\u201d and so forth, its selection as a part of the summary is more probable than others. The number of these phrases is counted for this feature.\n\nIncorporating Document Features ::: Learning Phase ::: Feature Extraction ::: Document-aware Features\nCosine position: As mentioned in (SECREF5) a good definition of position should take into account document length. A well-known formula used in the literature BIBREF38, BIBREF7 is\n\nin which index is an integer representing the order of sentences and T is the total number of sentences in document. This feature ranges from 0 to 1, the closer to the beginning or to the end, the higher value this feature will take. $\\alpha $ is a tuning parameter. As it increases, the value of this feature will be distributed more equally over sentences. In this manner, equal values of this feature in the training set represent a uniform notion of position in a document, so it becomes document-aware.\n\nRelative Length: the intuition behind this feature is explained in (SECREF5). A discussion went there that a simple count of words does not take into account that a sentence with a certain number of words may be considered long or short, based on the other sentences appeared the document. Taking this into consideration, we divided the number of words in the sentence by the average length of sentences in the document. More formally, the formula is:\n\nin which n is number of sentences in the document and $s_i$ is the i\u2019th sentence of it. Values greater than 1 could be interpreted as long and vice versa.\n\nTF-ISF: this feature counts the frequency of terms in a document and assigns higher values to sentences having more frequent terms. It also discounts terms which appear in more sentences. Since it is well explained in the literature, we have not included details and formula which are in references BIBREF34 and BIBREF39. Nonetheless, the aspect that matters in our discussion is that both frequency and inverse sentence frequency are terms which involve properties of context, and consequently are document-aware.\n\nPOS features: Here we introduce another way to include the ratio of part of speech (POS) units in features and keep them document-normalized. To do this, the number of occurrences of each POS unit should be divided by the number of them in the document, instead of that occurring in a sentence. The formal definition of the new document-aware features are as follows:\n\nIncorporating Document Features ::: Learning Phase ::: Feature Extraction ::: Explicit Document Features\nIn order to further investigate how effective are document specific features in sentence ranking, we defined several features for documents. These features are then calculated for each document and repeated in the feature vector of every sentence of that document. Their formal definition is described below and their effect is examined in the result and discussion section (SECREF5):\n\nDocument sentences: An important property of a document that affects summarization is the total number of sentences participating in sentence ranking. As this number grows, a summarizer should be more selective and precise. Also, some sentence features such as cue words, maybe more weighted for longer documents. In addition, the main contextual information is probably more distributed over sentences. In such a case even lower values of other features should be considered important.\n\nDocument words: the number of words in the document is another notion of document length. Since the number of sentences alone is not enough to represent document length, this feature should also be considered.\n\nTopical category: different topics such as political, economic, etc. have different writing styles and this might affect sentence ranking. For instance, numerical entities may appear more in economic or sport reports than in religious or social news. Therefore the weight of this attribute should be more or less, based on a document\u2019s category. So it needs to be included.", "output": "What features of the document are integrated into vectors of every sentence?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "On inspection, the drop in performance between translated and original Japanese was often a result of translations that were reasonable but not consistent with the labels. For example, when translating the first example in Figure FIGREF2, both machine translations map \u201cUTF8min\u98a8\u90aa\u201d, which means cold (the illness), into \u201cUTF8min\u5bd2\u3055\u201d, which means cold (low temperature). Another example is where the Japanese pseudo-tweet \u201cUTF8min\u82b1\u7c89\u75c7\u306e\u6642\u671f\u306f\u3059\u3054\u3044\u75b2\u308c\u308b\u3002\u201d was provided alongside an English pseudo-tweet \u201cAllergy season is so exhausting.\u201d. Here, the Japanese word for hay fever \u201cUTF8min\u82b1\u7c89\u75c7\u3002\u201d has been manually mapped to the less specific word \u201callergies\u201d in English; the machine translation maps back to Japanese using the word for \u201callergies\u201d i.e. \u201cUTF8min\u30a2\u30ec\u30eb\u30ae\u30fc\u201d in the katakana alphabet (katakana is used to express words derived from foreign languages), since there is no kanji character for the concept of allergies.", "output": "Is there any explanation why some choice of language pair is better than the other?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We extend the Amazon Conversational Bot Toolkit (CoBot) BIBREF6 which is a flexible event-driven framework. CoBot provides ASR results and natural language processing pipelines through the Alexa Skills Kit (ASK) BIBREF7. Gunrock corrects ASR according to the context (asr) and creates a natural language understanding (NLU) (nlu) module where multiple components analyze the user utterances. A dialog manager (DM) (dm) uses features from NLU to select topic dialog modules and defines an individual dialog flow. Each dialog module leverages several knowledge bases (knowledge). Then a natural language generation (NLG) (nlg) module generates a corresponding response. Finally, we markup the synthesized responses and return to the users through text to speech (TTS) (tts).", "output": "What the system designs introduced?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Experimental Setup", "output": "which chinese datasets were used?, what english datasets were used?, What domains are detected in this paper?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In this subsection, we see the influence of each component of a model on performance by removing or replacing its components. the SNLI dataset is used for experiments, and the best performing configuration is used as a baseline for modifications. We consider the following variants: (i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0 , (iii) models without INLINEFORM1 , and (iv) models that integrate lower contexts via peephole connections.", "output": "What were the baselines?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Maximum matching (MM) is one of the most popular fundamental and structural segmentation algorithms for word segmentation BIBREF19 .  However, it does not solve the problem of ambiguous words and unknown words that do not exist in the dictionary.", "output": "What are the limitations of existing Vietnamese word segmentation systems?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Our corpus comprises a total of 6,127 scientific entities, including 2,112 Process, 258 Method, 2,099 Material, and 1,658 Data entities. ", "output": "How large is the dataset?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "For instance, we observed that winter words were together and far away from summer ones. Week days were grouped as well and far from week-end days.", "output": "What geometric properties do embeddings display?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Inspired by the supervised reordering in conventional SMT, in this paper, we propose a Supervised Attention based NMT (SA-NMT) model. Specifically, similar to conventional SMT, we first run off-the-shelf aligners (GIZA++ BIBREF3 or fast_align BIBREF4 etc.) to obtain the alignment of the bilingual training corpus in advance. Then, treating this alignment result as the supervision of attention, we jointly learn attention and translation, both in supervised manners. Since the conventional aligners delivers higher quality alignment, it is expected that the alignment in the supervised attention NMT will be improved leading to better end-to-end translation performance.", "output": "Which conventional alignment models do they use as guidance?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Advanced neural architectures based on character input (CNNs, BPE, etc) are supposed to be able to learn how to handle spelling and morphology variations themselves, even for languages with rich morphology: `just add more layers!'. Contextualised embedding models follow this tradition: as a rule, they are trained on raw text collections, with minimal linguistic pre-processing. Below, we show that this is not entirely true.", "output": "Why is lemmatization not necessary in English?", "category": "Question Generation 1088.json"}]