[{"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Among all available parameters to tune the D2V algorithm released by Gensim, six of them were selected for optimisation BIBREF14. The window_size parameter affects the size of the sliding window used to parse texts. The alpha parameter represents the learning rate of the network. The sample setting allows the model to reduce the importance given to high-frequency words. The dm parameter defines the training used architecture (PV-DM or PV-DBOW). The hs option defines whether hierarchical softmax or negative sampling is used during the training. Finally, the vector_size parameter affects the number of dimensions composing the resulting vector. \n Question: What six parameters were optimized with grid search?", "output": "window_size alpha sample dm hs vector_size", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Instead of providing an analytic approximation, AEM uses a discriminator network to discriminate between the reconstructed documents from latent events and the original input documents. This essentially helps the generator to construct a more realistic document from a random noise drawn from a Dirichlet distribution. Due to the flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions. And the supervision signal provided by the discriminator will help generator to capture the event-related patterns. \n Question: How does this model overcome the assumption that all words in a document are generated from a single event?", "output": "flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions supervision signal provided by the discriminator will help generator to capture the event-related patterns", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "As fig:fit shows, estimated test accuracy is highly correlated with actual test accuracy for various datasets, with worst-case values $\\mu <1\\%$ and $\\sigma <5\\%$ . Note that the number of free parameters is small ($||\\le 6$) compared to the number of points (42\u201349 model-data configurations), demonstrating the appropriateness of the proposed function for modeling the complex error landscape. \n Question: What is proof that proposed functional form approximates well generalization error in practice?", "output": "estimated test accuracy is highly correlated with actual test accuracy for various datasets appropriateness of the proposed function for modeling the complex error landscape", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Baseline Experiments\nIn this section, we briefly describe a baseline and evaluation scripts that we release, with a detailed documentation, along with the corpus. \n Question: What type of evaluation is proposed for this task?", "output": "Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "1. BERT based fine-tuning: In the first approach, which is shown in Figure FIGREF8, very few changes are applied to the BERTbase. In this architecture, only the [CLS] token output provided by BERT is used. The [CLS] output, which is equivalent to the [CLS] token output of the 12th transformer encoder, a vector of size 768, is given as input to a fully connected network without hidden layer. The softmax activation function is applied to the hidden layer to classify.\n\n2. Insert nonlinear layers: Here, the first architecture is upgraded and an architecture with a more robust classifier is provided in which instead of using a fully connected network without hidden layer, a fully connected network with two hidden layers in size 768 is used. The first two layers use the Leaky Relu activation function with negative slope = 0.01, but the final layer, as the first architecture, uses softmax activation function as shown in Figure FIGREF8.\n\n3. Insert Bi-LSTM layer: Unlike previous architectures that only use [CLS] as the input for the classifier, in this architecture all outputs of the latest transformer encoder are used in such a way that they are given as inputs to a bidirectional recurrent neural network (Bi-LSTM) as shown in Figure FIGREF8. After processing the input, the network sends the final hidden state to a fully connected network that performs classification using the softmax activation function.\n\n4. Insert CNN layer: In this architecture shown in Figure FIGREF8, the outputs of all transformer encoders are used instead of using the output of the latest transformer encoder. So that the output vectors of each transformer encoder are concatenated, and a matrix is produced. The convolutional operation is performed with a window of size (3, hidden size of BERT which is 768 in BERTbase model) and the maximum value is generated for each transformer encoder by applying max pooling on the convolution output. By concatenating these values, a vector is generated which is given as input to a fully connected network. By applying softmax on the input, the classification operation is performed. \n Question: What new fine-tuning methods are presented?", "output": "BERT based fine-tuning Insert nonlinear layers Insert Bi-LSTM layer Insert CNN layer", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Three annotators, who have no knowledge about which system the response is from, are then required to evaluate among win (response$_1$ is better), loss (response$_2$ is better) and tie (they are equally good or bad) independently, considering four aspects: coherence, logical consistency, fluency and diversity. \n Question: What human judgement metrics are used?", "output": "coherence, logical consistency, fluency and diversity", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin.  Experimental results on real-world dataset demonstrate that our proposed model competes favorably with strong baseline models in all three specific tasks. \n Question: What are the specific tasks being unified?", "output": " three types of questions, namely tumor size, proximal resection margin and distal resection margin", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The focus of this section is on recently published datasets and LID research applicable to the South African context. An in depth survey of algorithms, features, datasets, shared tasks and evaluation methods may be found in BIBREF0.\n\nThe datasets for the DSL 2015 & DSL 2017 shared tasks BIBREF1 are often used in LID benchmarks and also available on Kaggle . The recently published JW300 parallel corpus BIBREF2 covers over 300 languages with around 100 thousand parallel sentences per language pair on average. The WiLI-2018 benchmark dataset BIBREF4 for monolingual written natural language identification includes around 1000 paragraphs of 235 languages. The NCHLT text corpora BIBREF7 is likely a good starting point for a shared LID task dataset for the South African languages BIBREF8. \n Question: Which datasets are employed for South African languages LID?", "output": "DSL 2015 DSL 2017 JW300 parallel corpus  NCHLT text corpora", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In addition, we evaluate the effectiveness of pre-training by comparing it with a jointly-trained model of forward translation and back-translation. Experimental results show that the encoder-decoder-reconstructor offers significant improvement in BLEU scores and alleviates the problem of repeating and missing words in the translation on English-Japanese translation task, and the encoder-decoder-reconstructor can not be trained well without pre-training, so it proves that we have to train the forward translation model in a manner similar to the conventional attention-based NMT as pre-training. \n Question: Is pre-training effective in their evaluation?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Guidelines for Evaluating Faithfulness\nWe propose the following guidelines for evaluating the faithfulness of explanations. These guidelines address common pitfalls and sub-optimal practices we observed in the literature.\n\nGuidelines for Evaluating Faithfulness ::: Be explicit in what you evaluate.\nConflating plausability and faithfulness is harmful. You should be explicit on which one of them you evaluate, and use suitable methodologies for each one. Of course, the same applies when designing interpretation techniques\u2014be clear about which properties are being prioritized.\n\nGuidelines for Evaluating Faithfulness ::: Faithfulness evaluation should not involve human-judgement on the quality of interpretation.\nWe note that: (1) humans cannot judge if an interpretation is faithful or not: if they understood the model, interpretation would be unnecessary; (2) for similar reasons, we cannot obtain supervision for this problem, either. Therefore, human judgement should not be involved in evaluation for faithfulness, as human judgement measures plausability.\n\nGuidelines for Evaluating Faithfulness ::: Faithfulness evaluation should not involve human-provided gold labels.\nWe should be able to interpret incorrect model predictions, just the same as correct ones. Evaluation methods that rely on gold labels are influenced by human priors on what should the model do, and again push the evaluation in the direction of plausability.\n\nGuidelines for Evaluating Faithfulness ::: Do not trust \u201cinherent interpretability\u201d claims.\nInherent interpretability is a claim until proven otherwise. Explanations provided by \u201cinherently interpretable\u201d models must be held to the same standards as post-hoc interpretation methods, and be evaluated for faithfulness using the same set of evaluation techniques.\n\nGuidelines for Evaluating Faithfulness ::: Faithfulness evaluation of IUI systems should not rely on user performance.\nEnd-task user performance in HCI settings is merely indicative of correlation between plausibility and model performance, however small this correlation is. While important to evaluate the utility of the interpretations for some use-cases, it is unrelated to faithfulness. \n Question: Which are key points in guidelines for faithfulness evaluation?", "output": "Be explicit in what you evaluate. Faithfulness evaluation should not involve human-judgement on the quality of interpretation. Faithfulness evaluation should not involve human-provided gold labels. Do not trust \u201cinherent interpretability\u201d claims. Faithfulness evaluation of IUI systems should not rely on user performance.", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "While confirming the effectiveness of BT, our study also proposed significantly cheaper ways to improve the baseline performance, using a slightly modified copy of the target, instead of its full BT.  \n Question: why are their techniques cheaper to implement?", "output": "They use a slightly modified copy of the target to create the pseudo-text instead of full BT to make their technique cheaper", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "To adapt the FactorCell BIBREF4 for our purposes, we replace user embeddings with a low-dimensional image representation. Thus, we are able to modify each query completion to be personalized to a specific image representation. \n Question: How they complete a user query prefix conditioned upon an image?", "output": "we replace user embeddings with a low-dimensional image representation", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In this method, we simply concatenate the corpora of multiple domains with two small modifications: a. Appending the domain tag \u201c<2domain>\" to the source sentences of the respective corpora.  \n Question: How did they use the domain tags?", "output": "Appending the domain tag \u201c<2domain>\" to the source sentences of the respective corpora", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": " We then train a paragraph vector model using the Document to Vector (Doc2Vec) framework BIBREF7 on the whole set (13 million) of preprocessed text records, although training on smaller sets (1 million) also produces good results. \n Question: Which text embedding methodologies are used?", "output": "Document to Vector (Doc2Vec)", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We use the VisDial v1.0 BIBREF0 dataset to train our models, where one example has an image with its caption, 9 question-answer pairs, and follow-up questions and candidate answers for each round. At round $r$, the caption and the previous question-answer pairs become conversational context. The whole dataset is split into 123,287/2,000/8,000 images for train/validation/test, respectively. Unlike the images in the train and validation sets, the images in the test set have only one follow-up question and candidate answers and their corresponding conversational context. \n Question: How big is dataset for this challenge?", "output": "133,287 images", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Our contribution leverages pre-trained word embeddings (GloVe, trained on wikipedia+gigaword corpus), the DepecheMood affective lexicon, and convolutional neural networks. \n Question: What is the source of their lexicon?", "output": "DepecheMood", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Two datasets are exploited in this article. Both datasets consist of plain text containing clinical narrative written in Spanish, and their respective manual annotations of sensitive information in BRAT BIBREF13 standoff format. NUBes BIBREF4 is a corpus of around 7,000 real medical reports written in Spanish and annotated with negation and uncertainty information. In order to avoid confusion between the two corpus versions, we henceforth refer to the version relevant in this paper as NUBes-PHI (from `NUBes with Personal Health Information'). The organisers of the MEDDOCAN shared task BIBREF3 curated a synthetic corpus of clinical cases enriched with sensitive information by health documentalists \n Question: What are the clinical datasets used in the paper?", "output": "MEDDOCAN NUBes-PHI", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Despite the focus on sharing datasets and source codes on popular software development platforms such as GitHub (github.com) or Zenodo (zenodo.org), it is still a challenge to use data or code from other groups. \n Question: Are datasets publicly available?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We use a benchmark dataset created by ceccarelli2013learning from the CoNLL 2003 data.  \n Question: What is the benchmark dataset?", "output": "a benchmark dataset created by ceccarelli2013learning from the CoNLL 2003 data", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "One of the most important operations on a word is to obtain the set of words whose meaning is similar to the word, or whose usage in text is similar to the word. We call this set the neighbor of the word. We have observed that the neighbor of a polysemic word consists of words that resemble the primary sense of the polysemic word.  Even though a word may be a polysemic, it usually corresponds to a single vector in distributed representation. This vector is primarily determined by the major sense, which is most frequently used. The information about a word's minor sense is subtle, and the effect of a minor sense is difficult to distinguish from statistical fluctuation. To measure the effect of a minor sense, this paper proposes to use the concept of surrounding uniformity. The surrounding uniformity roughly corresponds to statistical fluctuation in the vectors that correspond to the words in the neighbor Surrounding Uniformity (SU) can be expressed as follows: $SU(\\vec{w}) = \\frac{|\\vec{s}(\\vec{w})|}{|\\vec{w}| + \\sum _{i}^{N}|\\vec{a_i}(\\vec{w})|}$\n\nwhere $\\vec{s}(\\vec{w}) = \\vec{w} + \\sum _{i}^{N} \\vec{a_i}(\\vec{w}).$ \n Question: How is the fluctuation in the sense of the word and its neighbors measured?", "output": "Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:\n1) Setting N, the size of the neighbor.\n2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.\n3) Computing the surrounding uniformity for ai(0 < i \u2264 N) and w.\n4) Computing the mean m and the sample variance \u03c3 for the uniformities of ai .\n5) Checking whether the uniformity of w is less than m \u2212 3\u03c3. If the value is less than m \u2212 3\u03c3, we may regard w as a polysemic word.", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In this study, we investigate the performance of our proposed models on WSJ BIBREF5 .  \n Question: Which dataset do they use?", "output": "WSJ", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": " For PHASE-ONE, we randomly shuffled and divided the data (1913 signals from 14 individuals) into train (80%), development (10%) and test sets (10%). In PHASE-TWO, in order to perform a fair comparison with the previous methods reported on the same dataset, we perform a leave-one-subject out cross-validation experiment using the best settings we learn from PHASE-ONE. \n Question: How many electrodes were used on the subject in EEG sessions?", "output": "1913 signals", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Clean-labeled Datasets. We use three clean labeled datasets. The first one is the movie sentence polarity dataset from BIBREF19. The other two datasets are laptop and restaurant datasets collected from SemEval-2016 . Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source. \n Question: What is the dataset used to train the model?", "output": " movie sentence polarity dataset from BIBREF19 laptop and restaurant datasets collected from SemEval-201 we collected 2,000 reviews for each domain from the same review source", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "For the cross-domain polarity classification experiments, we use the second version of Multi-Domain Sentiment Dataset BIBREF0 . The data set contains Amazon product reviews of four different domains: Books (B), DVDs (D), Electronics (E) and Kitchen appliances (K).  \n Question: What domains are contained in the polarity classification dataset?", "output": "Books DVDs Electronics Kitchen appliances", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In the first one, shown in Figure FIGREF12, a special dedicated discriminator is added to the model to control that the latent representation does not contain stylistic information. The second extension of the baseline architecture does not use an adversarial component $D_z$ that is trying to eradicate information on $c$ from component $z$. Instead, the system, shown in Figure FIGREF16 feeds the \"soft\" generated sentence $\\tilde{G}$ into encoder $E$ and checks how close is the representation $E(\\tilde{G} )$ to the original representation $z = E(x)$ in terms of the cosine distance. We further refer to it as shifted autoencoder or SAE. We also study a combination of both approaches described above, shown on Figure FIGREF17. \n Question: What are three new proposed architectures?", "output": "special dedicated discriminator is added to the model to control that the latent representation does not contain stylistic information shifted autoencoder or SAE combination of both approaches", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. \n Question: Where does the ancient Chinese dataset come from?", "output": "ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "category": "Question Answering 1087.json"}]