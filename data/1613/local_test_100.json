[{"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Without ELMo (the same setting as 4-th row in Table 3 ), our data settings is the same as qin-EtAl:2017:Long whose performance was state-of-the-art and will be compared directly. We see that even without the pre-trained ELMo encoder, our performance is better, which is mostly attributed to our better sentence pair representations.", "output": "Why does their model do better than prior models?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We have collected more than 3,934,610 million tweets so far.", "output": "How big is the dataset?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We have observed that the conflict model is very sensitive to even minor differences and compensates in such cases where attention poses high bias towards similarities already there in the sequences.\n\nSequence 1: What are the best ways to learn French ?\n\nSequence 2: How do I learn french genders ?\n\nAttention only: 1\n\nAttention+Conflict: 0\n\nGround Truth: 0\n\nSequence 1: How do I prevent breast cancer ?\n\nSequence 2: Is breast cancer preventable ?\n\nAttention only: 1\n\nAttention+Conflict: 0\n\nGround Truth: 0\n\nWe provide two examples with predictions from the models with only attention and combination of attention and conflict. Each example is accompanied by the ground truth in our data.", "output": "Do they show on which examples how conflict works better than attention?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "On the other hand, Go-Explore Seq2Seq shows promising results by solving almost half of the unseen games. Figure FIGREF62 (in Appendix SECREF60) shows that most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game. These results demonstrate both the relative effectiveness of training a Seq2Seq model on Go-Explore trajectories, but they also indicate that additional effort needed for designing reinforcement learning algorithms that effectively generalize to unseen games.", "output": "How do the authors show that their learned policy generalize better than existing solutions to unseen games?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In task 1, there are two top categories, namely, chit-chat and task-oriented dialogue.", "output": "How many intents were classified?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "For every gendered character in the dataset, we ask annotators to create a new character with a persona of the opposite gender that is otherwise identical except for referring nouns or pronouns.", "output": "In the targeted data collection approach, what type of data is targetted?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Experiments ::: Results ::: Natural Language Inference: XNLI\nOn the XNLI benchmark, CamemBERT obtains improved performance over multilingual language models on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM) while using less than half the parameters (110M vs. 250M). However, its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa. It should be noted that CamemBERT uses far fewer parameters than RoBERTa (110M vs. 355M parameters).", "output": "Which tasks does CamemBERT not improve on?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "To identify the most suitable classifier for classifying the scalars associated with each text, we perform evaluations using the stochastic gradient descent, naive bayes, decision tree, and random forest classifiers.", "output": "What was the baseline?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In CBOW architecture the task is predicting the word given its context and in SG the task in predicting the context given the word. We built 16 models of word embeddings using the implementation of CBOW and Skip-gram methods in the FastText tool BIBREF9 .", "output": "What is specific about the specific embeddings?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In this experiment, we compute the correlation of the proposed NED measure with the patient-perceived emotional bond ratings. Since the proposed measure is asymmetric in nature, we compute the measures for both patient-to-therapist and therapist-to-patient entrainment. We report Pearson's correlation coefficients ( INLINEFORM0 ) for this experiment in Table TABREF26 along with their INLINEFORM1 -values. ", "output": "How do they correlate NED with emotional bond levels?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "For instance, question and aux are correlated because main-clause questions in English require subject-aux inversion and in many cases the insertion of auxiliary do (Do lions meow?).  Expletives, or \u201cdummy\u201d arguments, are semantically inert arguments. The most common expletives in English are it and there, although not all occurrences of these items are expletives.", "output": "Do they report results only on English data?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Wav2vec BIBREF22 proposed a multi-layer convolutional neural network optimized via a noise contrastive binary classification and was applied to WSJ ASR tasks. Our experiments consisted of three different setups: 1) a fully-supervised system using all labeled data; 2) an SSL system using wav2vec features; 3) an SSL system using our proposed DeCoAR features. All models used were based on deep BLSTMs with the CTC loss criterion.", "output": "What are baseline models on WSJ eval92 and LibriSpeech test-clean?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In contrast to existing bottlenecks, this work targets three different types of social networks (Formspring: a Q&A forum, Twitter: microblogging, and Wikipedia: collaborative knowledge repository) for three topics of cyberbullying (personal attack, racism, and sexism) without doing any explicit feature engineering by developing deep learning based models along with transfer learning.", "output": "What cyberbulling topics did they address?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Because our conversion rules are interpretable, we identify shortcomings in both resources, using each as validation for the other. We were able to find specific instances of incorrectly applied UniMorph annotation, as well as specific instances of cross-lingual inconsistency in both resources.", "output": "Do they look for inconsistencies between different languages' annotations in UniMorph?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am. During annotation, we generally relied on categories and guidelines assembled by BBN Technologies for TREC 2002 question answering track.", "output": "did they use a crowdsourcing platform for manual annotations?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "GP-GNNs first build a fully-connected graph $\\mathcal {G} = (\\mathcal {V}, \\mathcal {E})$ , where $\\mathcal {V}$ is the set of entities, and each edge $(v_i, v_j) \\in \\mathcal {E}, v_i, v_j \\in \\mathcal {V}$ corresponds to a sequence $s = x_0^{i,j}, x_1^{i,j}, \\dots , x_{l-1}^{i,j}$ extracted from the text. ", "output": "So this paper turns unstructured text inputs to parameters that GNNs can read?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'.", "output": "What are the selection criteria for \"causal statements\"?, How do they extract causality from text?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The output of a system with the target words in the predicted order is compared to the gold ranking of the DURel data set. As the metric to assess how well the model's output fits the gold ranking Spearman's $\\rho $ was used. The higher Spearman's rank-order correlation the better the system's performance.", "output": "How is evaluation performed?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Input features are 40 mel-scaled log filterbank enegries (FBanks) computed every 10 ms with 25 ms window, concatenated with deltas and delta-deltas (120 features in vector). We also tried to use spectrogram and experimented with different normalization techniques.", "output": "What features do they experiment with?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "we use a common sense definition of racist language, including all negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture.", "output": "how did they ask if a tweet was racist?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The English online magazine of ISIS was named Dabiq and first appeared on the dark web on July 2014 and continued publishing for 15 issues. This publication was followed by Rumiyah which produced 13 English language issues through September 2017. Looking through both Dabiq and Rumiyah, many issues of the magazines contain articles specifically addressing women, usually with \u201c to our sisters \u201d incorporated into the title.", "output": "Do they report results only on English data?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "To identify legally sound answers, we recruit seven experts with legal training to construct answers to Turker questions. ", "output": "Who were the experts used for annotation?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "However, a significant bottleneck that has been overlooked by previous efforts is the input vocabulary size and its corresponding word embedding matrix, often accounting for a significant proportion of all model parameters. For instance, the embedding table of the BERTBASE model, comprising over 30K WordPiece tokens BIBREF14, accounts for over $21\\%$ of the model size. While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "output": "Why are prior knowledge distillation techniques models are ineffective in producing student models with vocabularies different from the original teacher models?  ", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Hate memes were retrieved from Google Images with a downloading tool. We used the following queries to collect a total of 1,695 hate memes: racist meme (643 memes), jew meme (551 memes), and muslim meme (501 Memes). Non-hate memes were obtained from the Reddit Memes Dataset .", "output": "What is the source of memes?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German\u2013English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1.", "output": "what are the methods they compare with in the korean-english dataset?, what were their experimental results in the low-resource dataset?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "We compare our proposed discrete CVAE (DCVAE) with the two-stage sampling approach to three categories of response generation models:\n\nBaselines: Seq2seq, the basic encoder-decoder model with soft attention mechanism BIBREF30 used in decoding and beam search used in testing; MMI-bidi BIBREF5, which uses the MMI to re-rank results from beam search.\n\nCVAE BIBREF14: We adjust the original work which is for multi-round conversation for our single-round setting. For a fair comparison, we utilize the same keywords used in our network pre-training as the knowledge-guided features in this model.\n\nOther enhanced encoder-decoder models: Hierarchical Gated Fusion Unit (HGFU) BIBREF12, which incorporates a cue word extracted using pointwise mutual information (PMI) into the decoder to generate meaningful responses; Mechanism-Aware Neural Machine (MANM) BIBREF13, which introduces latent embeddings to allow for multiple diverse response generation.", "output": "What other kinds of generation models are used in experiments?", "category": "Question Generation 1088.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and you have to write an answerable question based on the context. Your questions can be extractive, abstractive, or yes-no questions.", "input": "The minimal change operation substantially changed the meaning of the sentence, and yet the embedding of the transformation lies very closely to the original sentence (average similarity of 0.930).", "output": "Do they do any analysis of of how the modifications changed the starting set of sentences?", "category": "Question Generation 1088.json"}]