[{"instruction": "In this task, you are given a set of paper reviews in English and Spanish language. Based on given reviews, your job is to generate decision, i.e., \"accept\" or \"reject\" for the given paper. Note that URLs in the text have been replaced with [Link].", "input": "Review_1: Trabajo muy interesante pues incorpora una evaluaci\u00f3n con usuarios del prototipo propuesto. Est\u00e1 bien redactado y los resultados son claros. Review_2: Los autores presentan el trabajo de la construcci\u00f3n y evaluaci\u00f3n de una herramienta visual como ayuda al proceso de miner\u00eda de datos. Se muestra los resultados de utilizar la misma sobre un grupo de usuarios y sobre una tarea espec\u00edfica.  Sugerencias: - En las conclusiones se menciona la oportunidad de construir otras herramientas de apoyo, sin embargo, no se mencionan cu\u00e1les podr\u00edan ser. - No se menciona la raz\u00f3n de seleccionar WEKA como herramienta base para la construcci\u00f3n del software propuesto. - Se deber\u00eda agregar como anexo el detalle del experimento realizado, indicando el modelo utilizado, resultados obtenidos, etc.   Forma: - El segundo p\u00e1rrafo de la introducci\u00f3n no deja clara cu\u00e1l es la intenci\u00f3n del mismo, deber\u00eda ser redactado nuevamente. - Se hace uso extensivo de siglas, las cuales confunden y dificultan la lectura del trabajo. Review_3: El art\u00edculo propone y eval\u00faa una aplicaci\u00f3n desarrollada para representar de mejor forma los resultados de algoritmos de reglas de asociaci\u00f3n. En general el art\u00edculo me parece bien escrito y estructurado. Tal vez, para mejorar, la estructura del documento agregar\u00eda una secci\u00f3n especial dedicada a la metodolog\u00eda y describir\u00eda \u00e9sta m\u00e1s detalladamente. Los autores eval\u00faan cualitativamente el rendimiento de la interfaz en t\u00e9rminos de la facilidad de interpretaci\u00f3n de la informaci\u00f3n que entrega. Para ello exponen la informaci\u00f3n a un grupo de usuarios. El m\u00e9todo de evaluaci\u00f3n por usuarios es uno conocido y validado. No obstante, el art\u00edculo explica muy superficialmente el proceso de validaci\u00f3n de usuarios usado. Por otra parte, los resultados de la evaluaci\u00f3n presentados son s\u00f3lo descriptivos. Podr\u00eda haberse usado t\u00e9cnicas estad\u00edsticas de medici\u00f3n de consensos de int\u00e9rpretes como el \u00edndice de Kappa u otros para analizar la calidad de la coincidencia entre los evaluadores. En s\u00edntesis, me parece un buen art\u00edculo con una idea bien elaborada pero que metodol\u00f3gicamente deber\u00eda ser mejorado. ", "output": "accept", "category": "Paper Review 892.json"}, {"instruction": "In this task, you are given a set of paper reviews in English and Spanish language. Based on given reviews, your job is to generate decision, i.e., \"accept\" or \"reject\" for the given paper. Note that URLs in the text have been replaced with [Link].", "input": "Review_1: The paper describes an experience concerning the automated inspection of spectra for the Pipeline Hubble Legacy Archive Grism data.  Comments:  In the conclusions the authors say that \"We have identified two classes of flawed spectra which were not picked up by the automatic classification because of their very small number of training samples. Per construction machine learning techniques can not classify such outliers.\" This sentence should be further explained. As Hastie, Tibshirani, and Friedman say in The Elements of Statistical Learning (see Chapter 7): \"it is too difficult to give a general rule on how much training data is enough; among other things, this depends on the signal-to-noise ratio of the underlying function, and the complexity of the models being fit to the data. \" So, by just saying that the number of training samples is small you do not provide enough information to decide whether machine learning techniques are adequate or not to solve a problem.  Other minor comments:  + If reference [2] has not yet been written or published it should be deleted from the paper.  The dataset may be of scientific importance.  It is mainly descriptive. Review_2: This manuscript addresses an interesting solution based in machine learning techniques to classify spectra legacy data of the Hubble Space Telescope in order to publish the results and \"good\" spectra in Internet to scientific community. The manuscript is well-written and results are robust. Experiment is sound and the manuscript seems acceptable in the current form. The work can be short, but very interesting to the Infonor and JCC community.  The main problem of the paper is that it is difficult to compare results for the classifiers utilized. A confusion matrix and parameters for each classifier could clarify results. Review_3: Interesting application domain.  Nothing new from a machine learning perspective. They authors should provide more information about the models they have obtained with the different classifiers (feature subset selection used, selected features, accuracies, statistical significance on the differences on accuracies, the models themselves, ...). Semi-supervised classification is the appropriate method for the last paragraph in the \"Methodology\" section. ", "output": "accept", "category": "Paper Review 892.json"}]