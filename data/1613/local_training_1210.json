[{"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We derive bounds on the sample complexity of empirical risk minimization (ERM) in the context of minimizing non-convex risks that admit the strict saddle property. Recent progress in non-convex optimization has yielded efficient algorithms for minimizing such functions. Our results imply that these efficient algorithms are statistically stable and also generalize well. In particular, we derive fast rates which resemble the bounds that are often attained in the strongly convex setting. We specify our bounds to Principal Component Analysis and Independent Component Analysis. Our results and techniquesmay pave the way for statistical analyses of additional strict saddle problems.", "output": "Fast Rates for Empirical Risk Minimization of Strict Saddle Problems", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Deep neural networks trained on large supervised datasets have led to impressive results in recent years. However, since well-annotated datasets can be prohibitively expensive and time-consuming to collect, recent work has explored the use of larger but noisy datasets that can be more easily obtained. In this paper, we investigate the behavior of deep neural networks on training sets with massively noisy labels. We show that successful learning is possible even with an essentially arbitrary amount of noise. For example, on MNIST we find that accuracy of above 90 percent is still attainable even when the dataset has been diluted with 100 noisy examples for each clean example. Such behavior holds across multiple patterns of label noise, even when noisy labels are biased towards confusing classes. Further, we show how the required dataset size for successful training increases with higher label noise. Finally, we present simple actionable techniques for improving learning in the regime of high label noise.", "output": "Deep Learning is Robust to Massive Label Noise", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In a controlled experiment of sequence-tosequence approaches for the task of sentence correction, we find that characterbased models are generally more effective than word-based models and models that encode subword information via convolutions, and that modeling the output data as a series of diffs improves effectiveness over standard approaches. Our strongest sequence-to-sequence model improves over our strongest phrase-based statistical machine translation model, with access to the same data, by 6 M2 (0.5 GLEU) points. Additionally, in the data environment of the standard CoNLL-2014 setup, we demonstrate that modeling (and tuning against) diffs yields similar or better M2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches.", "output": "Adapting Sequence Models for Sentence Correction", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Tasks such as record linkage and multi-target tracking, which involve reconstructing the set of objects that underlie some observed data, are particularly challenging for probabilistic inference. Recent work has achieved efficient and accurate inference on such problems using Markov chain Monte Carlo (MCMC) techniques with customized proposal distributions. Currently, implementing such a system requires coding MCMC state representations and acceptance probability calculations that are specific to a particular application. An alternative approach, which we pursue in this paper, is to use a general-purpose probabilistic modeling language (such as BLOG) and a generic Metropolis-Hastings MCMC algorithm that supports user-supplied proposal distributions. Our algorithm gains flexibility by using MCMC states that are only partial descriptions of possible worlds; we provide conditions under which MCMC over partial worlds yields correct answers to queries. We also show how to use a context-specific Bayes net to identify the factors in the acceptance probability that need to be computed for a given proposed move. Experimental results on a citation matching task show that our general-purpose MCMC engine compares favorably with an application-specific system.", "output": "General-Purpose MCMC Inference over Relational Structures", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In this paper, we propose a method which uses semi-supervised convolutional neural networks (CNNs) to select in-domain training data for statistical machine translation. This approach is particularly effective when only tiny amounts of in-domain data are available. The in-domain data and randomly sampled general-domain data are used to train a data selection model with semi-supervised CNN, then this model computes domain relevance scores for all the sentences in the generaldomain data set. The sentence pairs with top scores are selected to train the system. We carry out experiments on 4 language directions with three test domains. Compared with strong baseline systems trained with large amount of data, this method can improve the performance up to 3.1 BLEU. Its performances are significant better than three state-of-the-art language model based data selection methods. We also show that the in-domain data used to train the selection model could be as few as 100 sentences, which makes finegrained topic-dependent translation adaptation possible.", "output": "Semi-supervised Convolutional Networks for Translation Adaptation with Tiny Amount of In-domain Data", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.", "output": "DEEP CHARACTER-LEVEL NEURAL MACHINE TRANSLATION BY LEARNING MORPHOLOGY", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "This paper focuses on unsupervised modeling of morphological families, collectively comprising a forest over the language vocabulary. This formulation enables us to capture edgewise properties reflecting single-step morphological derivations, along with global distributional properties of the entire forest. These global properties constrain the size of the affix set and encourage formation of tight morphological families. The resulting objective is solved using Integer Linear Programming (ILP) paired with contrastive estimation. We train the model by alternating between optimizing the local log-linear model and the global ILP objective. We evaluate our system on three tasks: root detection, clustering of morphological families and segmentation. Our experiments demonstrate that our model yields consistent gains in all three tasks compared with the best published results.1", "output": "Unsupervised Learning of Morphological Forests", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "This paper presents to the best of our knowledge the first end-to-end object tracking approach which directly maps from raw sensor input to object tracks in sensor space without requiring any feature engineering or system identification in the form of plant or sensor models. Specifically, our system accepts a stream of raw sensor data at one end and, in real-time, produces an estimate of the entire environment state at the output including even occluded objects. We achieve this by framing the problem as a deep learning task and exploit sequence models in the form of recurrent neural networks to learn a mapping from sensor measurements to object tracks. In particular, we propose a learning method based on a form of input dropout which allows learning in an unsupervised manner, only based on raw, occluded sensor data without access to ground-truth annotations. We demonstrate our approach using a synthetic dataset designed to mimic the task of tracking objects in 2D laser data \u2013 as commonly encountered in robotics applications \u2013 and show that it learns to track many dynamic objects despite occlusions and the presence of sensor noise.", "output": "Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Inspired by biological vision systems, the over-complete local features with huge cardinality are increasingly used for face recognition during the last decades. Accordingly, feature selection has become more and more important and plays a critical role for face data description and recognition. In this paper, we propose a trainable feature selection algorithm based on the regularized frame for face recognition. By enforcing a sparsity penalty term on the minimum squared error (MSE) criterion, we cast the feature selection problem into a combinatorial sparse approximation problem, which can be solved by greedy methods or convex relaxation methods. Moreover, based on the same frame, we propose a sparse Ho-Kashyap (HK) procedure to obtain simultaneously the optimal sparse solution and the corresponding margin vector of the MSE criterion. The proposed methods are used for selecting the most informative Gabor features of face images for recognition and the experimental results on benchmark face databases demonstrate the effectiveness of the", "output": "Feature Selection via Sparse Approximation for Face Recognition", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNN, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.", "output": "Learning Multiagent Communication with Backpropagation", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Retention of residual skills for persons who partially lose their cognitive or physical ability is of utmost importance. Research is focused on developing systems that provide need-based assistance for retention of such residual skills. This paper describes a novel cognitive collaborative control architecture CA, designed to address the challenges of developing needbased assistance for wheelchair navigation. Organization of CA is detailed and results from simulation of the proposed architecture is presented. For simulation of our proposed architecture, we have used ROS (Robot Operating System) as a control framework and a 3D robotic simulator called USARSim (Unified System for Automation and Robot Simulation).", "output": "C3A: A Cognitive Collaborative Control Architecture For an Intelligent Wheelchair", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We present a novel natural language generation system for spoken dialogue systems capable of entraining (adapting) to users\u2019 way of speaking, providing contextually appropriate responses. The generator is based on recurrent neural networks and the sequence-to-sequence approach. It is fully trainable from data which include preceding context along with responses to be generated. We show that the context-aware generator yields significant improvements over the baseline in both automatic metrics and a human pairwise preference test.", "output": "A Context-aware Natural Language Generator for Dialogue Systems", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Robots will eventually be part of every household. It is thus critical to enable algorithms to learn from and be guided by non-expert users. In this paper, we bring a human in the loop, and enable a human teacher to give feedback to a learning agent in the form of natural language. We argue that a descriptive sentence can provide a much stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts. We propose a hierarchical phrase-based captioning model trained with policy gradients, and design a feedback network that provides reward to the learner by conditioning on the human-provided feedback. We show that by exploiting descriptive feedback our model learns to perform better than when given independently written human captions.", "output": "Teaching Machines to Describe Images via Natural Language Feedback", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Two popular approaches for distributed training of SVMs on big data are parameter averaging and ADMM. Parameter averaging is efficient but suffers from loss of accuracy with increase in number of partitions, while ADMM in the feature space is accurate but suffers from slow convergence. In this paper, we report a hybrid approach called weighted parameter averaging (WPA), which optimizes the regularized hinge loss with respect to weights on parameters. The problem is shown to be same as solving SVM in a projected space. We also demonstrate an O( 1 N ) stability bound on final hypothesis given by WPA, using novel proof techniques. Experimental results on a variety of toy and real world datasets show that our approach is significantly more accurate than parameter averaging for high number of partitions. It is also seen the proposed method enjoys much faster convergence compared to ADMM in features space.", "output": "Distributed Weighted Parameter Averaging for SVM Training on Big Data", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Bayesian Optimisation (BO) is a technique used in optimising a D-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on D even though the function depends on all D dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive.", "output": "High Dimensional Bayesian Optimisation and Bandits via Additive Models", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Many intelligent user interfaces employ applica\u00ad tion and user models to determine the user's pref\u00ad erences, goals and likely future actions. Such models require application analysis, adaptation and expansion. Building and maintaining such models adds a substantial amount of time and labour to the application development cycle. We present a system that observes the interface of an unmodified application and records users' inter\u00ad actions with the application. From a history of such observations we build a coarse state space of observed interface states and actions between them. To refine the space, we hypothesize sub\u00ad states based upon the histories that led users to a given state. We evaluate the information gain of possible state splits, varying the length of the histories considered in such splits. In this way, we automatically produce a stochastic dynamic model of the application and of how it is used. To evaluate our approach, we present models de\u00ad rived from real-world application usage data.", "output": "Building a Stochastic Dynamic Model of Application Use", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The field of speech recognition is in the midst of a paradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology. Using an attention mechanism in a recurrent encoder-decoder architecture solves the dynamic time alignment problem, allowing joint end-to-end training of the acoustic and language modeling components. In this paper we extend the end-to-end framework to encompass microphone array signal processing for noise suppression and speech enhancement within the acoustic encoding network. This allows the beamforming components to be optimized jointly within the recognition architecture to improve the end-to-end speech recognition objective. Experiments on the noisy speech benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system outperformed the attention-based baseline with input from a conventional adaptive beamformer.", "output": "Multichannel End-to-end Speech Recognition ", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Text Classification is a challenging and a red hot field in the current scenario and has great importance in text categorization applications. A lot of research work has been done in this field but there is a need to categorize a collection of text documents into mutually exclusive categories by extracting the concepts or features using supervised learning paradigm and different classification algorithms. In this paper, a new Fuzzy Similarity Based Concept Mining Model (FSCMM) is proposed to classify a set of text documents into pre defined Category Groups (CG) by providing them training and preparing on the sentence, document and integrated corpora levels along with feature reduction, ambiguity removal on each level to achieve high system performance. Fuzzy Feature Category Similarity Analyzer (FFCSA) is used to analyze each extracted feature of Integrated Corpora Feature Vector (ICFV) with the corresponding categories or classes. This model uses Support Vector Machine Classifier (SVMC) to classify correctly the training data patterns into two groups; i. e., + 1 and \u2013 1, thereby producing accurate and correct results. The proposed model works efficiently and effectively with great performance and high accuracy results. Keywords-Text Classification; Natural Language Processing; Feature Extraction; Concept Mining; Fuzzy Similarity Analyzer; Dimensionality Reduction; Sentence Level; Document Level; Integrated Corpora Level Processing.", "output": "A Fuzzy Similarity Based Concept Mining Model for Text Classification", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "One of the main challenges in Grid systems is designing an adaptive, scalable, and model-independent method for job scheduling to achieve a desirable degree of load balancing and system efficiency. Centralized job scheduling methods have some drawbacks, such as single point of failure and lack of scalability. Moreover, decentralized methods require a coordination mechanism with limited communications. In this paper, we propose a multi-agent approach to job scheduling in Grid, named Centralized Learning Distributed Scheduling (CLDS), by utilizing the reinforcement learning framework. The CLDS is a model free approach that uses the information of jobs and their completion time to estimate the efficiency of resources. In this method, there are a learner agent and several scheduler agents that perform the task of learning and job scheduling with the use of a coordination strategy that maintains the communication cost at a limited level. We evaluated the efficiency of the CLDS method by designing and performing a set of experiments on a simulated Grid system under different system scales and loads. The results show that the CLDS can effectively balance the load of system even in large scale and heavy loaded Grids, while maintains its adaptive performance and scalability.", "output": "A centralized reinforcement learning method for multi-agent job scheduling in Grid", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We present a system for recognising human activity given a symbolic representation of video content. The input of our system is a set of time-stamped short-term activities (STA) detected on video frames. The output is a set of recognised long-term activities (LTA), which are pre-defined temporal combinations of STA. The constraints on the STA that, if satisfied, lead to the recognition of a LTA, have been expressed using a dialect of the Event Calculus. In order to handle the uncertainty that naturally occurs in human activity recognition, we adapted this dialect to a state-of-the-art probabilistic logic programming framework. We present a detailed evaluation and comparison of the crisp and probabilistic approaches through experimentation on a benchmark dataset of human surveillance videos.", "output": "A Probabilistic Logic Programming Event Calculus", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We present a neural architecture for sequence processing. The ByteNet is a stack of two dilated convolutional neural networks, one to encode the source sequence and one to decode the target sequence, where the target network unfolds dynamically to generate variable length outputs. The ByteNet has two core properties: it runs in time that is linear in the length of the sequences and it preserves the sequences\u2019 temporal resolution. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent neural networks. The ByteNet also achieves a performance on raw character-level machine translation that approaches that of the best neural translation models that run in quadratic time. The implicit structure learnt by the ByteNet mirrors the expected alignments between the sequences.", "output": "Neural Machine Translation in Linear Time", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The advent of Web 2.0 has led to an increase in the amount of sentimental content available in the Web. Such content is often found in social media web sites in the form of movie or product reviews, user comments, testimonials, messages in discussion forums etc. Timely discovery of the sentimental or opinionated web content has a number of advantages, the most important of all being monetization. Understanding of the sentiments of human masses towards different entities and products enables better services for contextual advertisements, recommendation systems and analysis of market trends. The focus of our project is sentiment focussed web crawling framework to facilitate the quick discovery of sentimental contents of movie reviews and hotel reviews and analysis of the same. We use statistical methods to capture elements of subjective style and the sentence polarity. The paper elaborately discusses two supervised machine learning algorithms: K-Nearest Neighbour(K-NN) and Na\u00efve Bayes\u2019 and compares their overall accuracy, precisions as well as recall values. It was seen that in case of movie reviews Na\u00efve Bayes\u2019 gave far better results than K-NN but for hotel reviews these algorithms gave lesser, almost same", "output": "Sentiment Analysis of Review Datasets using Nai\u0308ve Bayes\u2019 and K-NN Classifier", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In this work we present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives state-of-the-art results for both.", "output": "Soft-to-Hard Vector Quantization for End-to-End Learned Compression of Images and Neural Networks", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task.", "output": "A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Indefinite similarity measures can be frequently found in bio-informatics by means of alignment scores, but are also common in other fields like shape measures in image retrieval. Lacking an underlying vector space, the data are given as pairwise similarities only. The few algorithms available for such data do not scale to larger datasets. Focusing on probabilistic batch classifiers, the Indefinite Kernel Fisher Discriminant (iKFD) and the Probabilistic Classification Vector Machine (PCVM) are both effective algorithms for this type of data but, with cubic complexity. Here we propose an extension of iKFD and PCVM such that linear runtime and memory complexity is achieved for low rank indefinite kernels. Employing the Nystr\u00f6m approximation for indefinite kernels, we also propose a new almost parameter free approach to identify the landmarks, restricted to a supervised learning problem. Evaluations at several larger similarity data from various domains show that the proposed methods provides similar generalization capabilities while being easier to parametrize and substantially faster for large scale data.", "output": "Probabilistic classifiers with low rank indefinite kernels", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The holy Quran is the holy book of the Muslims. It contains information about many domains. Often people search for particular concepts of holy Quran based on the relations among concepts. An ontological modeling of holy Quran can be useful in such a scenario. In this paper, we have modeled nature related concepts of holy Quran using OWL (Web Ontology Language) / RDF (Resource Description Framework). Our methodology involves identifying nature related concepts mentioned in holy Quran and identifying relations among those concepts. These concepts and relations are represented as classes/instances and properties of an OWL ontology. Later, in the result section it is shown that, using the Ontological model, SPARQL queries can retrieve verses and concepts of interest. Thus, this modeling helps semantic search and query on the holy Quran. In this work, we have used English translation of the holy Quran by Sahih International, Protege OWL Editor and for querying we have used SPARQL. Keywords\u2014 Quranic Ontology; Semantic Quran; Quranic Knowledge Representation.", "output": "Applying Ontological Modeling on Quranic \u201cNature\u201d Domain", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Neural networks are capable of learning rich, nonlinear feature representations shown to be beneficial in many predictive tasks. In this work, we use these models to explore the use of geographical features in predicting colorectal cancer survival curves for patients in the state of Iowa, spanning the years 1989 to 2012. Specifically, we compare model performance using a newly defined metric \u2013 area between the curves (ABC) \u2013 to assess (a) whether survival curves can be reasonably predicted for colorectal cancer patients in the state of Iowa, (b) whether geographical features improve predictive performance, and (c) whether a simple binary representation or richer, spectral clustering-based representation perform better. Our findings suggest that survival curves can be reasonably estimated on average, with predictive performance deviating at the five-year survival mark. We also find that geographical features improve predictive performance, and that the best performance is obtained using richer, spectral analysis-elicited features.", "output": "Learning Rich Geographical Representations: Predicting Colorectal Cancer Survival in the State of Iowa", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We first show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both taskspecific and static word vectors. The CNN models discussed herein improve upon the state-of-the-art on 4 out of 7 tasks, which include sentiment analysis and question classification.", "output": "Convolutional Neural Networks for Sentence Classification", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Recommender systems often use latent features to explain the behaviors of users and capture the properties of items. As users interact with different items over time, user and item features can influence each other, evolve and co-evolve over time. To accurately capture the fine grained nonlinear coevolution of these features, we propose a recurrent coevolutionary feature embedding process model, which combines recurrent neural network (RNN) with a multidimensional point process model. The RNN learns a nonlinear representation of user and item features which take into account mutual influence between user and item features, and the feature evolution over time. We also develop an efficient stochastic gradient algorithm for learning the model parameters, which can readily scale up to millions of events. Experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts.", "output": "Recurrent Coevolutionary Feature Embedding Processes for Recommendation", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "This paper introduces a new computing model based on the cooperation among Turing machines called orchestrated machines. Like universal Turing machines, orchestrated machines are also designed to simulate Turing machines but they can also modify the original operation of the included Turing machines to create a new layer of some kind of collective behavior. Using this new model we can define some interested notions related to cooperation ability of Turing machines such as the intelligence quotient or the emotional intelligence quotient for Turing machines.", "output": "Are there intelligent Turing machines?", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We study the generalization properties of stochastic gradient methods for learning with convex loss functions and linearly parameterized functions. We show that, in the absence of penalizations or constraints, the stability and approximation properties of the algorithm can be controlled by tuning either the step-size or the number of passes over the data. In this view, these parameters can be seen to control a form of implicit regularization. Numerical results complement the theoretical findings.", "output": "Generalization Properties and Implicit Regularization for Multiple Passes SGM", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Federated Learning is a machine learning setting where the goal is to train a highquality centralized model with training data distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of utmost importance. In this paper, we propose two ways to reduce the uplink communication costs. The proposed methods are evaluated on the application of training a deep neural network to perform image classification. Our best approach reduces the upload communication required to train a reasonable model by two orders of magnitude.", "output": "Federated Learning: Strategies for Improving Communication Efficiency", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We investigate the direct-sum problem in the context of differentially private PAC learning: What<lb>is the sample complexity of solving k learning tasks simultaneously under differential privacy, and how<lb>does this cost compare to that of solving k learning tasks without privacy? In our setting, an individual<lb>example consists of a domain element x labeled by k unknown concepts (c1, . . . ,<lb>ck). The goal of a<lb>multi-learner is to output k hypotheses (h1, . . . , hk) that generalize the input examples.<lb>Without concern for privacy, the sample complexity needed to simultaneously learn k concepts is<lb>essentially the same as needed for learning a single concept. Under differential privacy, the basic strategy<lb>of learning each hypothesis independently yields sample complexity that grows polynomially with k.<lb>For some concept classes, we give multi-learners that require fewer samples than the basic strategy.<lb>Unfortunately, however, we also give lower bounds showing that even for very simple concept classes,<lb>the sample cost of private multi-learning must grow polynomially in k.", "output": "Simultaneous Private Learning of Multiple Concepts", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Motivated by concerns for user privacy, we design a steganographic system (\u201cstegosystem\u201d) that enables two users to exchange encrypted messages without an adversary detecting that such an exchange is taking place. We propose a new linguistic stegosystem based on a Long ShortTerm Memory (LSTM) neural network. We demonstrate our approach on the Twitter and Enron email datasets and show that it yields high-quality steganographic text while significantly improving capacity (encrypted bits per word) relative to the state-of-the-art.", "output": "Generating Steganographic Text with LSTMs", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The study of social networks is a burgeoning research area. However, most existing work deals with networks that simply encode whether relationships exist or not. In contrast, relationships in signed networks can be positive (\u201clike\u201d, \u201ctrust\u201d) or negative (\u201cdislike\u201d, \u201cdistrust\u201d). The theory of social balance shows that signed networks tend to conform to some local patterns that, in turn, induce certain global characteristics. In this paper, we exploit both local as well as global aspects of social balance theory for two fundamental problems in the analysis of signed networks: sign prediction and clustering. Motivated by local patterns of social balance, we first propose two families of sign prediction methods: measures of social imbalance (MOIs), and supervised learning using high order cycles (HOCs). These methods predict signs of edges based on triangles and l-cycles for relatively small values of l. Interestingly, by examining measures of social imbalance, we show that the classic Katz measure, which is used widely in unsigned link prediction, actually has a balance theoretic interpretation when applied to signed networks. Furthermore, motivated by the global structure of balanced networks, we propose an effective low rank modeling approach for both sign prediction and clustering. For the low rank modeling approach, we provide theoretical performance guarantees via convex relaxations, scale it up to large problem sizes using a matrix factorization based algorithm, and provide extensive experimental validation including comparisons with local approaches. Our experimental results indicate that, by adopting a more global viewpoint of balance structure, we get significant performance and computational gains in prediction and clustering tasks on signed networks. Our work therefore highlights the usefulness of the global aspect of balance theory for the analysis of signed networks.", "output": "Prediction and Clustering in Signed Networks: A Local to Global Perspective", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We propose a simple, scalable, fully generative model for transition-based dependency parsing with high accuracy. The model, parameterized by Hierarchical Pitman-Yor Processes, overcomes the limitations of previous generative models by allowing fast and accurate inference. We propose an efficient decoding algorithm based on particle filtering that can adapt the beam size to the uncertainty in the model while jointly predicting POS tags and parse trees. The UAS of the parser is on par with that of a greedy discriminative baseline. As a language model, it obtains better perplexity than a n-gram model by performing semi-supervised learning over a large unlabelled corpus. We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation.", "output": "A Bayesian Model for Generative Transition-based Dependency Parsing", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Despite their success, convolutional neural networks are computationally expensive because they must examine all image locations. Stochastic attention-based models have been shown to improve computational efficiency at test time, but they remain difficult to train because of intractable posterior inference and high variance in the stochastic gradient estimates. Borrowing techniques from the literature on training deep generative models, we present the Wake-Sleep Recurrent Attention Model, a method for training stochastic attention networks which improves posterior inference and which reduces the variability in the stochastic gradients. We show that our method can greatly speed up the training time for stochastic attention networks in the domains of image classification and caption generation.", "output": "Learning Wake-Sleep Recurrent Attention Models", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Stochastic gradient descent (SGD) is a standard optimization method to minimize a training error with respect to network parameters in modern neural network learning. However, it typically suffers from proliferation of saddle points in the high-dimensional parameter space. Therefore, it is highly desirable to design an efficient algorithm to escape from these saddle points and reach a parameter region of better generalization capabilities. Here, we propose a simple extension of SGD, namely reinforced SGD, which simply adds previous first-order gradients in a stochastic manner with a probability that increases with learning time. As verified in a simple synthetic dataset, this method significantly accelerates learning compared with the original SGD. Surprisingly, it dramatically reduces over-fitting effects, even compared with state-of-the-art adaptive learning algorithm\u2014Adam. For a benchmark handwritten digits dataset, the learning performance is comparable to Adam, yet with an extra advantage of requiring one-fold less computer memory. The reinforced SGD is also compared with SGD with fixed or adaptive momentum parameter and Nesterov\u2019s momentum, which shows that the proposed framework is able to reach a similar generalization accuracy with less computational costs. Overall, our method introduces stochastic memory into gradients, which plays an important role in understanding how gradient-based training algorithms can work and its relationship with generalization abilities of deep networks.", "output": "Reinforced stochastic gradient descent for deep neural network learning", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Learning a good representation of text is key to many recommendation applications. Examples include news recommendation where texts to be recommended are constantly published everyday. However, most existing recommendation techniques, such as matrix factorization based methods, mainly rely on interaction histories to learn representations of items. While latent factors of items can be learned e\u0082ectively from user interaction data, in many cases, such data is not available, especially for newly emerged items. In this work, we aim to address the problem of personalized recommendation for completely new items with text information available. We cast the problem as a personalized text ranking problem and propose a general framework that combines text embedding with personalized recommendation. Users and textual content are embedded into latent feature space. \u008ce text embedding function can be learned end-to-end by predicting user interactions with items. To alleviate sparsity in interaction data, and leverage large amount of text data with li\u008ale or no user interactions, we further propose a joint text embedding model that incorporates unsupervised text embedding with a combination module. Experimental results show that our model can signi\u0080cantly improve the e\u0082ectiveness of recommendation systems on real-world datasets.", "output": "Joint Text Embedding for Personalized Content-based Recommendation", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We present two novel and contrasting Recurrent Neural Network (RNN) based architectures for extractive summarization of documents. The Classifier based architecture sequentially accepts or rejects each sentence in the original document order for its membership in the final summary. The Selector architecture, on the other hand, is free to pick one sentence at a time in any arbitrary order to piece together the summary. Our models under both architectures jointly capture the notions of salience and redundancy of sentences. In addition, these models have the advantage of being very interpretable, since they allow visualization of their predictions broken up by abstract features such as information content, salience and redundancy. We show that our models reach or outperform state-of-the-art supervised models on two different corpora. We also recommend the conditions under which one architecture is superior to the other based on experimental evidence.", "output": "EXTRACTIVE DOCUMENT SUMMARIZATION", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In this paper, we propose a novel multi-label learning framework, called Multi-Label Self-Paced Learning (MLSPL), in an attempt to incorporate the self-paced learning strategy into multi-label learning regime. In light of the benefits of adopting the easy-to-hard strategy proposed by self-paced learning, the devised MLSPL aims to learn multiple labels jointly by gradually including label learning tasks and instances into model training from the easy to the hard. We first introduce a self-paced function as a regularizer in the multi-label learning formulation, so as to simultaneously rank priorities of the label learning tasks and the instances in each learning iteration. Considering that different multi-label learning scenarios often need different self-paced schemes during optimization, we thus propose a general way to find the desired self-paced functions. Experimental results on three benchmark datasets suggest the state-of-the-art performance of our approach.", "output": "A Self-Paced Regularization Framework for Multi-Label Learning", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We propose a Multi-Layer Network based on the Bayesian framework of the Factor Graphs in Reduced Normal Form (FGrn) applied to a two-dimensional lattice. The Latent Variable Model (LVM) is the basic building block of a quadtree hierarchy built on top of a bottom layer of random variables that represent pixels of an image, a feature map, or more generally a collection of spatially distributed discrete variables. The multi-layer architecture implements a hierarchical data representation that, via belief propagation, can be used for learning and inference. Typical uses are pattern completion, correction and classification. The FGrn paradigm provides great flexibility and modularity and appears as a promising candidate for building deep networks: the system can be easily extended by introducing new and different (in cardinality and in type) variables. Prior knowledge, or supervised information, can be introduced at different scales. The FGrn paradigm provides a handy way for building all kinds of architectures by interconnecting only three types of units: Single Input Single Output (SISO) blocks, Sources and Replicators. The network is designed like a circuit diagram and the belief messages flow bidirectionally in the whole system. The learning algorithms operate only locally within each block. The framework is demonstrated in this paper in a three-layer structure applied to images extracted from a standard data set.", "output": "Towards Building Deep Networks with Bayesian Factor Graphs", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Domain adaptation, and transfer learning more generally, seeks to remedy the problem created when training and testing datasets are generated by different distributions. In this work, we introduce a new unsupervised domain adaptation algorithm for when there are multiple sources available to a learner. Our technique assigns a rough labeling on the target samples, then uses it to learn a transformation that aligns the two datasets before final classification. In this article we give a convenient implementation of our method, show several experiments using it, and compare it to other methods commonly used in the field.", "output": "Multi-Source Domain Adaptation Using Approximate Label Matching", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Device-free (DF) localization is an emerging technology that allows the detection and tracking of entities that do not carry any devices not participate actively in the localization process. Typically, DF systems require a large number of transmitters and receivers to achieve acceptable accuracy, which is not available in many scenarios such as homes and small businesses. In this paper, we introduce MonoStream as an accurate single-stream DF localization system that leverages the rich Channel State Information (CSI) as well as MIMO information from the physical layer to provide accurate DF localization with only one stream. To boost its accuracy and attain low computational requirements, MonoStream models the DF localization problem as an object recognition problem and uses a novel set of CSI-context features and techniques with proven accuracy and efficiency. Experimental evaluation in two typical testbeds, with a side-by-side comparison with the state-of-the-art, shows that MonoStream can achieve an accuracy of 0.95m with at least 26% enhancement in median distance error using a single stream only. This enhancement in accuracy comes with an efficient execution of less than 23ms per location update on a typical laptop. This highlights the potential of MonoStream usage for real-time DF tracking applications.", "output": "MonoStream: A Minimal-Hardware High Accuracy Device-free WLAN Localization System", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Abstract. We study the learning algorithm corresponding to the incremental gradient descent defined by the empirical risk over an infinite dimensional hypotheses space. We consider a statistical learning setting and show that, provided with a universal step-size and a suitable early stopping rule, the learning algorithm thus obtained is universally consistent and derive finite sample bounds. Our results provide a theoretical foundation for considering early stopping in online learning algorithms and shed light on the effect of allowing for multiple passes over the data.", "output": "REGULARIZATION BY EARLY STOPPING FOR ONLINE LEARNING ALGORITHMS", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Since its inception, the modus operandi of multi-task learning (MTL) has been to minimize the task-wise mean of the empirical risks. We introduce a generalized loss-compositional paradigm for MTL that includes a spectrum of formulations as a subfamily. One endpoint of this spectrum is minimax MTL: a new MTL formulation that minimizes the maximum of the tasks\u2019 empirical risks. Via a certain relaxation of minimax MTL, we obtain a continuum of MTL formulations spanning minimax MTL and classical MTL. The full paradigm itself is loss-compositional, operating on the vector of empirical risks. It incorporates minimax MTL, its relaxations, and many new MTL formulations as special cases. We show theoretically that minimax MTL tends to avoid worst case outcomes on newly drawn test tasks in the learning to learn (LTL) test setting. The results of several MTL formulations on synthetic and real problems in the MTL and LTL test settings are encouraging.", "output": "Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Vulnerability of state-of-the-art deep neural networks to adversarial attacks has been attracting a lot of attention recently. In this work we propose a new algorithm for constructing universal adversarial perturbations. Our approach is based on computing the so called (p, q)-singular vectors of the Jacobian matrices of hidden layers of a network. Resulting perturbations present interesting visual patterns and by using a batch of just 64 images we can construct adversarial perturbations with relatively high fooling rate. We also investigate a correlation between the singular values of the Jacobian matrices and the fooling rate of a corresponding singular vector.", "output": "Art of singular vectors and universal adversarial perturbations", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The game of Go is more challenging than other board games, due to the difficulty of constructing a position or move evaluation function. In this paper we investigate whether deep convolutional networks can be used to directly represent and learn this knowledge. We train a large 12-layer convolutional neural network by supervised learning from a database of human professional games. The network correctly predicts the expert move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional-search program GnuGo in 97% of games, and matched the performance of a state-of-the-art Monte-Carlo tree search that simulates two million positions per move.", "output": "MOVE EVALUATION IN GO USING DEEP CONVOLUTIONAL NEURAL NETWORKS", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "This paper presents a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA). Based on the synchronous belief propagation (BP) algorithm, we first develop a parallel belief propagation (PBP) algorithm on the parallel architecture. Because the extensive communication delay often causes a low efficiency of parallel topic modeling, we further use Zipf\u2019s law to reduce the total communication cost in PBP. Extensive experiments on different data sets demonstrate that CE-PBP achieves a higher topic modeling accuracy and reduces more than 80% communication cost than the state-of-the-art parallel Gibbs sampling (PGS) algorithm.", "output": "Communication-Efficient Parallel Belief Propagation for Latent Dirichlet Allocation", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The Generative Adversarial Network (GAN) has achieved great success in generating realistic (realvalued) synthetic data. However, convergence issues and difficulties dealing with discrete data hinder the applicability of GAN to text. We propose a framework for generating realistic text via adversarial training. We employ a long shortterm memory network as generator, and a convolutional network as discriminator. Instead of using the standard objective of GAN, we propose matching the high-dimensional latent feature distributions of real and synthetic sentences, via a kernelized discrepancy metric. This eases adversarial training by alleviating the mode-collapsing problem. Our experiments show superior performance in quantitative evaluation, and demonstrate that our model can generate realistic-looking sentences.", "output": "Adversarial Feature Matching for Text Generation", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "There are around a hundred installed apps on an average smartphone. The high number of apps and the limited number of app icons that can be displayed on the device\u2019s screen requires a new paradigm to address their visibility to the user. In this paper we propose a new online algorithm for dynamically predicting a set of apps that the user is likely to use. The algorithm runs on the user\u2019s device and constantly learns the user\u2019s habits at a given time, location, and device state. It is designed to actively help the user to navigate to the desired app as well as to provide a personalized feeling, and hence is aimed at maximizing the AUC. We show both theoretically and empirically that the algorithm maximizes the AUC, and yields good results on a set of 1,000 devices.", "output": "Context-Based Prediction of App Usage", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In this study, we introduce a new approach for learning language models by training them to estimate word-context pointwise mutual information (PMI), and then deriving the desired conditional probabilities from PMI at test time. Specifically, we show that with minor modifications to word2vec\u2019s algorithm, we get principled language models that are closely related to the well-established Noise Contrastive Estimation (NCE) based language models. A compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in word2vec to learn word embeddings.", "output": "A Simple Language Model based on PMI Matrix Approximations", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Question answering (QA) has been the subject of a resurgence over the past years. The said resurgence has led to a multitude of question answering (QA) systems being developed both by companies and research facilities. While a few components of QA systems get reused across implementations, most systems do not leverage the full potential of component reuse. Hence, the development of QA systems is currently still a tedious and time-consuming process. We address the challenge of accelerating the creation of novel or tailored QA systems by presenting a concept for a self-wiring approach to composing QA systems. Our approach will allow the reuse of existing, web-based QA systems or modules while developing new QA platforms. To this end, it will rely on QA modules being described using the Web Ontology Language. Based on these descriptions, our approach will be able to automatically compose QA systems using a data-driven approach automatically.", "output": "Self-Wiring Question Answering Systems", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "This paper describes the best first search strategy used by U-Plan (Mansell 1993a), a planning system that constructs quantitatively ranked plans given an incomplete description of an uncertain environment. U-Plan uses uncertain and incomplete evidence describing the environment, characterises it using a Dempster\u00ad Shafer interval, and generates a set of possible world states. Plan construction takes place in an abstraction hierarchy where strategic decisions are made before tactical decisions. Search through this abstraction hierarchy is guided by a quantitative measure (expected fulfilment) based on decision theory. The search strategy is best first with the provision to update expected fulfilments and review previous decisions in the light of planning developments. U-Pian generates multiple plans for multiple possible worlds, and attempts to use existing plans for new world situations. A super-plan is then constructed, based on merging the set of plans and appropriately timed knowledge acquisition operators, which are used to decide between plan alternatives during plan execution.", "output": "Operator Selection While Planning Under Uncertainty", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Berkeley FrameNet is a lexico-semantic resource for English based on the theory of frame semantics. It has been exploited in a range of natural language processing applications and has inspired the development of framenets for many languages. We present a methodological approach to the extraction and generation of a computational multilingual FrameNet-based grammar and lexicon. The approach leverages FrameNet-annotated corpora to automatically extract a set of cross-lingual semantico-syntactic valence patterns. Based on data from Berkeley FrameNet and Swedish FrameNet, the proposed approach has been implemented in Grammatical Framework (GF), a categorial grammar formalism specialized for multilingual grammars. The implementation of the grammar and lexicon is supported by the design of FrameNet, providing a frame semantic abstraction layer, an interlingual semantic API (application programming interface), over the interlingual syntactic API already provided by GF Resource Grammar Library. The evaluation of the acquired grammar and lexicon shows the feasibility of the approach. Additionally, we illustrate how the FrameNet-based grammar and lexicon are exploited in two distinct multilingual controlled natural language applications. The produced resources are available under an open source license.", "output": "A Multilingual FrameNet-based Grammar and Lexicon for Controlled Natural Language", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman\u2019s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.", "output": "A Distributional Perspective on Reinforcement Learning", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "A methodology for the development of a fuzzy expert system (FES) with application to earthquake prediction is presented. The idea is to reproduce the performance of a human expert in earthquake prediction. To do this, at the first step, rules provided by the human expert are used to generate a fuzzy rule base. These rules are then fed into an inference engine to produce a fuzzy inference system (FIS) and to infer the results. In this paper, we have used a Sugeno type fuzzy inference system to build the FES. At the next step, the adaptive network-based fuzzy inference system (ANFIS) is used to refine the FES parameters and improve its performance. The proposed framework is then employed to attain the performance of a human expert used to predict earthquakes in the Zagros area based on the idea of coupled earthquakes. While the prediction results are promising in parts of the testing set, the general performance indicates that prediction methodology based on coupled earthquakes needs more investigation and more complicated reasoning procedure to yield satisfactory predictions.", "output": "A Fuzzy Expert System for Earthquake Prediction, Case Study: The Zagros Range", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Interactive topic models are powerful tools for those seeking to understand large collections of text. However, existing sampling-based interactive topic modeling approaches scale poorly to large data sets. Anchor methods, which use a single word to uniquely identify a topic, offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for user-facing applications. We propose combinations of words as anchors, going beyond existing single word anchor algorithms\u2014an approach we call \u201cTandem Anchors\u201d. We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interactive and non-interactive approaches. Tandem anchors are faster and more intuitive than existing interactive approaches. Topic models distill large collections of text into topics, giving a high-level summary of the thematic structure of the data without manual annotation. In addition to facilitating discovery of topical trends (Gardner et al., 2010), topic modeling is used for a wide variety of problems including document classification (Rubin et al., 2012), information retrieval (Wei and Croft, 2006), author identification (Rosen-Zvi et al., 2004), and sentiment analysis (Titov and McDonald, 2008). However, the most compelling use of topic models is to help users understand large datasets (Chuang et al., 2012). Interactive topic modeling (Hu et al., 2014) allows non-experts to refine automatically generated topics, making topic models less of a \u201ctake it or leave it\u201d proposition. Including humans input during training improves the quality of the model and allows users to guide topics in a specific way, custom tailoring the model for a specific downstream task or analysis. The downside is that interactive topic modeling is slow\u2014algorithms typically scale with the size of the corpus\u2014and requires non-intuitive information from the user in the form of must-link and cannot-link constraints (Andrzejewski et al., 2009). We address these shortcomings of interactive topic modeling by using an interactive version of the anchor words algorithm for topic models. The anchor algorithm (Arora et al., 2013) is an alternative topic modeling algorithm which scales with the number of unique word types in the data rather than the number of documents or tokens (Section 1). This makes the anchor algorithm fast enough for interactive use, even in web-scale document collections. A drawback of the anchor method is that anchor words\u2014words that have high probability of being in a single topic\u2014are not intuitive. We extend the anchor algorithm to use multiple anchor words in tandem (Section 2). Tandem anchors not only improve interactive refinement, but also make the underlying anchor-based method more intuitive. For interactive topic modeling, tandem anchors produce higher quality topics than single word anchors (Section 3). Tandem anchors provide a framework for fast interactive topic modeling: users improve and refine an existing model through multiword anchors (Section 4). Compared to existing methods such as Interactive Topic Models (Hu et al., 2014), our method is much faster.", "output": "Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Natural language understanding and dialogue policy learning are both essential in conversational systems that predict the next system actions in response to a current user utterance. Conventional approaches aggregate separate models of natural language understanding (NLU) and system action prediction (SAP) as a pipeline that is sensitive to noisy outputs of error-prone NLU. To address the issues, we propose an end-to-end deep recurrent neural network with limited contextual dialogue memory by jointly training NLU and SAP on DSTC4 multi-domain human-human dialogues. Experiments show that our proposed model significantly outperforms the state-of-the-art pipeline models for both NLU and SAP, which indicates that our joint model is capable of mitigating the affects of noisy NLU outputs, and NLU model can be refined by error flows backpropagating from the extra supervised signals of system actions.", "output": "END-TO-END JOINT LEARNING OF NATURAL LANGUAGE UNDERSTANDING AND DIALOGUE MANAGER", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In this paper we explore a class of belief update operators, in which the definition of the operator is compositional with respect to the sentence to be added. The goal is to provide an update operator that is intuitive, in that its definition is based on a recursive decomposition of the update sentence\u2019s structure, and that may be reasonably implemented. In addressing update, we first provide a definition phrased in terms of the models of a knowledge base. While this operator satisfies a core group of the benchmark Katsuno-Mendelzon update postulates, not all of the postulates are satisfied. Other Katsuno-Mendelzon postulates can be obtained by suitably restricting the syntactic form of the sentence for update, as we show. In restricting the syntactic form of the sentence for update, we also obtain a hierarchy of update operators with Winslett\u2019s standard semantics as the most basic interesting approach captured. We subsequently give an algorithm which captures this approach; in the general case the algorithm is exponential, but with some not-unreasonable assumptions we obtain an algorithm that is linear in the size of the knowledge base. Hence the resulting approach has much better complexity characteristics than other operators in some situations. We also explore other compositional belief change operators: erasure is developed as a dual operator to update; we show that a forget operator is definable in terms of update; and we give a definition of the compositional revision operator. We obtain that compositional revision, under the most natural definition, yields the Satoh revision operator.", "output": "Compositional Belief Update", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We present a converged algorithm for Tikhonov regularized nonnegative matrix factorization (NMF). We specially choose this regularization because it is known that Tikhonov regularized least square (LS) is the more preferable form in solving linear inverse problems than the conventional LS. Because an NMF problem can be decomposed into LS subproblems, it can be expected that Tikhonov regularized NMF will be the more appropriate approach in solving NMF problems. The algorithm is derived using additive update rules which have been shown to have convergence guarantee. We equip the algorithm with a mechanism to automatically determine the regularization parameters based on the L-curve, a well-known concept in the inverse problems community, but is rather unknown in the NMF research. The introduction of this algorithm thus solves two inherent problems in Tikhonov regularized NMF algorithm research, i.e., convergence guarantee and regularization parameters determination.", "output": "A Converged Algorithm for Tikhonov Regularized Nonnegative Matrix Factorization with Automatic Regularization Parameters Determination", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Image captioning has so far been explored mostly in English, as most available datasets are in this language. However, the application of image captioning should not be restricted by language. Only few studies have been conducted for image captioning in a cross-lingual se\u008aing. Di\u0082erent from these works that manually build a dataset for a target language, we aim to learn a cross-lingual captioning model fully from machine-translated sentences. To conquer the lack of \u0083uency in the translated sentences, we propose in this paper a \u0083uency-guided learning framework. \u008ce framework comprises a module to automatically estimate the \u0083uency of the sentences and another module to utilize the estimated \u0083uency scores to e\u0082ectively train an image captioning model for the target language. As experiments on two bilingual (English-Chinese) datasets show, our approach improves both \u0083uency and relevance of the generated captions in Chinese, but without using any manually wri\u008aen sentences from the target language.", "output": "Fluency-Guided Cross-Lingual Image Captioning", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Many natural language understanding (NLU) tasks, such as shallow parsing (i.e., text chunking) and semantic slot filling, require the assignment of representative labels to the meaningful chunks in a sentence. Most of the current deep neural network (DNN) based methods consider these tasks as a sequence labeling problem, in which a word, rather than a chunk, is treated as the basic unit for labeling. These chunks are then inferred by the standard IOB (Inside-OutsideBeginning) labels. In this paper, we propose an alternative approach by investigating the use of DNN for sequence chunking, and propose three neural models so that each chunk can be treated as a complete unit for labeling. Experimental results show that the proposed neural sequence chunking models can achieve start-of-the-art performance on both the text chunking and slot filling tasks.", "output": "Neural Models for Sequence Chunking", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Opinion Mining and Sentiment Analysis is a process of identifying opinions in large unstructured/structured data and then analysing polarity of those opinions. Opinion mining and sentiment analysis have found vast application in analysing online ratings, analysing product based reviews, egovernance, and managing hostile content over the internet. This paper proposes an algorithm to implement aspect level sentiment analysis. The algorithm takes input from the remarks submitted by various teachers of a student. An aspect tree is formed which has various levels and weights are assigned to each branch to identify level of aspect. Aspect value is calculated by the algorithm by means of the proposed aspect tree. Dictionary based method is implemented to evaluate the polarity of the remark. The algorithm returns the aspect value clubbed with opinion value and sentiment value which helps in concluding the summarized value of remark. Keywords\u2014aspect tree, aspect value, opinion mining, opinion value, sentiment analysis", "output": "Aspect Based Sentiment Analysis to Extract Meticulous Opinion Value", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We demonstrate how quantum computation can provide non-trivial improvements in the computational and statistical complexity of the perceptron model. We develop two quantum algorithms for perceptron learning. The first algorithm exploits quantum information processing to determine a separating hyperplane using a number of steps sublinear in the number of data points N , namely O( \u221a N). The second algorithm illustrates how the classical mistake bound of O( 1 \u03b32 ) can be further improved to O( 1 \u221a \u03b3 ) through quantum means, where \u03b3 denotes the margin. Such improvements are achieved through the application of quantum amplitude amplification to the version space interpretation of the perceptron model.", "output": "Quantum Perceptron Models", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Two-timescale Stochastic Approximation (SA) algorithms are widely used in Reinforcement Learning (RL). Their iterates have two parts that are updated with distinct stepsizes. In this work we provide a recipe for analyzing two-timescale SA. Using it, we develop the first convergence rate result for them. From this result we extract key insights on stepsize selection. As an application, we obtain convergence rates for two-timescale RL algorithms such as GTD(0), GTD2, and TDC.", "output": "Two-Timescale Stochastic Approximation Convergence Rates with Applications to Reinforcement Learning", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "CAPTCHAs or reverse Turing tests are real-time assessments used by programs (or computers) to tell humans and machines apart. This is achieved by assigning and assessing hard AI problems that could only be solved easily by human but not by machines. Applications of such assessments range from stopping spammers from automatically filling online forms to preventing hackers from performing dictionary attack. Today, the race between makers and breakers of CAPTCHAs is at a juncture, where the CAPTCHAs proposed are not even answerable by humans. We consider such CAPTCHAs as non user friendly. In this paper, we propose a novel technique for reverse Turing test we call it the Line CAPTCHAs that mainly focuses on user friendliness while not compromising the security aspect that is expected to be provided by such a system.", "output": "User Friendly Line CAPTCHAs", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In cognitive sciences it is not uncommon to use various games effectively. For example, in artificial intelligence, the RoboCup [14] initiative was to set up to catalyse research on the field of autonomous agent technology. In this paper, we introduce a similar soccer simulation initiative to try to investigate a model of human consciousness and a notion of reality in the form of a cognitive problem. In addition, for example, the home pitch advantage and the objective role of the supporters could be naturally described and discussed in terms of this new soccer simulation model.", "output": "Quantum Consciousness Soccer Simulator", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "This study implements a vector space model approach to measure the sentiment orientations of words. Two representative vectors for positive/negative polarity are constructed using high-dimensional vector space in both an unsupervised and a semisupervised manner. A sentiment orientation value per word is determined by taking the difference between the cosine distances against the two reference vectors. These two conditions (unsupervised and semi-supervised) are compared against an existing unsupervised method (Turney, 2002). As a result of our experiment, we demonstrate that this novel approach significantly outperforms the previous unsupervised approach and is more practical and data efficient as well.", "output": "A New Approach for Measuring Sentiment Orientation based on Multi-Dimensional Vector Space", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Our objective is to efficiently design a robust projection matrix \u03a6 for the Compressive Sensing (CS) systems when applied to the signals that are not exactly sparse. The optimal projection matrix is obtained by mainly minimizing the average coherence of the equivalent dictionary. In order to drop the requirement of the sparse representation error (SRE) for a set of training data as in [15] [16], we introduce a novel penalty function independent of a particular SRE matrix. Without requiring of training data, we can efficiently design the robust projection matrix and apply it for most of CS systems, like a CS system for image processing with a conventional wavelet dictionary in which the SRE matrix is generally not available. Simulation results demonstrate the efficiency and effectiveness of the proposed approach compared with the state-of-the-art methods. In addition, we experimentally demonstrate with natural images that under similar compression rate, a CS system with a learned dictionary in high dimensions outperforms the one in low dimensions in terms of reconstruction accuracy. This together with the fact that our proposed method can efficiently work in high dimension suggests that a CS system can be potentially implemented beyond the small patches in sparsity-based image processing.", "output": "An Efficient Method for Robust Projection Matrix Design", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Latent Dirichlet Allocation (LDA) mining thematic structure of documents plays an important role in nature language processing and machine learning areas. However, the probability distribution from LDA only describes the statistical relationship of occurrences in the corpus and usually in practice, probability is not the best choice for feature representations. Recently, embedding methods have been proposed to represent words and documents by learning essential concepts and representations, such as Word2Vec and Doc2Vec. The embedded representations have shown more effectiveness than LDA-style representations in many tasks. In this paper, we propose the Topic2Vec approach which can learn topic representations in the same semantic vector space with words, as an alternative to probability. The experimental results show that Topic2Vec achieves interesting and meaningful results.", "output": "Topic2Vec: Learning Distributed Representations of Topics", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Multi objective (MO) optimization is an emerging field which is increasingly being implemented in many industries globally. In this work, the MO optimization of the extraction process of bioactive compounds from the Gardenia Jasminoides Ellis fruit was solved. Three swarm-based algorithms have been applied in conjunction with normal-boundary intersection (NBI) method to solve this MO problem. The gravitational search algorithm (GSA) and the particle swarm optimization (PSO) technique were implemented in this work. In addition, a novel Hopfield-enhanced particle swarm optimization was developed and applied to the extraction problem. By measuring the levels of dominance, the optimality of the approximate Pareto frontiers produced by all the algorithms were gauged and compared. Besides, by measuring the levels of convergence of the frontier, some understanding regarding the structure of the objective space in terms of its relation to the level of frontier dominance is uncovered. Detail comparative studies were conducted on all the algorithms employed and developed in this work.", "output": "Swarm Intelligence for Multiobjective Optimization of Extraction Process", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Convolutional Neural Networks (CNNs) are extensively used in image and video recognition, natural language processing and other machine learning applications. The success of CNNs in these areas corresponds with a significant increase in the number of parameters and computation costs. Recent approaches towards reducing these overheads involve pruning and compressing the weights of various layers without hurting the overall CNN performance. However, using model compression to generate sparse CNNs mostly reduces parameters from the fully connected layers and may not significantly reduce the final computation costs. In this paper, we present a compression technique for CNNs, where we prune the filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole planes in the network, together with their connecting convolution kernels, the computational costs are reduced significantly. In contrast to other techniques proposed for pruning networks, this approach does not result in sparse connectivity patterns. Hence, our techniques do not need the support of sparse convolution libraries and can work with the most efficient BLAS operations for matrix multiplications. In our results, we show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by upto 38% while regaining close to the original accuracy by retraining the networks.", "output": "Pruning Filters for Efficient ConvNets", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We propose a method for learning from streaming visual data using a compact, constant size representation of all the data that was seen until a given moment. Specifically, we construct a \u201ccoreset\u201d representation of streaming data using a parallelized algorithm, which is an approximation of a set with relation to the squared distances between this set and all other points in its ambient space. We learn an adaptive object appearance model from the coreset tree in constant time and logarithmic space and use it for object tracking by detection. Our method obtains excellent results for object tracking on three standard datasets over more than 100 videos. The ability to summarize data efficiently makes our method ideally suited for tracking in long videos in presence of space and time constraints. We demonstrate this ability by outperforming a variety of algorithms on the TLD dataset with 2685 frames on average. This coreset based learning approach can be applied for both real-time learning of small, varied data and fast learning of big data.", "output": "Coreset-Based Adaptive Tracking", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In this paper, we propose a context-aware keyword spotting model employing a character-level recurrent neural network (RNN) for spoken term detection in continuous speech. The RNN is end-toend trained with connectionist temporal classification (CTC) to generate the probabilities of character and word-boundary labels. There is no need for the phonetic transcription, senone modeling, or system dictionary in training and testing. Also, keywords can easily be added and modified by editing the text based keyword list without retraining the RNN. Moreover, the unidirectional RNN processes an infinitely long input audio streams without pre-segmentation and keywords are detected with low-latency before the utterance is finished. Experimental results show that the proposed keyword spotter significantly outperforms the deep neural network (DNN) and hidden Markov model (HMM) based keyword-filler model even with less computations.", "output": "Online Keyword Spotting with a Character-Level Recurrent Neural Network", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In recent years significant progress has been made in successfully training recurrent neural networks (RNNs) on sequence learning problems involving long range temporal dependencies. The progress has been made on three fronts: (a) Algorithmic improvements involving sophisticated optimization techniques, (b) network design involving complex hidden layer nodes and specialized recurrent layer connections and (c) weight initialization methods. In this paper, we focus on recently proposed weight initialization with identity matrix for the recurrent weights in a RNN. This initialization is specifically proposed for hidden nodes with Rectified Linear Unit (ReLU) non linearity. We offer a simple dynamical systems perspective on weight initialization process, which allows us to propose a modified weight initialization strategy. We show that this initialization technique leads to successfully training RNNs composed of ReLUs. We demonstrate that our proposal produces comparable or better solution for three toy problems involving long range temporal structure: the addition problem, the multiplication problem and the MNIST classification problem using sequence of pixels. In addition, we present results for a benchmark action recognition problem.", "output": "IMPROVING PERFORMANCE OF RECURRENT NEURAL NETWORK WITH RELU NONLINEARITY", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In this paper we present a short history of logics: from particular cases of 2-symbol or numerical valued logic to the general case of n-symbol or numerical valued logic. We show generalizations of 2-valued Boolean logic to fuzzy logic, also from the Kleene\u2019s and Lukasiewicz\u2019 3-symbol valued logics or Belnap\u2019s 4-symbol valued logic to the most general n-symbol or numerical valued refined neutrosophic logic. Two classes of neutrosophic norm (n-norm) and neutrosophic conorm (n-conorm) are defined. Examples of applications of neutrosophic logic to physics are listed in the last section. Similar generalizations can be done for n-Valued Refined Neutrosophic Set, and respectively nValued Refined Neutrosopjhic Probability.", "output": "n-Valued Refined Neutrosophic Logic and Its Applications to Physics", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Systematic use of the published results of randomized clinical trials is<lb>increasingly important in evidence-based medicine. In order to collate and<lb>analyze the results from potentially numerous trials, evidence tables are<lb>used to represent trials concerning a set of interventions of interest. An<lb>evidence table has columns for the patient group, for each of the interven-<lb>tions being compared, for the criterion for the comparison (e.g. proportion<lb>who survived after 5 years from treatment), and for each of the results.<lb>Currently, it is a labour-intensive activity to read each published paper<lb>and extract the information for each field in an evidence table. There have<lb>been some NLP studies investigating how some of the features from papers<lb>can be extracted, or at least the relevant sentences identified. However,<lb>there is a lack of an NLP system for the systematic extraction of each item<lb>of information required for an evidence table. We address this need by a<lb>combination of a maximum entropy classifier, and integer linear program-<lb>ming. We use the later to handle constraints on what is an acceptable<lb>classification of the features to be extracted. With experimental results,<lb>we demonstrate substantial advantages in using global constraints (such<lb>as the features describing the patient group, and the interventions, must<lb>occur before the features describing the results of the comparison).", "output": "Extraction of evidence tables from abstracts of randomized clinical trials using a maximum entropy classifier and global constraints", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We present a new decision rule, maximin safety, that seeks to maintain a large margin from the worst outcome, in much the same way minimax regret seeks to minimize distance from the best. We argue that maximin safety is valuable both descriptively and normatively. Descriptively, maximin safety explains the well-known decoy effect, in which the introduction of a dominated option changes preferences among the other options. Normatively, we provide an axiomatization that characterizes preferences induced by maximin safety, and show that maximin safety shares much of the same behavioral basis with minimax regret.", "output": "Maximin Safety: When Failing to Lose is Preferable to Trying to Win", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Nearest neighbor methods are a popular class of nonparametric estimators with several<lb>desirable properties, such as adaptivity to different distance scales in different regions of<lb>space. Prior work on convergence rates for nearest neighbor classification has not fully<lb>reflected these subtle properties. We analyze the behavior of these estimators in metric<lb>spaces and provide finite-sample, distribution-dependent rates of convergence under min-<lb>imal assumptions. As a by-product, we are able to establish the universal consistency of<lb>nearest neighbor in a broader range of data spaces than was previously known. We illus-<lb>trate our upper and lower bounds by introducing smoothness classes that are customized<lb>for nearest neighbor classification.", "output": "Rates of Convergence for Nearest Neighbor Classification", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Sparse coding is a common approach to learning local features for object recognition. Recently, there has been an increasing interest in learning features from spatio-temporal, binocular, or other multi-observation data, where the goal is to encode the relationship between images rather than the content of a single image. We provide an analysis of multi-view feature learning, which shows that hidden variables encode transformations by detecting rotation angles in the eigenspaces shared among multiple image warps. Our analysis helps explain recent experimental results showing that transformation-specific features emerge when training complex cell models on videos. Our analysis also shows that transformation-invariant features can emerge as a by-product of learning representations of transformations.", "output": "On multi-view feature learning", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The redundant features existing in high dimensional datasets always affect the performance of learning and mining algorithms. How to detect and remove them is an important research topic in machine learning and data mining research. In this paper, we propose a graph based approach to find and remove those redundant features automatically for high dimensional data. Based on sparse learning based unsupervised feature selection framework, Sparse Feature Graph (SFG) is introduced not only to model the redundancy between two features, but also to disclose the group redundancy between two groups of features. With SFG, we can divide the whole features into different groups, and improve the intrinsic structure of data by removing detected redundant features. With accurate data structure, quality indicator vectors can be obtained to improve the learning performance of existing unsupervised feature selection algorithms such as multi-cluster feature selection (MCFS). Our experimental results on benchmark datasets show that the proposed SFG and feature redundancy remove algorithm can improve the performance of unsupervised feature selection algorithms consistently.", "output": "Automatically Redundant Features Removal for Unsupervised Feature Selection via Sparse Feature Graph", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Convolutional neural networks excel in image recognition tasks, but this comes at the cost of high computational and memory complexity. To tackle this problem, [1] developed a tensor factorization framework to compress fully-connected layers. In this paper, we focus on compressing convolutional layers. We show that while the direct application of the tensor framework [1] to the 4-dimensional kernel of convolution does compress the layer, we can do better. We reshape the convolutional kernel into a tensor of higher order and factorize it. We combine the proposed approach with the previous work to compress both convolutional and fully-connected layers of a network and achieve 80\u00d7 network compression rate with 1.1% accuracy drop on the CIFAR-10 dataset.", "output": "Ultimate tensorization: compressing convolutional and FC layers alike", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Low dimensional representations of words allow<lb>accurate NLP models to be trained on limited<lb>annotated data. While most representations ig-<lb>nore words\u2019 local context, a natural way to in-<lb>duce context-dependent representations is to per-<lb>form inference in a probabilistic latent-variable<lb>sequence model. Given the recent success of<lb>continuous vector space word representations,<lb>we provide such an inference procedure for con-<lb>tinuous states, where words\u2019 representations are<lb>given by the posterior mean of a linear dynam-<lb>ical system. Here, efficient inference can be<lb>performed using Kalman filtering. Our learn-<lb>ing algorithm is extremely scalable, operating<lb>on simple cooccurrence counts for both param-<lb>eter initialization using the method of moments<lb>and subsequent iterations of EM. In our exper-<lb>iments, we employ our inferred word embed-<lb>dings as features in standard tagging tasks, ob-<lb>taining significant accuracy improvements. Fi-<lb>nally, the Kalman filter updates can be seen as a<lb>linear recurrent neural network. We demonstrate<lb>that using the parameters of our model to ini-<lb>tialize a non-linear recurrent neural network lan-<lb>guage model reduces its training time by a day<lb>and yields lower perplexity.", "output": "A Linear Dynamical System Model for Text", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Recent years have witnessed increasing interest in the potential benefits of \u2018intelligent\u2019 autonomous machines such as robots. Honda\u2019s Asimo humanoid robot, iRobot\u2019s Roomba robot vacuum cleaner and Google\u2019s driverless cars have fired the imagination of the general public, and social media buzz with speculation about a utopian world of helpful robot assistants or the coming robot apocalypse! However, there is a long way to go before autonomous systems reach the level of capabilities required for even the simplest of tasks involving human-robot interaction especially if it involves communicative behaviour such as speech and language. Of course the field of Artificial Intelligence (AI) has made great strides in these areas, and has moved on from abstract high-level rule-based paradigms to embodied architectures whose operations are grounded in real physical environments. What is still missing, however, is an overarching theory of intelligent communicative behaviour that informs system-level design decisions in order to provide a more coherent approach to system integration. This chapter introduces the beginnings of such a framework inspired by the principles of Perceptual Control Theory (PCT). In particular, it is observed that PCT has hitherto tended to view perceptual processes as a relatively straightforward series of transformations from sensation to perception, and has overlooked the potential of powerful generative model-based solutions that have emerged in practical fields such as visual or auditory scene analysis. Starting from first principles, a sequence of arguments is presented which not only shows how these ideas might be integrated into PCT, but which also extend PCT towards a remarkably symmetric architecture for a needs-driven communicative agent. It is concluded that, if behaviour is the control of perception (the central tenet of PCT), then perception (at least for communicative agents) is the simulation of behaviour.", "output": "PCT and Beyond: Towards a Computational Framework for \u2018Intelligent\u2019 Communicative Systems", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Natural language inference (NLI) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate learning-centered methods such as deep neural networks for the NLI task. In this paper, we propose a special long short-term memory (LSTM) architecture for NLI. Our model builds on top of a recently proposed neutral attention model for NLI but is based on a significantly different idea. Instead of deriving sentence embeddings for the premise and the hypothesis to be used for classification, our solution uses a matching-LSTM that performs word-by-word matching of the hypothesis with the premise. This LSTM is able to place more emphasis on important word-level matching results. In particular, we observe that this LSTM remembers important mismatches that are critical for predicting the contradiction or the neutral relationship label. Our experiments on the SNLI corpus show that our model outperforms the state of the art, achieving an accuracy of 86.1% on the test data.", "output": "Learning Natural Language Inference with LSTM", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Building Information Modeling (BIM) is a recent construction process based on a 3D model, containing every component related to the building achievement. Architects, structure engineers, method engineers, and others participant to the building process work on this model through the design-to-construction cycle. The high complexity and the large amount of information included in these models raise several issues, delaying its wide adoption in the industrial world. One of the most important is the visualization: professionals have difficulties to find out the relevant information for their job. Actual solutions suffer from two limitations: the BIM models information are processed manually and insignificant information are simply hidden, leading to inconsistencies in the building model. This paper describes a system relying on an ontological representation of the building information to label automatically the building elements. Depending on the user\u2019s department, the visualization is modified according to these labels by automatically adjusting the colors and image properties based on a saliency model. The proposed saliency model incorporates several adaptations to fit the specificities of architectural images.", "output": "Adaptive Visualisation System for Construction Building Information Models Using Saliency", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In this paper, we propose a multi-kernel classifier learning algorithm to optimize a given nonlinear and nonsmoonth multivariate classifier performance measure. Moreover, to solve the problem of kernel function selection and kernel parameter tuning, we proposed to construct an optimal kernel by weighted linear combination of some candidate kernels. The learning of the classifier parameter and the kernel weight are unified in a single objective function considering to minimize the upper boundary of the given multivariate performance measure. The objective function is optimized with regard to classifier parameter and kernel weight alternately in an iterative algorithm by using cutting plane algorithm. The developed algorithm is evaluated on two different pattern classification methods with regard to various multivariate performance measure optimization problems. The experiment results show the proposed algorithm outperforms the competing methods.", "output": "Multiple kernel multivariate performance learning using cutting plane algorithm", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "A major challenge in paraphrase research is the lack of parallel corpora. In this paper, we present a new method to collect large-scale sentential paraphrases from Twitter by linking tweets through shared URLs. The main advantage of our method is its simplicity, as it gets rid of the classifier or human in the loop needed to select data before annotation and subsequent application of paraphrase identification algorithms in the previous work. We present the largest human-labeled paraphrase corpus to date of 51,524 sentence pairs and the first cross-domain benchmarking for automatic paraphrase identification. In addition, we show that more than 30,000 new sentential paraphrases can be easily and continuously captured every month at \u223c70% precision, and demonstrate their utility for downstream NLP tasks through phrasal paraphrase extraction. We make our code and data freely available.1", "output": "A Continuously Growing Dataset of Sentential Paraphrases", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "This paper describes a new kind of knowledge representation and mining system which we are calling the Semantic Knowledge Graph. At its heart, the Semantic Knowledge Graph leverages an inverted index, along with a complementary uninverted index, to represent nodes (terms) and edges (the documents within intersecting postings lists for multiple terms/nodes). This provides a layer of indirection between each pair of nodes and their corresponding edge, enabling edges to materialize dynamically from underlying corpus statistics. As a result, any combination of nodes can have edges to any other nodes materialize and be scored to reveal latent relationships between the nodes. This provides numerous benefits: the knowledge graph can be built automatically from a real-world corpus of data, new nodes along with their combined edges can be instantly materialized from any arbitrary combination of preexisting nodes (using set operations), and a full model of the semantic relationships between all entities within a domain can be represented and dynamically traversed using a highly compact representation of the graph. Such a system has widespread applications in areas as diverse as knowledge modeling and reasoning, natural language processing, anomaly detection, data cleansing, semantic search, analytics, data classification, root cause analysis, and recommendations systems. The main contribution of this paper is the introduction of a novel system the Semantic Knowledge Graph which is able to dynamically discover and score interesting relationships between any arbitrary combination of entities (words, phrases, or extracted concepts) through dynamically materializing nodes and edges from a compact graphical representation built automatically from a corpus of data representative of a knowledge domain. The source code for our Semantic Knowledge Graph implementation is being published along with this paper to facilitate further research and extensions of this work.", "output": "The Semantic Knowledge Graph: A compact, auto-generated model for real-time traversal and ranking of any relationship within a domain", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Ordinary least squares (OLS) is the default method for fitting linear models, but is not applicable for problems with dimensionality larger than the sample size. For these problems, we advocate the use of a generalized version of OLS motivated by ridge regression, and propose two novel three-step algorithms involving least squares fitting and hard thresholding. The algorithms are methodologically simple to understand intuitively, computationally easy to implement efficiently, and theoretically appealing for choosing models consistently. Numerical exercises comparing our methods with penalization-based approaches in simulations and data analyses illustrate the great potential of the proposed algorithms.", "output": "No penalty no tears: Least squares in high-dimensional linear models", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "To what extent is the success of deep visualization due to the training? Could we do deep visualization using untrained, random weight networks? To address this issue, we explore new and powerful generative models for three popular deep visualization tasks using untrained, random weight convolutional neural networks. First we invert representations in feature spaces and reconstruct images from white noise inputs. The reconstruction quality is statistically higher than that of the same method applied on well trained networks with the same architecture. Next we synthesize textures using scaled correlations of representations in multiple layers and our results are almost indistinguishable with the original natural texture and the synthesized textures based on the trained network. Third, by recasting the content of an image in the style of various artworks, we create artistic images with high perceptual quality, highly competitive to the prior work of Gatys et al. on pretrained networks. To our knowledge this is the first demonstration of image representations using untrained deep neural networks. Our work provides a new and fascinating tool to study the representation of deep network architecture and sheds light on new understandings on deep visualization.", "output": "A Powerful Generative Model Using Random Weights for the Deep Image Representation", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "A number of representation schemes have been presented for use within Learning Classifier Systems, ranging from binary encodings to Neural Networks, and more recently Dynamical Genetic Programming (DGP). This paper presents results from an investigation into using a fuzzy DGP representation within the XCSF Learning Classifier System. In particular, asynchronous Fuzzy Logic Networks are used to represent the traditional condition-action production system rules. It is shown possible to use self-adaptive, open-ended evolution to design an ensemble of such fuzzy dynamical systems within XCSF to solve several well-known continuousvalued test problems.", "output": "Fuzzy Dynamical Genetic Programming in XCSF", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Algorithm portfolios represent a strategy of composing multiple heuristic algorithms, each suited to a different class of problems, within a single general solver that will choose the best suited algorithm for each input. This approach recently gained popularity especially for solving combinatoric problems, but optimization applications are still emerging. The COCO platform [6] [5] of the BBOB workshop series is the current standard way to measure performance of continuous black-box optimization algorithms. As an extension to the COCO platform, we present the Python-based COCOpf framework that allows composing portfolios of optimization algorithms and running experiments with different selection strategies. In our framework, we focus on black-box algorithm portfolio and online adaptive selection. As a demonstration, we measure the performance of stock SciPy [8] optimization algorithms and the popular CMA algorithm [4] alone and in a portfolio with two simple selection strategies. We confirm that even a naive selection strategy can provide improved performance across problem classes.", "output": "COCOpf: An Algorithm Portfolio Framework", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Chinese characters can be compared to a molecular structure: a character is analogous to a molecule, radicals are like atoms, calligraphic strokes correspond to elementary particles, and when characters form compounds, they are like molecular structures. In chemistry the conjunction of all of these structural levels produces what we perceive as matter. In language, the conjunction of strokes, radicals, characters, and compounds produces meaning. But when does meaning arise? We all know that radicals are, in some sense, the basic semantic components of Chinese script, but what about strokes? Considering the fact that many characters are made by adding individual strokes to (combinations of) radicals, we can legitimately ask the question whether strokes carry meaning, or not. In this talk I will present my project of extending traditional NLP techniques to radicals and strokes, aiming to obtain a deeper understanding of the way ideographic languages model the world.", "output": "Seeking Meaning in a Space Made out of Strokes, Radicals, Characters and Compounds", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "This paper describes several results of Wimmics, a research lab which names stands for: web-instrumented man-machine interactions, communities, and semantics. The approaches introduced here rely on graph-oriented knowledge representation, reasoning and operationalization to model and support actors, actions and interactions in web-based epistemic communities. The research results are applied to support and foster interactions in online communities and manage their resources.", "output": "Challenges in bridging Social Semantics and Formal Semanticson the Web", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We study the stability vis a vis adversarial noise of matrix factorization algorithm for matrix completion. In particular, our results include: (I) we bound the gap between the solution matrix of the factorization method and the ground truth in terms of root mean square error; (II) we treat the matrix factorization as a subspace fitting problem and analyze the difference between the solution subspace and the ground truth; (III) we analyze the prediction error of individual users based on the subspace stability. We apply these results to the problem of collaborative filtering under manipulator attack, which leads to useful insights and guidelines for collaborative filtering system design.", "output": "Stability of Matrix Factorization for Collaborative Filtering", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Classification and clustering have been studied separately in machine learning and computer vision. Inspired by the recent success of deep learning models in solving various vision problems (e.g., object recognition, semantic segmentation) and the fact that humans serve as the gold standard in assessing clustering algorithms, here, we advocate for a unified treatment of the two problems and suggest that hierarchical frameworks that progressively build complex patterns on top of the simpler ones (e.g., convolutional neural networks) offer a promising solution. We do not dwell much on the learning mechanisms in these frameworks as they are still a matter of debate, with respect to biological constraints. Instead, we emphasize on the compositionality of the real world structures and objects. In particular, we show that CNNs, trained end to end using back propagation with noisy labels, are able to cluster data points belonging to several overlapping shapes, and do so much better than the state of the art algorithms. The main takeaway lesson from our study is that mechanisms of human vision, particularly the hierarchal organization of the visual ventral stream should be taken into account in clustering algorithms (e.g., for learning representations in an unsupervised manner or with minimum supervision) to reach human level clustering performance. This, by no means, suggests that other methods do not hold merits. For example, methods relying on pairwise affinities (e.g., spectral clustering) have been very successful in many cases but still fail in some cases (e.g., overlapping clusters).", "output": "A new look at clustering through the lens of deep convolutional neural networks", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Convolutional neural networks provide visual features that perform remarkably well in many computer vision applications. However, training these networks requires significant amounts of supervision. This paper introduces a generic framework to train deep networks, end-to-end, with no supervision. We propose to fix a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align to them. This domain agnostic approach avoids the standard unsupervised learning issues of trivial solutions and collapsing of features. Thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to millions of images. The proposed approach produces representations that perform on par with state-of-the-art unsupervised methods on ImageNet and PASCAL VOC.", "output": "Unsupervised Learning by Predicting Noise", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "This paper presents an approach to identify efficient techniques used in Web Search Engine Optimization (SEO). Understanding SEO factors which can influence page\u2019s ranking in search engine is significant for webmasters who wish to attract large number of users to their website. Different from previous relevant research, in this study we developed an intelligent Meta search engine which aggregates results from various search engines and ranks them based on several important SEO parameters. The research tries to establish that using more SEO parameters in ranking algorithms helps in retrieving better search results thus increasing user satisfaction. Initial results generated from Meta search engine outperformed existing search engines in terms of better retrieved search results with high precision.", "output": "An Innovative Approach for online Meta Search Engine Optimization", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Despite tremendous progress in computer vision, there has not been an attempt for machine learning on very large-scale medical image databases. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital\u2019s Picture Archiving and Communication System. With natural language processing, we mine a collection of representative \u223c216K two-dimensional key images selected by clinicians for diagnostic reference, and match the images with their descriptions in an automated manner. Our system interleaves between unsupervised learning and supervised learning on documentand sentence-level text collections, to generate semantic labels and to predict them given an image. Given an image of a patient scan, semantic topics in radiology levels are predicted, and associated key-words are generated. Also, a number of frequent disease types are detected as present or absent, to provide more specific interpretation of a patient scan. This shows the potential of largescale learning and prediction in electronic patient records available in most modern clinical institutions.", "output": "Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database for Automated Image Interpretation", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Cooperative games model the allocation of profit from joint actions, following considerations such as stability and fairness. We propose the reliability extension of such games, where agents may fail to participate in the game. In the reliability extension, each agent only \u201csurvives\u201d with a certain probability, and a coalition\u2019s value is the probability that its surviving members would be a winning coalition in the base game. We study prominent solution concepts in such games, showing how to approximate the Shapley value and how to compute the core in games with few agent types. We also show that applying the reliability extension may stabilize the game, making the core non-empty even when the base game has an empty core.", "output": "Solving Cooperative Reliability Games", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The skip-thought model has been proven to be effective at learning sentence representations and capturing sentence semantics. In this paper, we propose a suite of techniques to trim and improve it. First, we validate a hypothesis that, given a current sentence, inferring the previous and inferring the next sentence provide similar supervision power, therefore only one decoder for predicting the next sentence is preserved in our trimmed skip-thought model. Second, we present a connection layer between encoder and decoder to help the model to generalize better on semantic relatedness tasks. Third, we found that a good word embedding initialization is also essential for learning better sentence representations. We train our model unsupervised on a large corpus with contiguous sentences, and then evaluate the trained model on 7 supervised tasks, which includes semantic relatedness, paraphrase detection, and text classification benchmarks. We empirically show that, our proposed model is a faster, lighter-weight and equally powerful alternative to the original skip-thought model.", "output": "Trimming and Improving Skip-thought Vectors", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "State-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems. We present a fully Bayesian approach to inference and learning (i.e. state estimation and system identification) in nonlinear nonparametric state-space models. We place a Gaussian process prior over the state transition dynamics, resulting in a flexible model able to capture complex dynamical phenomena. To enable efficient inference, we marginalize over the transition dynamics function and infer directly the joint smoothing distribution using specially tailored Particle Markov Chain Monte Carlo samplers. Once a sample from the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically. Our approach preserves the full nonparametric expressivity of the model and can make use of sparse Gaussian processes to greatly reduce computational complexity.", "output": "Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The goal of this paper is to investigate the connection between the performance gain that can be obtained by selftraining and the similarity between the corpora used in this approach. Self-training is a semi-supervised technique designed to increase the performance of machine learning algorithms by automatically classifying instances of a task and adding these as additional training material to the same classifier. In the context of language processing tasks, this training material is mostly an (annotated) corpus. Unfortunately self-training does not always lead to a performance increase and whether it will is largely unpredictable. We show that the similarity between corpora can be used to identify those setups for which self-training can be beneficial. We consider this research as a step in the process of developing a classifier that is able to adapt itself to each new test corpus that it is presented with.", "output": "Predicting the Effectiveness of Self-Training: Application to Sentiment Classification", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We prove in this paper that the expected value of the objective function of the k-means++ algorithm for samples converges to population expected value. As k-means++, for samples, provides with constant factor approximation for k-means objectives, such an approximation can be achieved for the population with increase of the sample size. This result is of potential practical relevance when one is considering using subsampling when clustering large data sets (large data bases).", "output": "On the Consistency of k-means++ algorithm", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that contains the two celebrated policy and value iteration methods. Despite its generality, MPI has not been thoroughly studied, especially its approximation form which is used when the state and/or action spaces are large or infinite. In this paper, we propose three implementations of approximate MPI (AMPI) that are extensions of well-known approximate DP algorithms: fitted-value iteration, fitted-Q iteration, and classification-based policy iteration. We provide error propagation analyses that unify those for approximate policy and value iteration. On the last classification-based implementation, we develop a finite-sample analysis that shows that MPI\u2019s main parameter allows to control the balance between the estimation error of the classifier and the overall value function approximation.", "output": "Approximate Modified Policy Iteration", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The paper presents a new script classification method for the discrimination of the South Slavic medieval labels. It consists in the textural analysis of the script types. In the first step, each letter is coded by the equivalent script type, which is defined by its typographical features. Obtained coded text is subjected to the run-length statistical analysis and to the adjacent local binary pattern analysis in order to extract the features. The result shows a diversity between the extracted features of the scripts, which makes the feature classification more effective. It is the basis for the classification process of the script identification by using an extension of a state-of-the-art approach for document clustering. The proposed method is evaluated on an example of hand-engraved in stone and handprinted in paper labels in old Cyrillic, angular and round Glagolitic. Experiments demonstrate very positive results, which prove the effectiveness of the proposed method.", "output": "An Approach to the Analysis of the South Slavic Medieval Labels Using Image Texture", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Clustering is an effective technique in data mining to generate groups that are the matter of interest. Among various clustering approaches, the family of k-means algorithms and min-cut algorithms gain most popularity due to their simplicity and efficacy. The classical k-means algorithm partitions a number of data points into several subsets by iteratively updating the clustering centers and the associated data points. By contrast, a weighted undirected graph is constructed in min-cut algorithms which partition the vertices of the graph into two sets. However, existing clustering algorithms tend to cluster minority of data points into a subset, which shall be avoided when the target dataset is balanced. To achieve more accurate clustering for balanced dataset, we propose to leverage exclusive lasso on k-means and min-cut to regulate the balance degree of the clustering results. By optimizing our objective functions that build atop the exclusive lasso, we can make the clustering result as much balanced as possible. Extensive experiments on several large-scale datasets validate the advantage of the proposed algorithms compared to the state-of-the-art clustering algorithms.", "output": "Balanced k-Means and Min-Cut Clustering", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "An increasing amount of analytics is performed on data that is procured in a real-time fashion to make real-time decisions. Such tasks include simple reporting on streams to sophisticated model building. However, the practicality of such analyses are impeded in several domains because they are faced with a fundamental trade-off between data collection latency and analysis accuracy. In this paper, we study this trade-off in the context of a specific domain, Cellular Radio Access Networks (RAN). Our choice of this domain is influenced by its commonalities with several other domains that produce real-time data, our access to a large live dataset, and their real-time nature and dimensionality which makes it a natural fit for a popular analysis technique, machine learning (ML). We find that the latency accuracy trade-off can be resolved using two broad, general techniques: intelligent data grouping and task formulations that leverage domain characteristics. Based on this, we present CellScope, a system that addresses this challenge by applying a domain specific formulation and application of Multi-task Learning (MTL) to RAN performance analysis. It achieves this goal using three techniques: feature engineering to transform raw data into effective features, a PCA inspired similarity metric to group data from geographically nearby base stations sharing performance commonalities, and a hybrid online-offline model for efficient model updates. Our evaluation of CellScope shows that its accuracy improvements over direct application of ML range from 2.5\u00d7 to 4.4\u00d7 while reducing the model update overhead by up to 4.8\u00d7. We have also used CellScope to analyze a live LTE consisting of over 2 million subscribers for a period of over 10 months, where it uncovered several problems and insights, some of them previously unknown.", "output": "Fast and Accurate Performance Analysis of LTE Radio Access Networks", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "A common approach in positive-unlabeled learning is to train a classification model between labeled and unlabeled data. This strategy is in fact known to give an optimal classifier under mild conditions; however, it results in biased empirical estimates of the classifier performance. In this work, we show that the typically used performance measures such as the receiver operating characteristic curve, or the precisionrecall curve obtained on such data can be corrected with the knowledge of class priors; i.e., the proportions of the positive and negative examples in the unlabeled data. We extend the results to a noisy setting where some of the examples labeled positive are in fact negative and show that the correction also requires the knowledge of the proportion of noisy examples in the labeled positives. Using state-of-the-art algorithms to estimate the positive class prior and the proportion of noise, we experimentally evaluate two correction approaches and demonstrate their efficacy on real-life data. Introduction Performance estimation in binary classification is tightly related to the nature of the classification task. As a result, different performance measures may be directly optimized during training. When (mis)classification costs are available, the classifier is ideally trained and evaluated in a costsensitive mode to minimize the expected cost (Whalen 1971; Elkan 2001). More often, however, classification costs are unknown and the overall performance is assessed by averaging the performance over a range of classification modes. The most extensively studied and widely used performance evaluation in binary classification involves estimating the Receiver Operating Characteristic (ROC) curve that plots the true positive rate of a classifier as a function of its false positive rate (Fawcett 2006). The ROC curve provides insight into trade-offs between the classifier\u2019s accuracies on positive versus negative examples over a range of decision thresholds. Furthermore, the area under the ROC curve (AUC) has a meaningful probabilistic interpretation that correlates with the ability of the classifier to separate classes and is often used to rank classifiers (Hanley and McNeil 1982). Another important performance criterion generally used in information retrieval relies on the precision-recall Copyright c \u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. (pr-rc) curve, a plot of precision as a function of recall. The precision-recall evaluation, including summary statistics derived from the pr-rc curve, may be preferred to ROC curves when classes are heavily skewed (Davis and Goadrich 2006). Although model learning and performance evaluation in a supervised setting are well understood (Hastie et al. 2001), the availability of unlabeled data gives additional options and also presents new challenges. A typical semi-supervised scenario involves the availability of positive, negative and (large quantities of) unlabeled data. Here, the unlabeled data can be used to improve training (Blum and Mitchell 1998) or unbias the labeled data (Cortes et al. 2008); e.g., to estimate class proportions that are necessary to calibrate the model and accurately estimate precision when class balances (but not class-conditional distributions) in labeled data are not representative (Saerens et al. 2002). This is often the case when it is more expensive or difficult to label examples of one class than the examples of the other. A special case of the semi-supervised setting arises when the examples of only one class are labeled. It includes open-world domains such as molecular biology where, for example, wet lab experiments determining a protein\u2019s activity are generally conclusive; however, the absence of evidence about a protein\u2019s function cannot be interpreted as the evidence of absence. This is because, even when the labeling is attempted, a functional assay may not lead to the desired activity for a number of experimental reasons. In other domains, such as social networks, only positive examples can be collected (such as \u2018liking\u2019 a particular product) because, by design, the negative labeling is not allowed. The development of classification models in this setting is often referred to as positiveunlabeled learning (Denis et al. 2005). State-of-the-art techniques in positive-unlabeled learning tackle this problem by treating the unlabeled sample as negatives and training a classifier to distinguish between labeled (positive) and unlabeled examples. Following Elkan and Noto (2008), we refer to the classifiers trained on a labeled sample from the true distribution of inputs, containing both positive and negative examples, as traditional classifiers. Similarly, we refer to the classifiers trained on the labeled versus unlabeled data as non-traditional classifiers. In theory, the true performance of both traditional and non-traditional classifiers can be evaluated on a labeled samar X iv :1 70 2. 00 51 8v 1 [ st at .M L ] 2 F eb 2 01 7 ple from the true distribution (traditional evaluation). However, this is infeasible for non-traditional learners because such a sample is not available in positive-unlabeled learning. As a result, the non-traditional classifiers are evaluated by using the unlabeled sample as substitute for labeled negatives (non-traditional evaluation). Surprisingly, for a variety of performance criteria, non-traditional classifiers achieve similar performance under traditional evaluation as optimal traditional classifiers (Blanchard et al. 2010; Menon et al. 2015). The intuition for these results comes from the fact that in many practical situations, the posterior distributions in traditional and non-traditional setting provide the same optimal ranking of data points on a given test sample (Jain et al. 2016; Jain, White, and Radivojac 2016). Furthermore, the widely-accepted evaluation approaches using ROC or pr-rc curves are insensitive to the variation of raw prediction scores unless they affect the ranking. Though the efficacy of non-traditional classifiers has been thoroughly studied (Peng et al. 2003; Elkan and Noto 2008; Ward et al. 2009; Menon et al. 2015), estimating their true performance has been much less explored. Such performance estimation often involves computing the fraction(s) of correctly and incorrectly classified examples from both classes; however, in absence of labeled negatives, the fractions computed under the non-traditional evaluation are incorrect, resulting in biased estimates. Figure 1 illustrates the effect of this bias by showing the traditional and nontraditional ROC curves on a handmade data set. Because some of the unlabeled examples in the training set are in fact positive, the area under the ROC curve estimated when the unlabeled examples were considered negative (nontraditional setting) underestimates the true performance for positive versus negative classification (traditional setting). This paper formalizes and evaluates performance estimation of a non-traditional classifier in the traditional setting when the only available training data are (possibly noisy) positive examples and unlabeled data. We show that the true (traditional) performance of such a classifier can be recovered with the knowledge of class priors and the fraction of mislabeled examples in the positive set. We derive formulas for converting the ROC and pr-rc curves from the nontraditional to the traditional setting. Using these recovery formulas, we present methods to estimate true classification performance. Our experiments provide evidence that the methods for the recovery of a classifier\u2019s performance are sound and effective. Problem formulation Consider a binary classification problem from input x \u2208 X to output y \u2208 Y = {0, 1} in a positive-unlabeled setting. Let f be the true distribution over the input space X from which the unlabeled sample is drawn and let f1 and f0 be the distributions of the positive and negative examples, respectively. It follows that f can be expressed as a two-component mixture containing f1 and f0 as f(x) = \u03b1f1(x) + (1\u2212 \u03b1)f0(x), for all x \u2208 X where \u03b1 \u2208 [0, 1) is the mixing proportion (positive class prior) giving the proportion of positives in f . \u0301 \u0301 \u00b0 \u00b0 AUC = 0.8000 AUC = 0.9375 B. Positive vs. unlabeled A. Data set: prediction scores and class labels C. Positive vs. negative 0.986 yes, as 1 0.943 no 0.863 yes, as 1 0.789 no 0.009 no 0.699 yes, as 1 0.473 no 0.211 no 1 1 1 0 1 0 0 0 Prediction Labeled True class label", "output": "Recovering True Classifier Performance in Positive-Unlabeled Learning", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Finding inclusion-minimal hitting sets for a given collection of sets is a fundamental combinatorial problem with applications in domains as diverse as Boolean algebra, computational biology, and data mining. Much of the algorithmic literature focuses on the problem of recognizing the collection of minimal hitting sets; however, in many of the applications, it is more important to generate these hitting sets. We survey twenty algorithms from across a variety of domains, considering their history, classification, useful features, and computational performance on a variety of synthetic and real-world inputs. We also provide a suite of implementations of these algorithms with a ready-to-use, platform-agnostic interface based on Docker containers and the AlgoRun framework, so that interested computational scientists can easily perform similar tests with inputs from their own research areas on their own computers or through a convenient Web interface.", "output": "The minimal hitting set generation problem: algorithms and computation", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We use some of the largest order statistics of the random projections of a reference signal to construct a binary embedding that is adapted to signals correlated with such signal. The embedding is characterized from the analytical standpoint and shown to provide improved performance on tasks such as classification in a reduced-dimensionality space. Keywords\u2014Binary Embeddings, Random projections", "output": "Binary adaptive embeddings from order statistics of random projections", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Solving algebraic word problems requires executing a series of arithmetic operations\u2014a program\u2014to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.", "output": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The fastest known exact algorithms for scorebased structure discovery in Bayesian networks on n nodes run in time and space 2n. The usage of these algorithms is limited to networks on at most around 25 nodes mainly due to the space requirement. Here, we study space\u2013time tradeoffs for finding an optimal network structure. When little space is available, we apply the Gurevich\u2013 Shelah recurrence\u2014originally proposed for the Hamiltonian path problem\u2014and obtain time 2n in space 2n for any s = n/2, n/4, n/8, . . .; we assume the indegree of each node is bounded by a constant. For the more practical setting with moderate amounts of space, we present a novel scheme. It yields running time 2(3/2)n in space 2(3/4)n for any p = 0, 1, . . . , n/2; these bounds hold as long as the indegrees are at most 0.238n. Furthermore, the latter scheme allows easy and efficient parallelization beyond previous algorithms. We also explore empirically the potential of the presented techniques.", "output": "Exact Structure Discovery in Bayesian Networks with Less Space", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The problem of solving (n \u2212 1)-puzzle and cooperative path-finding (CPF) sub-optimally by rule based algorithms is addressed in this manuscript. The task in the puzzle is to rearrange n \u2212 1 pebbles on the square grid of the size of n\u00d7n using one vacant position to a desired goal configuration. An improvement to the existent polynomial-time algorithm is proposed and experimentally analyzed. The improved algorithm is trying to move pebbles in a more efficient way than the original algorithm by grouping them into so-called snakes and moving them jointly within the snake. An experimental evaluation showed that the algorithm using snakes produces solutions that are 8% to 9% shorter than solutions generated by the original algorithm. The snake-based relocation has been also integrated into rule-based algorithms for solving the CPF problem sub-optimally, which is a closely related task. The task in CPF is to relocate a group of abstract robots that move over an undirected graph to given goal vertices. Robots can move to unoccupied neighboring vertices and at most one robot can be placed in each vertex. The (n \u2212 1)-puzzle is a special case of CPF where the underlying graph is represented by a 4-connected grid and there is only one vacant vertex. Two major rule-based algorithms for CPF were included in our study \u2013 BIBOX and PUSH-and-SWAP (PUSH-and-ROTATE). Improvements gained by using snakes in the BIBOX algorithm were stable around 30% in (n \u2212 1)-puzzle solving and up to 50% in CPFs over bi-connected graphs with various ear decompositions and multiple vacant vertices. In the case of the PUSHand-SWAP algorithm the improvement achieved by snakes was around 5% to 8%. However, the improvement was unstable and hardly predictable in the case of PUSH-and-SWAP.", "output": "Improvements in Sub-optimal Solving of the (N \u2212 1)-Puzzle via Joint Relocation of Pebbles and its Applications to Rule-based Cooperative Path-Finding", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "With the accelerated development of robot technologies, optimal control becomes one of the central themes of research. In traditional approaches, the controller, by its internal functionality, finds appropriate actions on the basis of the history of sensor values, guided by the goals, intentions, objectives, learning schemes, and so on planted into it. The idea is that the controller controls the world\u2014the body plus its environment\u2014as reliably as possible. However, in elastically actuated robots this approach faces severe difficulties. This paper advocates for a new paradigm of self-organized control. The paper presents a solution with a controller that is devoid of any functionalities of its own, given by a fixed, explicit and context-free function of the recent history of the sensor values. When applying this controller to a muscletendon driven arm-shoulder system from the Myorobotics toolkit, we observe a vast variety of self-organized behavior patterns: when left alone, the arm realizes pseudo-random sequences of different poses but one can also manipulate the system into definite motion patterns. But most interestingly, after attaching an object, the controller gets in a functional resonance with the object\u2019s internal dynamics: when given a half-filled bottle, the system spontaneously starts shaking the bottle so that maximum response from the dynamics of the water is being generated. After attaching a pendulum to the arm, the controller drives the pendulum into a circular mode. In this way, the robot discovers dynamical affordances of objects its body is interacting with. We also discuss perspectives for using this controller paradigm for intention driven behavior generation.", "output": "Self-organized control for musculoskeletal robots", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We define two algorithms for propagating information in classification problems with pairwise relationships. The algorithms are based on contraction maps and are related to non-linear diffusion and random walks on graphs. The approach is also related to message passing algorithms, including belief propagation and mean field methods. The algorithms we describe are guaranteed to converge on graphs with arbitrary topology. Moreover they always converge to a unique fixed point, independent of initialization. We prove that the fixed points of the algorithms under consideration define lower-bounds on the energy function and the max-marginals of a Markov random field. The theoretical results also illustrate a relationship between message passing algorithms and value iteration for an infinite horizon Markov decision process. We illustrate the practical application of the algorithms under study with numerical experiments in image restoration, stereo depth estimation and binary classification on a grid.", "output": "Diffusion Methods for Classification with Pairwise Relationships", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "1 Ranks, processes and flat grammar ....................................................................................... 2 2 The Rank-Interpretation Architecture for Multilinear Grammars ......................................... 6 2.1 Outline of the Rank-Interpretation Architecture framework .............................................. 6 2.2 A preliminary note on linearity and hierarchy at the phrasal rank ..................................... 7 2.3 Characterisation of the Rank-Interpretation Architecture .................................................. 9 2.3.1 Background ..................................................................................................................... 9 2.3.2 Formal summary ........................................................................................................... 10 2.3.3 Contrast with traditional views of language architecture .............................................. 11 2.3.4 Search for sui generis properties of ranks ..................................................................... 11 2.4 Procedural perspectives on the rank hierarchy ................................................................. 12 3 The discourse rank .............................................................................................................. 15 3.1 The primacy of discourse patterning ................................................................................ 15 3.2 Intonation of an adjacency pair ........................................................................................ 16 3.3 Chanted \u2018call\u2019 intonation .................................................................................................. 18 4 The utterance or text rank ................................................................................................... 20 5 The phrase rank ................................................................................................................... 22 5.1 Characteristics of phrasal structure .................................................................................. 22 5.2 Linear sequences, iteration: regular and subregular grammars ........................................ 23 5.3 A note on long-distance and cross-serial dependencies ................................................... 26 5.4 Prosodic-phonetic interpretation at the phrasal rank ........................................................ 27 6 The word rank ..................................................................................................................... 30 6.1 Flat words ......................................................................................................................... 30 6.2 Flat derivations ................................................................................................................. 30 6.3 Flat compounds ................................................................................................................ 31 6.4 Prosodic-phonetic interpretation at the word rank ........................................................... 31 7 Summary and conclusion .................................................................................................... 33 7.1 From Duality to Multilinear Grammar and Rank Interpretation Architecture ................. 33 7.2 Generalisation to stochastic flat linear models ................................................................. 34 7.3 Future work ...................................................................................................................... 35 8 References ........................................................................................................................... 36", "output": "Multilinear Grammar: Ranks and Interpretations", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The activation function of Deep Neural Networks (DNNs) has undergone many changes during the last decades. Since the advent of the well-known non-saturated Rectified Linear Unit (ReLU), many have tried to further improve the performance of the networks with more elaborate functions. Examples are the Leaky ReLU (LReLU) to remove zero gradients and Exponential Linear Unit (ELU) to reduce bias shift. In this paper, we introduce the Parametric ELU (PELU), an adaptive activation function that allows the DNNs to adopt different non-linear behaviors throughout the training phase. We contribute in three ways: (1) we show that PELU increases the network flexibility to counter vanishing gradient, (2) we provide a gradient-based optimization framework to learn the parameters of the function, and (3) we conduct several experiments on MNIST, CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN, ResNet and Vgg, to demonstrate the general applicability of the approach. Our proposed PELU has shown relative error improvements of 4.45% and 5.68% on CIFAR10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet, along with faster convergence rate in almost all test scenarios. We also observed that Vgg using PELU tended to prefer activations saturating close to zero, as in ReLU, except at last layer, which saturated near -2. These results suggest that varying the shape of the activations during training along with the other parameters helps to control vanishing gradients and bias shift, thus facilitating learning.", "output": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multiway, multilingual model on ten language pairs from WMT\u201915 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.", "output": "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We describe a novel non-parametric statistical hypothesis test of relative dependence between a source variable and two candidate target variables. Such a test enables us to determine whether one source variable is significantly more dependent on a first target variable or a second. Dependence is measured via the HilbertSchmidt Independence Criterion (HSIC), resulting in a pair of empirical dependence measures (source-target 1, source-target 2). We test whether the first dependence measure is significantly larger than the second. Modeling the covariance between these HSIC statistics leads to a provably more powerful test than the construction of independent HSIC statistics by subsampling. The resulting test is consistent and unbiased, and (being based on U-statistics) has favorable convergence properties. The test can be computed in quadratic time, matching the computational complexity of standard empirical HSIC estimators. The effectiveness of the test is demonstrated on several real-world problems: we identify language groups from a multilingual corpus, and we prove that tumor location is more dependent on gene expression than chromosomal imbalances. Source code is available for download at https://github. com/wbounliphone/reldep. Proceedings of the 32 International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).", "output": "A low variance consistent test of relative dependency", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Framing is a political strategy in which politicians carefully word their statements in order to control public perception of issues. Previous works exploring political framing typically analyze frame usage in longer texts, such as Congressional speeches. We present a collection of weakly supervised models which harness collective classification to predict the frames used in political discourse on the microblogging platform, Twitter. Our global probabilistic models show that by combining both lexical features of tweets and network-based behavioral features of Twitter, we are able to increase the average, unsupervised F1 score by 21.52 points over a lexical baseline alone.", "output": "Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We discuss relations between Residual Networks (ResNet), Recurrent Neural Networks (RNNs) and the primate visual cortex. We begin with the observation that a shallow RNN is exactly equivalent to a very deep ResNet with weight sharing among the layers. A direct implementation of such a RNN, although having orders of magnitude fewer parameters, leads to a performance similar to the corresponding ResNet. We propose 1) a generalization of both RNN and ResNet architectures and 2) the conjecture that a class of moderately deep RNNs is a biologically-plausible model of the ventral stream in visual cortex. We demonstrate the effectiveness of the architectures by testing them on the CIFAR-10 dataset. This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF 1231216. 1 ar X iv :1 60 4. 03 64 0v 1 [ cs .L G ] 1 3 A pr 2 01 6", "output": "Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In this work we use the recent advances in representation learning to propose a neural architecture for the problem of natural language inference. Our approach is aligned to mimic how a human does the natural language inference process given two statements. The model uses variants of Long Short Term Memory (LSTM), attention mechanism and composable neural networks, to carry out the task. Each part of our model can be mapped to a clear functionality humans do for carrying out the overall task of natural language inference. The model is end-to-end differentiable enabling training by stochastic gradient descent. On Stanford Natural Language Inference(SNLI) dataset, the proposed model achieves better accuracy numbers than all published models in literature.", "output": "A Neural Architecture Mimicking Humans End-to-End for Natural Language Inference", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "This is a working paper summarizing results of an ongoing research project whose aim is to uniquely characterize the uncertainty mea\u00ad sure for the Dempster-Shafer Theory. A set of intuitive axiomatic requirements is pre\u00ad sented, some of their implications are shown, and the proof is given of the minimality of re\u00ad cently proposed measure AU among all mea\u00ad sures satisfying the proposed requirements.", "output": "Toward a Characterization of Uncertainty Measure for the Dempster-Shafer Theory", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Literature on Constraint Satisfaction exhibits the definition of several \u201cstructural\u201d properties that can be possessed by CSPs, like (in)consistency, substitutability or interchangeability. Current tools for constraint solving typically detect such properties efficiently by means of incomplete yet effective algorithms, and use them to reduce the search space and boost search. In this paper, we provide a unifying framework encompassing most of the properties known so far, both in CSP and other fields\u2019 literature, and shed light on the semantical relationships among them. This gives a unified and comprehensive view of the topic, allows new, unknown, properties to emerge, and clarifies the computational complexity of the various detection problems. In particular, among the others, two new concepts, fixability and removability emerge, that come out to be the ideal characterisations of values that may be safely assigned or removed from a variable\u2019s domain, while preserving problem satisfiability. These two notions subsume a large number of known properties, including inconsistency, substitutability and others. Because of the computational intractability of all the property-detection problems, by following the CSP approach we then determine a number of relaxations which provide sufficient conditions for their tractability. In particular, we exploit forms of language restrictions and local reasoning.", "output": "A Unifying Framework for Structural Properties of CSPs: Definitions, Complexity, Tractability", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We propose a novel approach to reduce memory consumption of the backpropagation through time (BPTT) algorithm when training recurrent neural networks (RNNs). Our approach uses dynamic programming to balance a trade-off between caching of intermediate results and recomputation. The algorithm is capable of tightly fitting within almost any user-set memory budget while finding an optimal execution policy minimizing the computational cost. Computational devices have limited memory capacity and maximizing a computational performance given a fixed memory budget is a practical use-case. We provide asymptotic computational upper bounds for various regimes. The algorithm is particularly effective for long sequences. For sequences of length 1000, our algorithm saves 95% of memory usage while using only one third more time per iteration than the standard BPTT.", "output": "Memory-Efficient Backpropagation Through Time", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "This paper proposes a simple test for compositionality (i.e., literal usage) of a word or phrase in a context-specific way. The test is computationally simple, relying on no external resources and only uses a set of trained word vectors. Experiments show that the proposed method is competitive with state of the art and displays high accuracy in context-specific compositionality detection of a variety of natural language phenomena (idiomaticity, sarcasm, metaphor) for different datasets in multiple languages. The key insight is to connect compositionality to a curious geometric property of word embeddings, which is of independent interest.", "output": "Geometry of Compositionality", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "This paper tackles temporal resolution of documents, such as determining when a document is about or when it was written, based only on its text. We apply techniques from information retrieval that predict dates via language models over a discretized timeline. Unlike most previous works, we rely solely on temporal cues implicit in the text. We consider both document-likelihood and divergence based techniques and several smoothing methods for both of them. Our best model predicts the mid-point of individuals\u2019 lives with a median of 22 and mean error of 36 years for Wikipedia biographies from 3800 B.C. to the present day. We also show that this approach works well when training on such biographies and predicting dates both for nonbiographical Wikipedia pages about specific years (500 B.C. to 2010 A.D.) and for publication dates of short stories (1798 to 2008). Together, our work shows that, even in absence of temporal extraction resources, it is possible to achieve remarkable temporal locality across a diverse set of texts.", "output": "Dating Texts without Explicit Temporal Cues", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We propose a novel language-independent approach for improving machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. More precisely, we improve the translation from a resource-poor source language X1 into a resourcerich language Y given a bi-text containing a limited number of parallel sentences for X1-Y and a larger bi-text for X2-Y for some resource-rich language X2 that is closely related to X1. This is achieved by taking advantage of the opportunities that vocabulary overlap and similarities between the languages X1 and X2 in spelling, word order, and syntax offer: (1) we improve the word alignments for the resource-poor language, (2) we further augment it with additional translation options, and (3) we take care of potential spelling differences through appropriate transliteration. The evaluation for Indonesian\u2192English using Malay and for Spanish\u2192English using Portuguese and pretending Spanish is resource-poor shows an absolute gain of up to 1.35 and 3.37 BLEU points, respectively, which is an improvement over the best rivaling approaches, while using much less additional data. Overall, our method cuts the amount of necessary \u201creal\u201d training data by a factor of 2\u20135.", "output": "Improving Statistical Machine Translation for a Resource-Poor Language Using Related Resource-Rich Languages", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Evolutionary algorithms provide a technique to discover such networks automatically. Despite significant computational requirements, we show that evolving models that rival large, hand-designed architectures is possible today. We employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions. To do this, we use novel and intuitive mutation operators that navigate large search spaces. We stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.", "output": "Large-Scale Evolution of Image Classifiers", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Reinforcement learning algorithms need to deal with the exponential growth of states and actions when exploring optimal control in high-dimensional spaces. This is known as the curse of dimensionality. By projecting the agent\u2019s state onto a low-dimensional manifold, we can represent the state space in a smaller and more efficient representation. By using this representation during learning, the agent can converge to a good policy much faster. We test this approach in the Mario Benchmarking Domain. When using dimensionality reduction in Mario, learning converges much faster to a good policy. But, there is a critical convergence-performance trade-off. By projecting onto a low-dimensional manifold, we are ignoring important data. In this paper, we explore this trade-off of convergence and performance. We find that learning in as few as 4 dimensions (instead of 9), we can improve performance past learning in the full dimensional space at a faster convergence rate.", "output": "Using PCA to Efficiently Represent State Spaces", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Real estate appraisal, which is the process of estimating the price for real estate properties, is crucial for both buys and sellers as the basis for negotiation and transaction. Traditionally, the repeat sales model has been widely adopted to estimate real estate price. However, it depends the design and calculation of a complex economic related index, which is challenging to estimate accurately. Today, real estate brokers provide easy access to detailed online information on real estate properties to their clients. We are interested in estimating the real estate price from these large amounts of easily accessed data. In particular, we analyze the prediction power of online house pictures, which is one of the key factors for online users to make a potential visiting decision. The development of robust computer vision algorithms makes the analysis of visual content possible. In this work, we employ a Recurrent Neural Network (RNN) to predict real estate price using the state-of-the-art visual features. The experimental results indicate that our model outperforms several of other state-of-the-art baseline algorithms in terms of both mean absolute error (MAE) and mean absolute percentage error (MAPE).", "output": "Image Based Appraisal of Real Estate Properties", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "String Kernel (SK) techniques, especially those using gapped k-mers as features (gk), have obtained great success in classifying sequences like DNA, protein, and text. However, the state-of-the-art gk-SK runs extremely slow when we increase the dictionary size (\u2303) or allow more number of mismatches (M). This is because current gk-SK uses a trie-based algorithm to calculate co-occurrence of mismatched substrings resulting in a time cost proportional to O(\u2303 ). We propose a fast algorithm for calculating Gapped k-mer Kernel using Counting (GaKCo). GaKCo uses associative arrays to calculate the co-occurrence of substrings using cumulative counting. This algorithm is fast, scalable to larger \u2303 and M , and naturally parallelizable. We provide a rigorous asymptotic analysis that compares GaKCo with the state-of-the-art gk-SK. Theoretically, the time cost of GaKCo is independent of the \u2303 term that slows down the trie-based approach. Experimentally, we observe that GaKCo achieves the same accuracy as the state-of-the-art and outperforms its speed by factors of 2, 100, and 4, on classifying sequences of DNA (5 datasets), protein (12 datasets), and character-based English text (2 datasets), respectively .", "output": "GaKCo: a Fast Gapped k-mer string Kernel using Counting", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "R\u00e9sum\u00e9. Le crowdsourcing, un enjeu \u00e9conomique majeur, est le fait d\u2019externaliser une t\u00e2che interne d\u2019une entreprise vers le grand-public, la foule. C\u2019est ainsi une forme de sous-traitance digitale destin\u00e9e \u00e0 toute personne susceptible de pouvoir r\u00e9aliser la t\u00e2che demand\u00e9e g\u00e9n\u00e9ralement rapide et non automatisable. L\u2019\u00e9valuation de la qualit\u00e9 du travail des participants est cependant un probl\u00e8me majeur en crowdsourcing. En effet, les contributions doivent \u00eatre contr\u00f4l\u00e9es pour assurer l\u2019efficacit\u00e9 et la pertinence d\u2019une campagne. Plusieurs m\u00e9thodes ont \u00e9t\u00e9 propos\u00e9es pour \u00e9valuer le niveau d\u2019expertise des participants. Ce travail a la particularit\u00e9 de proposer une m\u00e9thode de calcul de degr\u00e9s d\u2019expertise en pr\u00e9sence de donn\u00e9es dont l\u2019ordre de classement est connu. Les degr\u00e9s d\u2019expertise sont ensuite consid\u00e9r\u00e9s sur des donn\u00e9es sans ordre pr\u00e9-\u00e9tabli. Cette m\u00e9thode fond\u00e9e sur la th\u00e9orie des fonctions de croyance tient compte des incertitudes des r\u00e9ponses et est \u00e9valu\u00e9e sur des donn\u00e9es r\u00e9elles d\u2019une campagne r\u00e9alis\u00e9e en 2016.", "output": "Une mesure d\u2019expertise pour le crowdsourcing", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We consider principal component analysis for contaminated data-set in the high dimensional regime, where the dimensionality of each observation is comparable or even more than the number of observations. We propose a deterministic high-dimensional robust PCA algorithm which inherits all theoretical properties of its randomized counterpart, i.e., it is tractable, robust to contaminated points, easily kernelizable, asymptotic consistent and achieves maximal robustness \u2013 a breakdown point of 50%. More importantly, the proposed method exhibits significantly better computational efficiency, which makes it suitable for large-scale real applications.", "output": "Robust PCA in High-dimension: A Deterministic Approach", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Statistical topic models efficiently facilitate the exploration of large-scale data sets. Many models have been developed and broadly used to summarize the semantic structure in news, science, social media, and digital humanities. However, a common and practical objective in data exploration tasks is not to enumerate all existing topics, but to quickly extract representative ones that broadly cover the content of the corpus, i.e., a few topics that serve as a good summary of the data. Most existing topic models fit exactly the same number of topics as a user specifies, which have imposed an unnecessary burden to the users who have limited prior knowledge. We instead propose new models that are able to learn fewer but more representative topics for the purpose of data summarization. We propose a reinforced random walk that allows prominent topics to absorb tokens from similar and smaller topics, thus enhances the diversity among the top topics extracted. With this reinforced random walk as a general process embedded in classical topic models, we obtain diverse topic models that are able to extract the most prominent and diverse topics from data. The inference procedures of these diverse topic models remain as simple and efficient as the classical models. Experimental results demonstrate that the diverse topic models not only discover topics that better summarize the data, but also require minimal prior knowledge of the users.", "output": "Less is More: Learning Prominent and Diverse Topics for Data Summarization", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We explore beyond existing work on learning from demonstration by asking the question: \u201cCan robots learn to teach?\u201d, that is, can a robot autonomously learn an instructional policy from expert demonstration and use it to instruct or collaborate with humans in executing complex tasks in uncertain environments? In this paper we pursue a solution to this problem by leveraging the idea that humans often implicitly decompose a higher level task into several subgoals whose execution brings the task closer to completion. We propose Dirichlet process based non-parametric Inverse Reinforcement Learning (DPMIRL) approach for reward based unsupervised clustering of task space into subgoals. This approach is shown to capture the latent subgoals that a human teacher would have utilized to train a novice. The notion of \u201caction primitive\u201d is introduced as the means to communicate instruction policy to humans in the least complicated manner, and as a computationally efficient tool to segment demonstration data. We evaluate our approach through experiments on hydraulic actuated scaled model of an excavator and evaluate and compare different teaching strategies utilized by the robot.", "output": "Can Co-robots Learn to Teach?", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We show that strategies implemented in automatic theorem proving involve an interesting tradeoff between execution speed, proving speedup/computational time and usefulness of information. We advance formal definitions for these concepts by way of a notion of normality related to an expected (optimal) theoretical speedup when adding useful information (other theorems as axioms), as compared with actual strategies that can be effectively and efficiently implemented. We propose the existence of an ineluctable tradeoff between this normality and computational time complexity. The argument quantifies the usefulness of information in terms of (positive) speed-up. The results disclose a kind of no-free-lunch scenario and a tradeoff of a fundamental nature. The main theorem in this paper together with the numerical experiment\u2014undertaken using two different automatic theorem provers (AProS and Prover9) on random theorems of propositional logic\u2014provide strong theoretical and empirical arguments for the fact that finding new useful information for solving a specific problem (theorem) is, in general, as hard as the problem (theorem) itself.", "output": "Rare Speed-up in Automatic Theorem Proving Reveals Tradeoff Between Computational Time and Information Value", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Given an image, humans effortlessly run the image formation process backwards in their minds: they can tell albedo from shading, foreground from background, and imagine the occluded parts of the scene behind foreground objects. In this work, we propose a weakly supervised inversion machine trained to generate similar imaginations that when rendered using differentiable, graphics-like decoders, produce the original visual input. We constrain the imagination spaces by providing exemplar memory repositories in the form of foreground segmented objects, albedo, shading, background scenes and imposing adversarial losses on the imagination spaces. Our model learns to perform such inversion with weak supervision, without ever having seen paired annotated data, that is, without having seen the image paired with the corresponding ground-truth imaginations. We demonstrate our method by applying it to three Computer Vision tasks: image in-painting, intrinsic decomposition and object segmentation, each task having its own differentiable renderer. Data driven adversarial imagination priors effectively guide inversion, minimize the need for hand designed priors of smoothness or good continuation, or the need for paired annotated data. Consider Figure 1. We imagine a missing triangle occluding three small black circles rather than three carefully arranged pacman shapes \u2013 which is what the pixels depict. In (b), we do not perceive two parts of the sea separated by a standing person, rather a continuous sea landscape. In (c), we explain the input as a \u201dmasked 8\u201d rather than two semicircles. Consistent explanations of visual observations in terms of familiar concepts and memories we call \u201cimaginations\u201d. Imaginations invert the image formation process and propose 3D shape, camera pose, scene layering, spatial layout, albedo, shading, inpainted, un-occluded perceptions of the world, necessary for the understanding of the visual scene and interaction with it. Gestalt philosophers (Smith (1988)) proposed a set or principles to explain formation of such percepts, such as, closure, center surround pop-out, good continuity, smoothness etc, which many works attempt to hand design principles to incorporate those into computational frameworks of e.g., perceptual grouping (Yu (2003)). In this work, we present a learning-based inversion model that uses data-driven priors instead. We propose a computational model that addresses inverse problems in Computer Vision using adversarial imagination priors. Figure 2 illustrates our model. It is comprised of a generator neural network that given a visual input predicts visual imaginations, such as, in-painted image, un-occluded background scene, object segmentation, albedo and shading etc. Relevant memories, assumed to +", "output": "ADVERSARIAL IMAGINATION PRIORS", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "It is well known that conditional indepen\u00ad dence can be used to factorize a joint prob\u00ad ability into a multiplication of conditional probabilities. This paper proposes a con\u00ad structive definition of intercausal indepen\u00ad dence, which can be used to further factorize a conditional probability. An inference algo\u00ad rithm is developed, which makes use of both conditional independence and intercausal in\u00ad dependence to reduce inference complexity in Bayesian networks.", "output": "Intercausal Independence and Heterogeneous Factorization", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Optunity is a free software package dedicated to hyperparameter optimization. It contains various types of solvers, ranging from undirected methods to direct search, particle swarm and evolutionary optimization. The design focuses on ease of use, flexibility, code clarity and interoperability with existing software in all machine learning environments. Optunity is written in Python and contains interfaces to environments such as R and MATLAB. Optunity uses a BSD license and is freely available online at http://www.optunity.net.", "output": "Easy Hyperparameter Search Using Optunity", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "As demand drives systems to generalize to various domains and problems, the study of multitask, transfer and lifelong learning has become an increasingly important pursuit. In discrete domains, performance on the Atari game suite has emerged as the de facto benchmark for assessing multitask learning. However, in continuous domains there is a lack of agreement on standard multitask evaluation environments which makes it difficult to compare different approaches fairly. In this work, we describe a benchmark set of tasks that we have developed in an extendable framework based on OpenAI Gym. We run a simple baseline using Trust Region Policy Optimization and release the framework publicly to be expanded and used for the systematic comparison of multitask, transfer, and lifelong learning in continuous domains.", "output": "Benchmark Environments for Multitask Learning in Continuous Domains", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Planning is a notoriously difficult computational problem of high worst-case complexity. Researchers have been investing significant efforts to develop heuristics or restrictions to make planning practically feasible. Case-based planning is a heuristic approach where one tries to reuse previous experience when solving similar problems in order to avoid some of the planning effort. Plan reuse may offer an interesting alternative to plan generation in some settings. We provide theoretical results that identify situations in which plan reuse is provably tractable. We perform our analysis in the framework of parameterized complexity, which supports a rigorous worst-case complexity analysis that takes structural properties of the input into account in terms of parameters. A central notion of parameterized complexity is fixed-parameter tractability which extends the classical notion of polynomial-time tractability by utilizing the effect of structural properties of the problem input. We draw a detailed map of the parameterized complexity landscape of several variants of problems that arise in the context of case-based planning. In particular, we consider the problem of reusing an existing plan, imposing various restrictions in terms of parameters, such as the number of steps that can be added to the existing plan to turn it into a solution of the planning instance at hand.", "output": "Parameterized Complexity Results for Plan Reuse", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Spectral methods have greatly advanced the estimation of latent variable models, generating a sequence of novel and efficient algorithms with strong theoretical guarantees. However, current spectral algorithms are largely restricted to mixtures of discrete or Gaussian distributions. In this paper, we propose a kernel method for learning multi-view latent variable models, allowing each mixture component to be nonparametric. The key idea of the method is to embed the joint distribution of a multi-view latent variable into a reproducing kernel Hilbert space, and then the latent parameters are recovered using a robust tensor power method. We establish that the sample complexity for the proposed method is quadratic in the number of latent components and is a low order polynomial in the other relevant parameters. Thus, our non-parametric tensor approach to learning latent variable models enjoys good sample and computational efficiencies. Moreover, the non-parametric tensor power method compares favorably to EM algorithm and other existing spectral algorithms in our experiments.", "output": "Nonparametric Estimation of Multi-View Latent Variable Models", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In this memory we made the design of an indexing model for Arabic language and adapting standards for describing learning resources used (the LOM and their application profiles) with learning conditions such as levels education of students, their levels of understanding... the pedagogical context with taking into account the representative elements of the text, text's length,... in particular, we highlight the specificity of the Arabic language which is a complex language, characterized by its flexion, its voyellation and its agglutination. Keyword: indexing model, pedagogical indexation, complexity of the Arabic language, standard description of educational resources, pedagogical context, intrinsic and extrinsic properties, indexing text, prism, facet.", "output": "Developing a model for a text database indexed pedagogically for teaching the Arabic language", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "To achieve acceptable performance for AI tasks, one can either use sophisticated feature extraction methods as the first layer in a twolayered supervised learning model, or learn the features directly using a deep (multilayered) model. While the first approach is very problem-specific, the second approach has computational overheads in learning multiple layers and fine-tuning of the model. In this paper, we propose an approach called wide learning based on arc-cosine kernels, that learns a single layer of infinite width. We propose exact and inexact learning strategies for wide learning and show that wide learning with single layer outperforms single layer as well as deep architectures of finite width for some benchmark datasets.", "output": "To go deep or wide in learning?", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We propose a supervised machine learning approach for boosting existing signal and image recovery methods and demonstrate its efficacy on example of image reconstruction in computed tomography. Our technique is based on a local nonlinear fusion of several image estimates, all obtained by applying a chosen reconstruction algorithm with different values of its control parameters. Usually such output images have different bias/variance trade-off. The fusion of the images is performed by feed-forward neural network trained on a set of known examples. Numerical experiments show an improvement in reconstruction quality relatively to existing direct and iterative reconstruction methods.", "output": "Spatially-Adaptive Reconstruction in Computed Tomography using Neural Networks", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We present and analyze an agnostic active learning algorithm that works without keeping a version space. This is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning, and only hypotheses from this set are ever returned. By avoiding this version space approach, our algorithm sheds the computational burden and brittleness associated with maintaining version spaces, yet still allows for substantial improvements over supervised learning for classification.", "output": "Agnostic Active Learning Without Constraints", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In this paper, we present a joint compression and classification approach of EEG and EMG signals using a deep learning approach. Specifically, we build our system based on the deep autoencoder architecture which is designed not only to extract discriminant features in the multimodal data representation but also to reconstruct the data from the latent representation using encoder-decoder layers. Since autoencoder can be seen as a compression approach, we extend it to handle multimodal data at the encoder layer, reconstructed and retrieved at the decoder layer. We show through experimental results, that exploiting both multimodal data intercorellation and intracorellation 1) Significantly reduces signal distortion particularly for high compression levels 2) Achieves better accuracy in classifying EEG and EMG signals recorded and labeled according to the sentiments of the volunteer.", "output": "Multimodal deep learning approach for joint EEG-EMG data compression and classification", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Filters in a convolutional network are typically parametrized in a pixel basis. As an orthonormal basis, pixels may represent any arbitrary vector in R. In this paper, we relax this orthonormality requirement and extend the set of viable bases to the generalized notion of frames. When applying suitable frame bases to ResNets on Cifar-10+ we demonstrate improved error rates by substitution only. By exploiting the transformation properties of such generalized bases, we arrive at steerable frames, that allow to continuously transform CNN filters under arbitrary Lie-groups. Further allowing us to locally separate pose from canonical appearance. We implement this in the Dynamic Steerable Frame Network, that dynamically estimates the transformations of filters, conditioned on its input. The derived method presents a hybrid of Dynamic Filter Networks and Spatial Transformer Networks that can be implemented in any convolutional architecture, as we illustrate in two examples. First, we illustrate estimation properties of steerable frames with a Dynamic Steerable Frame Network, compared to a Dynamic Filter Network on the task of edge detection, where we show clear advantages of the derived steerable frames. Lastly, we insert the Dynamic Steerable Frame Network as a module in a convolutional LSTM on the task of limited-data hand-gesture recognition from video and illustrate effective dynamic regularization and show clear advantages over Spatial Transformer Networks. In this paper, we have laid out the foundations of Frame-based convolutional networks and Dynamic Steerable Frame Networks while illustrating their advantages for continuously transforming features and data-efficient learning.", "output": "DYNAMIC STEERABLE FRAME NETWORKS", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We provide the first extensive evaluation of how using different types of context to learn skip-gram word embeddings affects performance on a wide range of intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic tasks tend to exhibit a clear preference to particular types of contexts and higher dimensionality, more careful tuning is required for finding the optimal settings for most of the extrinsic tasks that we considered. Furthermore, for these extrinsic tasks, we find that once the benefit from increasing the embedding dimensionality is mostly exhausted, simple concatenation of word embeddings, learned with different context types, can yield further performance gains. As an additional contribution, we propose a new variant of the skip-gram model that learns word embeddings from weighted contexts of substitute words.", "output": "The Role of Context Types and Dimensionality in Learning Word Embeddings", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We used MetaMap and YTEX as a basis for the construction of two separate systems to participate in the 2013 ShARe/CLEF eHealth Task 1[9], the recognition of clinical concepts. No modifications were directly made to these systems, but output concepts were filtered using stop concepts, stop concept text and UMLS semantic type. Concept boundaries were also adjusted using a small collection of rules to increase precision on the strict task. Overall MetaMap had better performance than YTEX on the strict task, primarily due to a 20% performance improvement in precision. In the relaxed task YTEX had better performance in both precision and recall giving it an overall F-Score 4.6% higher than MetaMap on the test data. Our results also indicated a 1.3% higher accuracy for YTEX in UMLS CUI mapping.", "output": "Evaluation of YTEX and MetaMap for clinical concept recognition", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Resolving abstract anaphora is an important, but difficult task for text understanding. Yet, with recent advances in representation learning this task becomes a more tangible aim. A central property of abstract anaphora is that it establishes a relation between the anaphor embedded in the anaphoric sentence and its (typically non-nominal) antecedent. We propose a mention-ranking model that learns how abstract anaphors relate to their antecedents with an LSTM-Siamese Net. We overcome the lack of training data by generating artificial anaphoric sentence\u2013 antecedent pairs. Our model outperforms state-of-the-art results on shell noun resolution. We also report first benchmark results on an abstract anaphora subset of the ARRAU corpus. This corpus presents a greater challenge due to a mixture of nominal and pronominal anaphors and a greater range of confounders. We found model variants that outperform the baselines for nominal anaphors, without training on individual anaphor data, but still lag behind for pronominal anaphors. Our model selects syntactically plausible candidates and \u2013 if disregarding syntax \u2013 discriminates candidates using deeper features.", "output": "A Mention-Ranking Model for Abstract Anaphora Resolution", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Deep neural networks (DNNs) are powerful types of artificial neural networks (ANNs) that use several hidden layers. They have recently gained considerable attention in the speech transcription and image recognition community (Krizhevsky et al., 2012) for their superior predictive properties including robustness to overfitting. However their application to algorithmic trading has not been previously researched, partly because of their computational complexity. This paper describes the application of DNNs to predicting financial market movement directions. In particular we describe the configuration and training approach and then demonstrate their application to backtesting a simple trading strategy over 43 different Commodity and FX future mid-prices at 5-minute intervals. All results in this paper are generated using a C++ implementation on the Intel Xeon Phi co-processor which is 11.4x faster than the serial version and a Python strategy backtesting environment both of which are available as open source code written by the authors.", "output": "Classification-based Financial Markets Prediction using Deep Neural Networks", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Precise geocoding and time normalization for text requires that location and time phrases be identified. Many state-of-the-art geoparsers and temporal parsers suffer from low recall. Categories commonly missed by parsers are: nouns used in a nonspatiotemporal sense, adjectival and adverbial phrases, prepositional phrases, and numerical phrases. We collected and annotated data set by querying commercial web searches API with such spatiotemporal expressions as were missed by state-of-theart parsers. Due to the high cost of sentence annotation, active learning was used to label training data, and a new strategy was designed to better select training examples to reduce labeling cost. For the learning algorithm, we applied an average perceptron trained Featurized Hidden Markov Model (FHMM). Five FHMM instances were used to create an ensemble, with the output phrase selected by voting. Our ensemble model was tested on a range of sequential labeling tasks, and has shown competitive performance. Our contributions include (1) an new dataset annotated with named entities and expanded spatiotemporal expressions; (2) a comparison of inference algorithms for ensemble models showing the superior accuracy of Belief Propagation over Viterbi Decoding; (3) a new example re-weighting method for active ensemble learning that \u201cmemorizes\u201d the latest examples trained; (4) a spatiotemporal parser that jointly recognizes expanded spatiotemporal expressions as well as named entities.", "output": "Recognizing Extended Spatiotemporal Expressions by Actively Trained Average Perceptron Ensembles", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The GLEU metric was proposed for evaluating grammatical error corrections using n-gram overlap with a set of reference sentences, as opposed to precision/recall of specific annotated errors (Napoles et al., 2015). This paper describes improvements made to the GLEU metric that address problems that arise when using an increasing number of reference sets. Unlike the originally presented metric, the modified metric does not require tuning. We recommend that this version be used instead of the original version.1", "output": "GLEU Without Tuning", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We present a novel neural model HyperVec to learn hierarchical embeddings for hypernymy detection and directionality. While previous embeddings have shown limitations on prototypical hypernyms, HyperVec represents an unsupervised measure where embeddings are learned in a specific order and capture the hypernym\u2013hyponym distributional hierarchy. Moreover, our model is able to generalize over unseen hypernymy pairs, when using only small sets of training data, and by mapping to other languages. Results on benchmark datasets show that HyperVec outperforms both state-of-theart unsupervised measures and embedding models on hypernymy detection and directionality, and on predicting graded lexical entailment.", "output": "Hierarchical Embeddings for Hypernymy Detection and Directionality", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Multicriteria decision analysis aims at supporting a person facing a decision problem involving conflicting criteria. We consider an additive utility model which provides robust conclusions based on preferences elicited from the decision maker. The recommendations based on these robust conclusions are even more convincing if they are complemented by explanations. We propose a general scheme, based on sequence of preference swaps, in which explanations can be computed. We show first that the length of explanations can be unbounded in the general case. However, in the case of binary reference scales, this length is bounded and we provide an algorithm to compute the corresponding explanation.", "output": "Explaining robust additive utility models by sequences of preference swaps", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The rapid advancement of machine learning techniques has re-energized research into general artificial intelligence. While the idea of domain-agnostic meta-learning is appealing, this emerging field must come to terms with its relationship to human cognition and the statistics and structure of the tasks humans perform. The position of this article is that only by aligning our agents\u2019 abilities and environments with those of humans do we stand a chance at developing general artificial intelligence (GAI).", "output": "Minimally Naturalistic Artificial Intelligence", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "This paper discusses real-time alignment of audio signals of music performance to the corresponding score (a.k.a. score following) which can handle tempo changes, errors and arbitrary repeats and/or skips (repeats/skips) in performances. This type of score following is particularly useful in automatic accompaniment for practices and rehearsals, where errors and repeats/skips are often made. Simple extensions of the algorithms previously proposed in the literature are not applicable in these situations for scores of practical length due to the problem of large computational complexity. To cope with this problem, we present two hidden Markov models of monophonic performance with errors and arbitrary repeats/skips, and derive efficient score-following algorithms with an assumption that the prior probability distributions of score positions before and after repeats/skips are independent from each other. We confirmed real-time operation of the algorithms with music scores of practical length (around 10000 notes) on a modern laptop and their tracking ability to the input performance within 0.7 s on average after repeats/skips in clarinet performance data. Further improvements and extension for polyphonic signals are also discussed. Keywords\u2014Score following, audio-to-score alignment, arbitrary repeats and skips, fast Viterbi algorithm, hidden Markov model, music signal processing", "output": "Real-Time Audio-to-Score Alignment of Music Performances Containing Errors and Arbitrary Repeats and Skips", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various highdimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.", "output": "#Exploration:A Study of Count-Based Explorationfor Deep Reinforcement Learning", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Learning. Abstract: Document categorization is a technique where the category of a document is determined. In this paper three well-known supervised learning techniques which are Support Vector Machine(SVM), Na\u00efve Bayes(NB) and Stochastic Gradient Descent(SGD) compared for Bengali document categorization. Besides classifier, classification also depends on how feature is selected from dataset. For analyzing those classifier performances on predicting a document against twelve categories several feature selection techniques are also applied in this article namely Chi square distribution, normalized TFIDF (term frequency-inverse document frequency) with word analyzer. So, we attempt to explore the efficiency of those three-classification algorithms by using two different feature selection techniques in this article.", "output": "A Comparative Study on Different Types of Approaches to Bengali document Categorization", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In educational technology and learning sciences, there are multiple uses for a predictive model of whether a student will perform a task correctly or not. For example, an intelligent tutoring system may use such a model to estimate whether or not a student has mastered a skill. We analyze the significance of data recency in making such predictions, i.e., asking whether relatively more recent observations of a student\u2019s performance matter more than relatively older observations. We develop a new Recent-Performance Factors Analysis model that takes data recency into account. The new model significantly improves predictive accuracy over both existing logistic-regression performance models and over novel baseline models in evaluations on real-world and synthetic datasets. As a secondary contribution, we demonstrate how the widely used cross-validation with 0-1 loss is inferior to AIC and to cross-validation with L1 prediction error loss as a measure of model performance.", "output": "Predicting Performance During Tutoring with Models of Recent Performance", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Conventional dependency parsers rely on a statistical model and a transition system or graph algorithm to enforce tree-structured outputs during training and inference. In this work we formalize dependency parsing as the problem of selecting the head (a.k.a. parent) of each word in a sentence. Our model which we call DENSE (as shorthand for Dependency Neural Selection) employs bidirectional recurrent neural networks for the head selection task. Without enforcing any structural constraints during training, DENSE generates (at inference time) trees for the overwhelming majority of sentences (95% on an English dataset), while remaining non-tree outputs can be adjusted with a maximum spanning tree algorithm. We evaluate DENSE on four languages (English, Chinese, Czech, and German) with varying degrees of non-projectivity. Despite the simplicity of our approach, experiments show that the resulting parsers are on par with or outperform the state of the art.", "output": "Dependency Parsing as Head Selection", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Driven by the multi-level structure of human intracranial electroencephalogram (iEEG) recordings of epileptic seizures, we introduce a new variant of a hierarchical Dirichlet Process\u2014the multi-level clustering hierarchical Dirichlet Process (MLC-HDP)\u2014that simultaneously clusters datasets on multiple levels. Our seizure dataset contains brain activity recorded in typically more than a hundred individual channels for each seizure of each patient. The MLC-HDP model clusters over channels-types, seizure-types, and patient-types simultaneously. We describe this model and its implementation in detail. We also present the results of a simulation study comparing the MLC-HDP to a similar model, the Nested Dirichlet Process and finally demonstrate the MLC-HDP\u2019s use in modeling seizures across multiple patients. We find the MLC-HDP\u2019s clustering to be comparable to independent human physician clusterings. To our knowledge, the MLCHDP model is the first in the epilepsy literature capable of clustering seizures within and between patients.", "output": "A Hierarchical Dirichlet Process Model with Multiple Levels of Clustering for Human EEG Seizure Modeling", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Bayesian optimization has proven to be a highly effective methodology for the global optimization of unknown, expensive and multimodal functions. The ability to accurately model distributions over functions is critical to the effectiveness of Bayesian optimization. Although Gaussian processes provide a flexible prior over functions, there are various classes of functions that remain difficult to model. One of the most frequently occurring of these is the class of non-stationary functions. The optimization of the hyperparameters of machine learning algorithms is a problem domain in which parameters are often manually transformed a priori, for example by optimizing in \u201clog-space,\u201d to mitigate the effects of spatially-varying length scale. We develop a methodology for automatically learning a wide family of bijective transformations or warpings of the input space using the Beta cumulative distribution function. We further extend the warping framework to multi-task Bayesian optimization so that multiple tasks can be warped into a jointly stationary space. On a set of challenging benchmark optimization tasks, we observe that the inclusion of warping greatly improves on the state-of-the-art, producing better results faster and more reliably.", "output": "INPUT WARPING FOR BAYESIAN OPTIMIZATION OF NON-STATIONARY FUNCTIONS BY JASPER SNOEK", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In 2015, stroke was the number one cause of death in Indonesia. The majority type of stroke is ischemic. The standard tool for diagnosing stroke is CT-Scan. For developing countries like Indonesia, the availability of CT-Scan is very limited and still relatively expensive. Because of the availability, another device that potential to diagnose stroke in Indonesia is EEG. Ischemic stroke occurs because of obstruction that can make the cerebral blood flow (CBF) on a person with stroke has become lower than CBF on a normal person (control) so that the EEG signal have a deceleration. On this study, we perform the ability of 1D Convolutional Neural Network (1DCNN) to construct classification model that can distinguish the EEG and EOG stroke data from EEG and EOG control data. To accelerate training process our model we use Batch Normalization. Involving 62 person data object and from leave one out the scenario with five times repetition of measurement we obtain the average of accuracy 0.86 (F-Score 0.861) only at 200 epoch. This result is better than all over shallow and popular classifiers as the comparator (the best result of accuracy 0.69 and F-Score 0.72 ). The feature used in our study were only 24 \u2018handcrafted\u2019 feature with simple feature extraction process.", "output": "Ischemic Stroke Identification Based on EEG and EOG using 1D Convolutional Neural Network and Batch Normalization", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Given an existing trained neural network, it is often desirable to be able to add new capabilities without hindering performance of already learned tasks. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network. We propose a method which fully preserves performance on the original task, with only a small increase (around 20%) in the number of required parameters while performing on par with more costly finetuning procedures, which typically double the number of parameters. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method and explore different aspects of its behavior.", "output": "Incremental Learning Through Deep Adaptation", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Prepositions are very common and very ambiguous, and understanding their sense is critical for understanding the meaning of the sentence. Supervised corpora for the preposition-sense disambiguation task are small, suggesting a semi-supervised approach to the task. We show that signals from unannotated multilingual data can be used to improve supervised prepositionsense disambiguation. Our approach pre-trains an LSTM encoder for predicting the translation of a preposition, and then incorporates the pre-trained encoder as a component in a supervised classification system, and fine-tunes it for the task. The multilingual signals consistently improve results on two preposition-sense datasets.", "output": "Semi Supervised Preposition-Sense Disambiguation using Multilingual Data", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We propose a pool-based non-parametric active learning algorithm for general metric spaces, called MArgin Regularized Metric Active Nearest Neighbor (MARMANN), which outputs a nearest-neighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARMANN is significantly lower than that of any passive learner with similar error guarantees. MARMANN is based on a generalized sample compression scheme, and a new label-efficient active model-selection procedure.", "output": "Active Nearest-Neighbor Learning in Metric Spaces", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "While known algorithms for sensitivity analysis and parameter tuning in probabilistic networks have a running time that is exponential in the size of the network, the exact computational complexity of these problems has not been established as yet. In this paper we study several variants of the tuning problem and show that these problems are NP-complete in general. We further show that the problems remain NP-complete or PP-complete, for a number of restricted variants. These complexity results provide insight in whether or not recent achievements in sensitivity analysis and tuning can be extended to more general, practicable methods.", "output": "The Computational Complexity of Sensitivity Analysis and Parameter Tuning", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "This paper proposes DRL-Sense\u2014a multisense word representation learning model, to address the word sense ambiguity issue, where a sense selection module and a sense representation module are jointly learned in a reinforcement learning fashion. A novel reward passing procedure is proposed to enable joint training on the selection and representation modules. The modular design implements pure senselevel representation learning with linear time sense selection (decoding). We further develop a non-parametric learning algorithm and a sense exploration mechanism for better flexibility and robustness. The experiments on benchmark data show that the proposed approach achieves the state-of-the-art performance on contextual word similarities and comparable performance with Google\u2019s word2vec while using much less training data.", "output": "DRL-Sense: Deep Reinforcement Learning for Multi-Sense Word Representations", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Parsimony, including sparsity and low rank, has been shown to successfully model data in numerous machine learning and signal processing tasks. Traditionally, such modeling approaches rely on an iterative algorithm that minimizes an objective function with parsimony-promoting terms. The inherently sequential structure and data-dependent complexity and latency of iterative optimization constitute a major limitation in many applications requiring real-time performance or involving large-scale data. Another limitation encountered by these modeling techniques is the difficulty of their inclusion in discriminative learning scenarios. In this work, we propose to move the emphasis from the model to the pursuit algorithm, and develop a process-centric view of parsimonious modeling, in which a learned deterministic fixed-complexity pursuit process is used in lieu of iterative optimization. We show a principled way to construct learnable pursuit process architectures for structured sparse and robust low rank models, derived from the iteration of proximal descent algorithms. These architectures learn to approximate the exact parsimonious representation at a fraction of the complexity of the standard optimization methods. We also show that appropriate training regimes allow to naturally extend parsimonious models to discriminative settings. State-ofthe-art results are demonstrated on several challenging problems in image and audio processing with several orders of magnitude speedup compared to the exact optimization algorithms. \u2217P. Sprechmann and G. Sapiro are with the Department of Electrical and Computer Engineering, Duke University, Durham 27708, USA. Email: pablo.sprechmann@duke.edu, guillermo.sapiro@duke.edu. \u2020A. M. Bronsteind is with School of Electrical Engineering, Tel Aviv University, Tel Aviv 69978, Israel.Email: bron@eng.tau.ac.il. \u2021Work partially supported by NSF, ONR, NGA, DARPA, AFOSR, ARO, and BSF. 1 ar X iv :1 21 2. 36 31 v1 [ cs .L G ] 1 4 D ec 2 01 2", "output": "Learning Efficient Sparse and Low Rank Models", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four families of problems for which some of the commonly used existing algorithms fail or suffer significant difficulty. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.", "output": "Failures of Deep Learning", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Many studies on the cost-sensitive learning assumed that a unique cost matrix is known for a problem. However, this assumption may not hold for many real-world problems. For example, a classifier might need to be applied in several circumstances, each of which associates with a different cost matrix. Or, different human experts have different opinions about the costs for a given problem. Motivated by these facts, this study aims to seek the minimax classifier over multiple cost matrices. In summary, we theoretically proved that, no matter how many cost matrices are involved, the minimax problem can be tackled by solving a number of standard cost-sensitive problems and sub-problems that involve only two cost matrices. As a result, a general framework for achieving minimax classifier over multiple cost matrices is suggested and justified by preliminary empirical studies.", "output": "Minimax Classifier for Uncertain Costs", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The paper studies machine learning problems where each example is described using a set of Boolean features and where hypotheses are represented by linear threshold elements. One method of increasing the expressiveness of learned hypotheses in this context is to expand the feature set to include conjunctions of basic features. This can be done explicitly or where possible by using a kernel function. Focusing on the well known Perceptron and Winnow algorithms, the paper demonstrates a tradeoff between the computational efficiency with which the algorithm can be run over the expanded feature space and the generalization ability of the corresponding learning algorithm. We first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efficiently run the Perceptron algorithm over a feature space of exponentially many conjunctions; however we also show that using such kernels, the Perceptron algorithm can provably make an exponential number of mistakes even when learning simple functions. We then consider the question of whether kernel functions can analogously be used to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. Known upper bounds imply that the Winnow algorithm can learn Disjunctive Normal Form (DNF) formulae with a polynomial mistake bound in this setting. However, we prove that it is computationally hard to simulate Winnow\u2019s behavior for learning DNF over such a feature set. This implies that the kernel functions which correspond to running Winnow for this problem are not efficiently computable, and that there is no general construction that can run Winnow with kernels.", "output": "Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We theoretically analyze and compare the following five popular multiclass classification methods: One vs. All, All Pairs, Tree-based classifiers, Error Correcting Output Codes (ECOC) with randomly generated code matrices, and Multiclass SVM. In the first four methods, the classification is based on a reduction to binary classification. We consider the case where the binary classifier comes from a class of VC dimension d, and in particular from the class of halfspaces over R. We analyze both the estimation error and the approximation error of these methods. Our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions. Our proof technique employs tools from VC theory to analyze the approximation error of hypothesis classes. This is in sharp contrast to most, if not all, previous uses of VC theory, which only deal with estimation error.", "output": "Multiclass Learning Approaches: A Theoretical Comparison with Implications", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Topic models provide a useful method for dimensionality reduction and exploratory data<lb>analysis in large text corpora. Most approaches to topic model inference have been based on<lb>a maximum likelihood objective. Efficient algorithms exist that approximate this objective,<lb>but they have no provable guarantees. Recently, algorithms have been introduced that provide<lb>provable bounds, but these algorithms are not practical because they are inefficient and not ro-<lb>bust to violations of model assumptions. In this paper we present an algorithm for topic model<lb>inference that is both provable and practical. The algorithm produces results comparable to the<lb>best MCMC implementations while running orders of magnitude faster.", "output": "A Practical Algorithm for Topic Modeling with Provable Guarantees", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We present an approach for the verification of feed-forward neural networks in which all nodes have a piece-wise linear activation function. Such networks are often used in deep learning and have been shown to be hard to verify for modern satisfiability modulo theory (SMT) and integer linear programming (ILP) solvers. The starting point of our approach is the addition of a global linear approximation of the overall network behavior to the verification problem that helps with SMT-like reasoning over the network behavior. We present a specialized verification algorithm that employs this approximation in a search process in which it infers additional node phases for the non-linear nodes in the network from partial node phase assignments, similar to unit propagation in classical SAT solving. We also show how to infer additional conflict clauses and safe node fixtures from the results of the analysis steps performed during the search. The resulting approach is evaluated on collision avoidance and handwritten digit recognition case studies.", "output": "Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "As the complexity of deep neural networks (DNNs) trend to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on functional hashing to compress DNNs, namely FunHashNN. For each entry in a deep net, FunHashNN uses multiple low-cost hash functions to fetch values in the compression space, and then employs a small reconstruction network to recover that entry. The reconstruction network is plugged into the whole network and trained jointly. FunHashNN includes the recently proposed HashedNets [7] as a degenerated case, and benefits from larger value capacity and less reconstruction loss. We further discuss extensions with dual space hashing and multi-hops. On several benchmark datasets, FunHashNN demonstrates high compression ratios with little loss on prediction accuracy.", "output": "Functional Hashing for Compressing Neural Networks", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We present a deep hierarchical recurrent neural network for sequence tagging. Given a sequence of words, our model employs deep gated recurrent units on both character and word levels to encode morphology and context information, and applies a conditional random field layer to predict the tags. Our model is task independent, language independent, and feature engineering free. We further extend our model to multi-task and crosslingual joint training by sharing the architecture and parameters. Our model achieves state-of-the-art results in multiple languages on several benchmark tasks including POS tagging, chunking, and NER. We also demonstrate that multi-task and cross-lingual joint training can improve the performance in various cases.", "output": "Multi-Task Cross-Lingual Sequence Tagging from Scratch", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In this work we study variance in the results of neural network training on a wide variety of configurations in automatic speech recognition. Although this variance itself is well known, this is, to the best of our knowledge, the first paper that performs an extensive empirical study on its effects in speech recognition. We view training as sampling from a distribution and show that these distributions can have a substantial variance. These results show the urgent need to rethink the way in which results in the literature are reported and interpreted.", "output": "Training variance and performance evaluation of neural networks in speech", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Or\u2019s of And\u2019s (OA) models are comprised of a small number of disjunctions of conjunctions, also called disjunctive normal form. An example of an OA model is as follows: If (x1 = \u2018blue\u2019 AND x2 = \u2018middle\u2019) OR (x1 = \u2018yellow\u2019), then predict Y = 1, else predict Y = 0. Or\u2019s of And\u2019s models have the advantage of being interpretable to human experts, since they are a set of conditions that concisely capture the characteristics of a specific subset of data. We present two optimization-based machine learning frameworks for constructing OA models, Optimized OA (OOA) and its faster version, Optimized OA with Approximations (OOAx). We prove theoretical bounds on the properties of patterns in an OA model. We build OA models as a diagnostic screening tool for obstructive sleep apnea, that achieves high accuracy with a substantial gain in interpretability over other methods.", "output": "Learning Optimized Or\u2019s of And\u2019s", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "This paper describes our participation in Task 5 track 2 of SemEval 2017 to predict the sentiment of financial news headlines for a specific company on a continuous scale between -1 and 1. We tackled the problem using a number of approaches, utilising a Support Vector Regression (SVR) and a Bidirectional Long Short-Term Memory (BLSTM). We found an improvement of 4-6% using the LSTM model over the SVR and came fourth in the track. We report a number of different evaluations using a finance specific word embedding model and reflect on the effects of using different evaluation metrics.", "output": "Lancaster A at SemEval-2017 Task 5: Evaluation metrics matter: predicting sentiment from financial news headlines", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Structured sparse optimization is an important and challenging problem for analyzing high-dimensional data in a variety of applications such as bioinformatics, medical imaging, social networks, and astronomy. Although a number of structured sparsity models have been explored, such as trees, groups, clusters, and paths, connected subgraphs have been rarely explored in the current literature. One of the main technical challenges is that there is no structured sparsity-inducing norm that can directly model the space of connected subgraphs, and there is no exact implementation of a projection oracle for connected subgraphs due to its NP-hardness. In this paper, we explore efficient approximate projection oracles for connected subgraphs, and propose two new efficient algorithms, namely, GRAPH-IHT and GRAPH-GHTP, to optimize a generic nonlinear objective function subject to connectivity constraint on the support of the variables. Our proposed algorithms enjoy strong guarantees analogous to several current methods for sparsity-constrained optimization, such as Projected Gradient Descent (PGD), Approximate Model Iterative Hard Thresholding (AM-IHT), and Gradient Hard Thresholding Pursuit (GHTP) with respect to convergence rate and approximation accuracy. We apply our proposed algorithms to optimize several well-known graph scan statistics in several applications of connected subgraph detection as a case study, and the experimental results demonstrate that our proposed algorithms outperform state-of-the-art methods.", "output": "Technical Report: Graph-Structured Sparse Optimization for Connected Subgraph Detection", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Reinforcement learning has been applied to many interesting problems such as the famous TD-gammon [1] and the inverted helicopter flight [2]. However little effort has been put into developing methods to learn policies for complex persistent tasks and tasks that are time-sensitive. In this paper we take a step towards solving this problem by using signal temporal logic (STL) as task specification, and taking advantage of the temporal abstraction feature that the options framework provide. We show via simulation that a relatively easy to implement algorithm that combines STL and options can learn a satisfactory policy with a small number of training cases.", "output": "A Hierarchical Reinforcement Learning Method for Persistent Time-Sensitive Tasks", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "It is now a common practice to compare models of human language processing by predicting participant reactions (such as reading times) to corpora consisting of rich naturalistic linguistic materials. However, many of the corpora used in these studies are based on naturalistic text and thus do not contain many of the low-frequency syntactic constructions that are often required to distinguish processing theories. Here we describe a new corpus consisting of English texts edited to contain many low-frequency syntactic constructions while still sounding fluent to native speakers. The corpus is annotated with hand-corrected parse trees and includes self-paced reading time data. Here we give an overview of the content of the corpus and release the data.1", "output": "The Natural Stories Corpus", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Dependency parsers are among the most crucial tools in natural language processing as they have many important applications in downstream tasks such as information retrieval, machine translation and knowledge acquisition. We introduce the Yara Parser, a fast and accurate open-source dependency parser based on the arc-eager algorithm and beam search. It achieves an unlabeled accuracy of 93.32 on the standard WSJ test set which ranks it among the top dependency parsers. At its fastest, Yara can parse about 4000 sentences per second when in greedy mode (1 beam). When optimizing for accuracy (using 64 beams and Brown cluster features), Yara can parse 45 sentences per second. The parser can be trained on any syntactic dependency treebank and different options are provided in order to make it more flexible and tunable for specific tasks. It is released with the Apache version 2.0 license and can be used for both commercial and academic purposes. The parser can be found at https: //github.com/yahoo/YaraParser.", "output": "Yara Parser: A Fast and Accurate Dependency Parser", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Background: Allowing patients to access their own electronic health record (EHR) notes through online patient portals has the potential to improve patient-centered care. However, EHR notes contain abundant medical jargon that can be difficult for patients to comprehend. One way to help patients is to reduce information overload and help them focus on medical terms that matter most to them. Targeted education can then be developed to improve patient EHR comprehension and the quality of care. Objective: The aim of this work was to develop FIT (Finding Important Terms for patients), an unsupervised natural language processing (NLP) system that ranks medical terms in EHR notes based on their importance to patients. Methods: We built FIT on a new unsupervised ensemble ranking model derived from the biased random walk algorithm to combine heterogeneous information resources for ranking candidate terms from each EHR note. Specifically, FIT integrates four single views (rankers) for term importance: patient use of medical concepts, document-level term salience, word-occurrence based term relatedness, and topic coherence. It also incorporates partial information of term importance as conveyed by terms\u2019 unfamiliarity levels and semantic types. We evaluated FIT on 90 expert-annotated EHR notes and used the four single-view rankers as baselines. In addition, we implemented three benchmark unsupervised ensemble ranking methods as strong baselines. Results: FIT achieved 0.885 AUC-ROC for ranking candidate terms from EHR notes to identify important terms. When including term identification, the performance of FIT for identifying important terms from EHR notes was 0.813 AUC-ROC. Both performance scores significantly exceeded the corresponding scores from the four single rankers (P<.001). FIT also outperformed the three ensemble rankers for most metrics. Its performance is relatively insensitive to its parameter. Conclusions: FIT can automatically identify EHR terms important to patients. It may help develop future interventions to improve quality of care. By using unsupervised learning as well as a robust and flexible framework for information fusion, FIT can be readily applied to other domains and applications.", "output": "Unsupervised Ensemble Ranking of Terms in Electronic Health Record Notes Based on Their Importance to Patients", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We address the statistical and optimization impacts of using classical sketch versus Hessian sketch to solve approximately the Matrix Ridge Regression (MRR) problem. Prior research has considered the effects of classical sketch on least squares regression (LSR), a strictly simpler problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR\u2014namely, it recovers nearly optimal solutions. In contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the \u201cmass\u201d in the responses and the optimal objective value. For both types of approximations, the regularization in the sketched MRR problem gives it significantly different statistical properties from the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the biases and variances of sketched MRR; these establish that the variance is significantly increased when classical sketches are used, while the bias is significantly increased when using Hessian sketches. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases this gap. Thus, in the distributed setting, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the statistical risks incurred by sketching.", "output": "Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Mental illness is one of the most pressing public health issues of our time. While counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. In this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. We develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. Applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes.", "output": "Large-scale Analysis of Counseling Conversations: An Application of Natural Language Processing to Mental Health", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences \u2013 patches of image pixels and high-level representations (\u201cpercepts\u201d) of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We try to visualize and interpret the learned features. We stress test the model by running it on longer time scales and on out-of-domain data. We further evaluate the representations by finetuning them for a supervised learning problem \u2013 human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only a few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.", "output": "Unsupervised Learning of Video Representations using LSTMs", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The cascade model is a well-established model of user interaction with content. In this work, we propose cascading bandits, a learning variant of the model where the objective is to learn K most attractive items out of L ground items. We cast the problem as a stochastic combinatorial bandit with a non-linear reward function and partially observed weights of items. Both of these are challenging in the context of combinatorial bandits. We propose two computationally-efficient algorithms for our problem, CascadeUCB1 and CascadeKL-UCB, and prove gap-dependent upper bounds on their regret. We also derive a lower bound for cascading bandits and show that it matches the upper bound of CascadeKL-UCB up to a logarithmic factor. Finally, we evaluate our algorithms on synthetic problems. Our experiments demonstrate that the algorithms perform well and robustly even when our modeling assumptions are violated.", "output": "Cascading Bandits", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Planning for distributed agents with partial state information is considered from a decision\u00ad theoretic perspective. We describe generaliza\u00ad tions of both the MDP and POMDP models that allow for decentralized control. For even a small number of agents, the finite-horizon prob\u00ad lems corresponding to both of our models are complete for nondeterministic exponential time. These complexity results illustrate a fundamen\u00ad tal difference between centralized and decentral\u00ad ized control of Markov processes. In contrast to the MDP and POMDP problems, the problems we consider provably do not admit polynomial\u00ad time algorithms and most likely require doubly exponential time to solve in the worst case. We have thus provided mathematical evidence corre\u00ad sponding to the intuition that decentralized plan\u00ad ning problems cannot easily be reduced to cen\u00ad tralized problems and solved exactly using estab\u00ad lished techniques.", "output": "The Complexity of Decentralized Control of Markov Decision Processes", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Inspired by the hierarchical hidden Markov models (HHMM), we present the hierarchical semi-Markov conditional random field (HSCRF), a generalisation of embedded undirected Markov chains to model complex hierarchical, nested Markov processes. It is parameterised in a discriminative framework and has polynomial time algorithms for learning and inference. Importantly, we consider partiallysupervised learning and propose algorithms for generalised partially-supervised learning and constrained inference. We demonstrate the HSCRF in two applications: (i) recognising human activities of daily living (ADLs) from indoor surveillance cameras, and (ii) noun-phrase chunking. We show that the HSCRF is capable of learning rich hierarchical models with reasonable accuracy in both fully and partially observed data cases.", "output": "Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. After a linear transformation of the data, each component is normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and an additive constant. We optimize the parameters of this transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. We find that the optimized transformation successfully Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than previous methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We also demonstrate the use of the model as a prior density in removing additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized (unsupervised) using the same Gaussianization objective, to capture additional probabilistic structure.", "output": "GENERALIZED NORMALIZATION TRANSFORMATION", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of 108\u00d7 and 17.7\u00d7 respectively, proving that it outperforms the recent pruning method by considerable margins. The code will be made publicly available.", "output": "Dynamic Network Surgery for Efficient DNNs", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The bootstrap provides a simple and powerful means of assessing the quality of estimators. However, in settings involving large datasets, the computation of bootstrap-based quantities can be prohibitively demanding. As an alternative, we present the Bag of Little Bootstraps (BLB), a new procedure which incorporates features of both the bootstrap and subsampling to obtain a robust, computationally efficient means of assessing estimator quality. BLB is well suited to modern parallel and distributed computing architectures and retains the generic applicability, statistical efficiency, and favorable theoretical properties of the bootstrap. We provide the results of an extensive empirical and theoretical investigation of BLB\u2019s behavior, including a study of its statistical correctness, its largescale implementation and performance, selection of hyperparameters, and performance on real data.", "output": "The Big Data Bootstrap", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We develop a general duality between neural networks and compositional kernels, striving towards a better understanding of deep learning. We show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space. Hence, though the training objective is hard to optimize in the worst case, the initial weights form a good starting point for optimization. Our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power. \u2217Email: amitdaniely@google.com \u2020Email: rf@cs.stanford.edu. Work performed at Google. \u2021Email: singer@google.com ar X iv :1 60 2. 05 89 7v 1 [ cs .L G ] 1 8 Fe b 20 16", "output": "Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Consider a weighted or unweighted k-nearest neighbor graph that has been built on n data points drawn randomly according to some density p on R. We study the convergence of the shortest path distance in such graphs as the sample size tends to infinity. We prove that for unweighted kNN graphs, this distance converges to an unpleasant distance function on the underlying space whose properties are detrimental to machine learning. We also study the behavior of the shortest path distance in weighted kNN graphs.", "output": "Shortest path distance in random k-nearest neighbor graphs", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs). We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality. This representation also enables a new form of stochastic regularization by randomized modification of resolution. We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling. Finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training.", "output": "Spectral Representations for Convolutional Neural Networks", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Graph models are relevant in many fields, such as distributed computing, intelligent tutoring systems or social network analysis. In many cases, such models need to take changes in the graph structure into account, i.e. a varying number of nodes or edges. Predicting such changes within graphs can be expected to yield important insight with respect to the underlying dynamics, e.g. with respect to user behaviour. However, predictive techniques in the past have almost exclusively focused on single edges or nodes. In this contribution, we attempt to predict the future state of a graph as a whole. We propose to phrase time series prediction as a regression problem and apply dissimilarityor kernel-based regression techniques, such as 1-nearest neighbor, kernel regression and Gaussian process regression, which can be applied to graphs via graph kernels. The output of the regression is a point embedded in a pseudo-Euclidean space, which can be analyzed using subsequent dissimilarityor kernel-based processing methods. We discuss strategies to speed up Gaussian Processes regression from cubic to linear time and evaluate our approach on two well-established theoretical models of graph evolution as well as two real data sets from the domain of intelligent tutoring systems. We find that simple regression methods, such as kernel regression, are sufficient to capture the dynamics in the theoretical models, but that Gaussian process regression significantly improves the prediction error for real-world data.", "output": "Time Series Prediction for Graphs in Kernel and Dissimilarity Spaces\u2217\u2020", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In multilingual question answering, either the question needs to be translated into the document language, or vice versa. In addition to direction, there are multiple methods to perform the translation, four of which we explore in this paper: word-based, 10-best, contextbased, and grammar-based. We build a feature for each combination of translation direction and method, and train a model that learns optimal feature weights. On a large forum dataset consisting of posts in English, Arabic, and Chinese, our novel learn-to-translate approach was more effective than a strong baseline (p < 0.05): translating all text into English, then training a classifier based only on English (original or translated) text.", "output": "Learning to Translate for Multilingual Question Answering", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "It has recently been shown that supervised learning with the popular logistic loss is equivalent to optimizing the exponential loss over sufficient statistics about the class: Rademacher observations (rados). We first show that this unexpected equivalence can actually be generalized to other example / rado losses, with necessary and sufficient conditions for the equivalence, exemplified on four losses that bear popular names in various fields: exponential (boosting), mean-variance (finance), Linear Hinge (on-line learning), ReLU (deep learning), and unhinged (statistics). Second, we show that the generalization unveils a surprising new connection to regularized learning, and in particular a sufficient condition under which regularizing the loss over examples is equivalent to regularizing the rados (with Minkowski sums) in the equivalent rado loss. This brings simple and powerful rado-based learning algorithms for sparsity-controlling regularization, that we exemplify on a boosting algorithm for the regularized exponential rado-loss, which formally boosts over four types of regularization, including the popular ridge and lasso, and the recently coined SLOPE \u2014 we obtain the first proven boosting algorithm for this last regularization. Through our first contribution on the equivalence of rado and example-based losses, \u03a9R.ADABOOST appears to be an efficient proxy to boost the regularized logistic loss over examples using whichever of the four regularizers (and any linear combination of them, e.g., for elastic net regularization). We are not aware of any regularized logistic loss formal boosting algorithm with such a wide spectrum of regularizers. Experiments display that regularization consistently improves performances of rado-based learning, and may challenge or beat the state of the art of example-based learning even when learning over small sets of rados. Finally, we connect regularization to \u03b5-differential privacy, and display how tiny budgets (e.g. \u03b5 < 10) can be afforded on big domains while beating (protected) example-based learning.", "output": "Learning Games and Rademacher Observations Losses", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Recurrent Neural Networks (RNN) have obtained excellent result in many natural language processing (NLP) tasks. However, understanding and interpreting the source of this success remains a challenge. In this paper, we propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only amplifies the power of RNN but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data. We demonstrate the power of RMN on language modeling and sentence completion tasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM) network on three large German, Italian, and English dataset. Additionally we perform indepth analysis of various linguistic dimensions that RMN captures. On Sentence Completion Challenge, for which it is essential to capture sentence coherence, our RMN obtains 69.2% accuracy, surpassing the previous stateof-the-art by a large margin.", "output": "Recurrent Memory Network for Language Modeling", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We introduce a new interpretation of two re\u00ad lated notions conditional utility and utility independence. Unlike the traditional inter\u00ad pretation, the new interpretation render the notions the direct analogues of their prob\u00ad abilistic counterparts. To capture these no\u00ad tions formally, we appeal to the notion of util\u00ad ity distribution, introduced in previous paper. We show that utility distributions, which have a structure that is identical to that of probability distributions, can be viewed as a special case of an additive multiattribute utility functions, and show how this special case permits us to capture the novel senses of conditional utility and utility independence. Finally, we present the notion of utility net\u00ad works, which do for utilities what Bayesian networks do for probabilities. Specifically, utility networks exploit the new interpreta\u00ad tion of conditional utility and utility indepen\u00ad dence to compactly represent a utility distri\u00ad bution.", "output": "Conditional Utility, Utility Independence, and Utility Networks", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The shortest path between two concepts in a taxonomic ontology is commonly used to represent the semantic distance between concepts in the edge-based semantic similarity measures. In the past, the edge counting is considered to be the default method for the path computation, which is simple, intuitive and has low computational complexity. However, a large lexical taxonomy of such as WordNet has the irregular densities of links between concepts due to its broad domain but. The edge counting-based path computation is powerless for this non-uniformity problem. In this paper, we advocate that the path computation is able to be separated from the edge-based similarity measures and form various general computing models. Therefore, in order to solve the problem of non-uniformity of concept density in a large taxonomic ontology, we propose a new path computing model based on the compensation of local area density of concepts, which is equal to the number of direct hyponyms of the subsumers of concepts in their shortest path. This path model considers the local area density of concepts as an extension of the edge-based path and converts the local area density divided by their depth into the compensation for edge-based path with an adjustable parameter, which idea has been proven to be consistent with the information theory. This model is a general path computing model and can be applied in various edge-based similarity algorithms. The experiment results show that the proposed path model improves the average correlation between edge-based measures with human judgments on Miller and Charles benchmark from less than 0.8 to more than 0.85, and has a big advantage in efficiency than information content (IC) computation in a dynamic ontology, thereby successfully solving the non-uniformity problem of taxonomic ontology.", "output": "A density compensation-based path computing model for measuring semantic similarity", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The main advantage of Constraint Programming (CP) approaches for sequential pattern mining (SPM) is their modularity, which includes the ability to add new constraints (regular expressions, length restrictions, etc). The current best CP approach for SPM uses a global constraint (module) that computes the projected database and enforces the minimum frequency; it does this with a filtering algorithm similar to the PrefixSpan method. However, the resulting system is not as scalable as some of the most advanced mining systems like Zaki\u2019s cSPADE. We show how, using techniques from both data mining and CP, one can use a generic constraint solver and yet outperform existing specialized systems. This is mainly due to two improvements in the module that computes the projected frequencies: first, computing the projected database can be sped up by pre-computing the positions at which an symbol can become unsupported by a sequence, thereby avoiding to scan the full sequence each time; and second by taking inspiration from the trailing used in CP solvers to devise a backtracking-aware data structure that allows fast incremental storing and restoring of the projected database. Detailed experiments show how this approach outperforms existing CP as well as specialized systems for SPM, and that the gain in efficiency translates directly into increased efficiency for other settings such as mining with regular expressions.", "output": "An Efficient Algorithm for Mining Frequent Sequence with Constraint Programming", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Our experience of the world is multimodal we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.", "output": "Multimodal Machine Learning: A Survey and Taxonomy", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We consider online learning of ensembles of portfolio selection algorithms and aim to regularize risk by encouraging diversification with respect to a predefined risk-driven grouping of stocks. Our procedure uses online convex optimization to control capital allocation to underlying investment algorithms while encouraging non-sparsity over the given grouping. We prove a logarithmic regret for this procedure with respect to the best-in-hindsight ensemble. We applied the procedure with known mean-reversion portfolio selection algorithms using the standard GICS industry sector grouping. Empirical Experimental results showed an impressive percentage increase of risk-adjusted return (Sharpe ratio).", "output": "Online Learning of Portfolio Ensembles with Sector Exposure Regularization", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Recommendation system is a common demand in daily life and matrix completion is a widely adopted technique for this task. However, most matrix completion methods lack semantic interpretation and usually result in weak-semantic recommendations. To this end, this paper proposes a Semantic Analysis approach for Recommendation systems (SAR), which applies a two-level hierarchical generative process that assigns semantic properties and categories for user and item. SAR learns semantic representations of users/items merely from user ratings on items, which offers a new path to recommendation by semantic matching with the learned representations. Extensive experiments demonstrate SAR outperforms other state-of-the-art baselines substantially.", "output": "SAR: A Semantic Analysis Approach for Recommendation", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We describe a novel approach to monitoring high level behaviors using concepts from AI planning. Our goal is to understand what a program is doing based on its system call trace. This ability is particularly important for detecting malware. We approach this problem by building an abstract model of the operating system using the STRIPS planning language, casting system calls as planning operators. Given a system call trace, we simulate the corresponding operators on our model and by observing the properties of the state reached, we learn about the nature of the original program and its behavior. Thus, unlike most statistical detection methods that focus on syntactic features, our approach is semantic in nature. Therefore, it is more robust against obfuscation techniques used by malware that change the outward appearance of the trace but not its effect. We demonstrate the efficacy of our approach by evaluating it on actual system call traces.", "output": "A Planning Approach to Monitoring Computer Programs\u2019 Behavior", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We consider online learning algorithms that guarantee worst-case regret rates in adversarial environments (so they can be deployed safely and will perform robustly), yet adapt optimally to favorable stochastic environments (so they will perform well in a variety of settings of practical importance). We quantify the friendliness of stochastic environments by means of the well-known Bernstein (a.k.a. generalized Tsybakov margin) condition. For two recent algorithms (Squint for the Hedge setting and MetaGrad for online convex optimization) we show that the particular form of their data-dependent individual-sequence regret guarantees implies that they adapt automatically to the Bernstein parameters of the stochastic environment. We prove that these algorithms attain fast rates in their respective settings both in expectation and with high probability.", "output": "Combining Adversarial Guarantees and Stochastic Fast Rates in Online Learning", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In supervised binary hashing, one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes, for application to fast image retrieval. This typically results in a difficult optimization problem, nonconvex and nonsmooth, because of the discrete variables involved. Much work has simply relaxed the problem during training, solving a continuous optimization, and truncating the codes a posteriori. This gives reasonable results but is quite suboptimal. Recent work has tried to optimize the objective directly over the binary codes and achieved better results, but the hash function was still learned a posteriori, which remains suboptimal. We propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates. This closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other. The resulting algorithm can be seen as a corrected, iterated version of the procedure of optimizing first over the codes and then learning the hash function. Compared to this, our optimization is guaranteed to obtain better hash functions while being not much slower, as demonstrated experimentally in various supervised datasets. In addition, our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions.", "output": "Optimizing affinity-based binary hashing using auxiliary coordinates", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Wit is a quintessential form of rich interhuman interaction, and is often grounded in a specific situation (e.g., a comment in response to an event). In this work, we attempt to build computational models that can produce witty descriptions for a given image. Inspired by a cognitive account of humor appreciation, we employ linguistic wordplay, specifically puns. We compare our approach against meaningful baseline approaches via human studies. In a Turing test style evaluation, people find our model\u2019s description for an image to be wittier than a human\u2019s witty description 55% of the time!", "output": "Punny Captions: Witty Wordplay in Image Descriptions", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In a recent paper, Levy and Goldberg [2] pointed out an interesting connection between prediction-based word embedding models and count models based on pointwise mutual information. Under certain conditions, they showed that both models end up optimizing equivalent objective functions. This paper explores this connection in more detail and lays out the factors leading to differences between these models. We find that the most relevant differences from an optimization perspective are (i) predict models work in a low dimensional space where embedding vectors can interact heavily; (ii) since predict models have fewer parameters, they are less prone to overfitting. Motivated by the insight of our analysis, we show how count models can be regularized in a principled manner and provide closed-form solutions for L1 and L2 regularization. Finally, we propose a new embedding model with a convex objective and the additional benefit of being intelligible.", "output": "Towards a Better Understanding of Predict and Count Models", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "With the advent of modern computer networks, fault diagnosis has been a focus of research activity. This paper reviews the history of fault diagnosis in networks and discusses the main methods in information gathering section, information analyzing section and diagnosing and revolving section of fault diagnosis in networks. Emphasis will be placed upon knowledge-based methods with discussing the advantages and shortcomings of the different methods. The survey is concluded with a description of some open problems. Keywords-fault diagnosis in networks; expert system; Bayesian networks; artificial neural network", "output": "Survey of modern Fault Diagnosis methods in networks", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Deep CCA is a recently proposed deep neural network extension to the traditional canonical correlation analysis (CCA), and has been successful for multi-view representation learning in several domains. However, stochastic optimization of the deep CCA objective is not straightforward, because it does not decouple over training examples. Previous optimizers for deep CCA are either batch-based algorithms or stochastic optimization using large minibatches, which can have high memory consumption. In this paper, we tackle the problem of stochastic optimization for deep CCA with small minibatches, based on an iterative solution to the CCA objective, and show that we can achieve as good performance as previous optimizers and thus alleviate the memory requirement.", "output": "Stochastic Optimization for Deep CCA via Nonlinear Orthogonal Iterations", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "This papers shows that using separators, which is a pair of two complementary contractors, we can easily and efficiently solve the localization problem of a robot with sonar measurements in an unstructured environment. We introduce separators associated with the Minkowski sum and the Minkowski difference in order to facilitate the resolution. A test-case is given in order to illustrate the principle of the approach.", "output": "Minkowski Operations of Sets with Application to Robot Localization", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We formalize Simplified Boardgames language, which describes a subclass of arbitrary board games. The language structure is based on the regular expressions, which makes the rules easily machine-processable while keeping the rules concise and fairly human-readable.", "output": "SIMPLIFIED BOARDGAMES", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The ability to monitor the progress of students\u2019 academic performance is a critical issue to the academic community of higher learning. A system for analyzing students\u2019 results based on cluster analysis and uses standard statistical algorithms to arrange their scores data according to the level of their performance is described. In this paper, we also implemented k-mean clustering algorithm for analyzing students\u2019 result data. The model was combined with the deterministic model to analyze the students\u2019 results of a private Institution in %igeria which is a good benchmark to monitor the progression of academic performance of students in higher Institution for the purpose of making an effective decision by the academic planners. Keywordsk \u2013 mean, clustering, academic performance, algorithm.", "output": "Application of k-Means Clustering algorithm for prediction of Students\u2019 Academic Performance", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "A modelling language is decribed which is suitable for the correlation of information when the underlying functional model of the system is incomplete or uncertain and the temporal dependencies are imprecise. An efficient an incremental implementation is outlined which depends on cost functions satisfying certain criteria. Possibilistic logic and probability theory (as it is used in the applications targetted) satisfy these criteria.", "output": "Exploiting Uncertain and Temporal lnfonnation in Correlation", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In this work we consider the stochastic minimization of nonsmooth convex loss functions, a central problem in machine learning. We propose a novel algorithm called Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), which exploits the structure of common nonsmooth loss functions to achieve optimal convergence rates for a class of problems including SVMs. It is the first stochastic algorithm that can achieve the optimal O(1/t) rate for minimizing nonsmooth loss functions (with strong convexity). The fast rates are confirmed by empirical comparisons, in which ANSGD significantly outperforms previous subgradient descent algorithms including SGD.", "output": "Stochastic Smoothing for Nonsmooth Minimizations: Accelerating SGD by Exploiting Structure", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Implementing an accurate and fast activation function with low cost is a crucial aspect to the implementation of Deep Neural Networks (DNNs) on FPGAs. We propose a highaccuracy approximation approach for the hyperbolic tangent activation function of artificial neurons in DNNs. It is based on the Discrete Cosine Transform Interpolation Filter (DCTIF). The proposed architecture combines simple arithmetic operations on stored samples of the hyperbolic tangent function and on input data. The proposed DCTIF implementation achieves two orders of magnitude greater precision than previous work while using the same or fewer computational resources. Various combinations of DCTIF parameters can be chosen to tradeoff the accuracy and complexity of the hyperbolic tangent function. In one case, the proposed architecture approximates the hyperbolic tangent activation function with 10 maximum error while requiring only 1.52 Kbits memory and 57 LUTs of a Virtex-7 FPGA. We also discuss how the activation function accuracy affects the performance of DNNs in terms of their training and testing accuracies. We show that a high accuracy approximation can be necessary in order to maintain the same DNN training and testing performances realized by the exact function.", "output": "Accurate and Efficient Hyperbolic Tangent Activation Function on FPGA using the DCT Interpolation Filter", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Submodular functions describe a variety of discrete problems in machine learn-<lb>ing, signal processing, and computer vision. However, minimizing submodular<lb>functions poses a number of algorithmic challenges. Recent work introduced an<lb>easy-to-use, parallelizable algorithm for minimizing submodular functions that<lb>decompose as the sum of \u201csimple\u201d submodular functions. Empirically, this al-<lb>gorithm performs extremely well, but no theoretical analysis was given. In this<lb>paper, we show that the algorithm converges linearly, and we provide upper and<lb>lower bounds on the rate of convergence. Our proof relies on the geometry of<lb>submodular polyhedra and draws on results from spectral graph theory.", "output": "On the Convergence Rate of Decomposable Submodular Function Minimization", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In this paper, we suggest a novel data-driven approach to active learning: Learning Active Learning (LAL). The key idea behind LAL is to train a regressor that predicts the expected error reduction for a potential sample in a particular learning state. By treating the query selection procedure as a regression problem we are not restricted to dealing with existing AL heuristics; instead, we learn strategies based on experience from previous active learning experiments. We show that LAL can be learnt from a simple artificial 2D dataset and yields strategies that work well on real data from a wide range of domains. Moreover, if some domain-specific samples are available to bootstrap active learning, the LAL strategy can be tailored for a particular problem.", "output": "Learning Active Learning from Real and Synthetic Data", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "There is a brief description of the probabilistic causal graph model for representing, reasoning with, and learn\u00ad ing causal structure using Bayesian networks. It is then argued that this model is closely related to how humans reason with and learn causal structure. It is shown that studies in psychology on discounting (reasoning concern\u00ad ing how the presence of one cause of an effect makes an\u00ad other cause less probable) support the hypothesis that humans reach the same judgments as algorithms for do\u00ad ing inference in Bayesian networks. Next, it is shown how studies by Piaget indicate that humans learn causal structure by observing the same independencies and de\u00ad pendencies as those used by certain algorithms for learn\u00ad ing the structure of a Bayesian network. Based on this indication, a subjective definition of causality is for\u00ad warded. Finally, methods for further testing the accu\u00ad racy of these claims are discussed.", "output": "THE CoGNITIVE PROCESSING OF CAUSAL KNOWLEDGE", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "The local computation technique (Shafer et a!. 1987, Shafer and Shenoy 1988, Shenoy and Shafer 1986) is used for propagating belief functions in so\u00ad called a Markov Tree. In this paper, we describe an efficient implementation of belief function propagation on the basis of the local computation technique. The presented method avoids all the redundant computations in the propagation process, and so makes the computational complexity decrease with respect to other existing implementations (Hsia and Shenoy 1989, Zarley et a!. 1988). We also give a combined algorithm for both propagation and re-propagation which makes the re-propagation process more efficient when one or more of the prior belief functions is changed.", "output": "An Efficient Implementation of Belief Function Propagation", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In the present paper we show that distributional information is particularly important when considering concept availability under implicit language learning conditions. Based on results from different behavioural experiments we argue that the implicit learnability of semantic regularities depends on the degree to which the relevant concept is reflected in language use. In our simulations, we train a VectorSpace model on either an English or a Chinese corpus and then feed the resulting representations to a feed-forward neural network. The task of the neural network was to find a mapping between the word representations and the novel words. Using datasets from four behavioural experiments, which used different semantic manipulations, we were able to obtain learning patterns very similar to those obtained by humans.", "output": "A Distributional Semantics Approach to Implicit Language Learning", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Social media and data mining are increasingly being used to analyse political and societal issues. Here we undertake the classification of social media users as supporting or opposing ongoing independence movements in their territories. Independence movements occur in territories whose citizens have conflicting national identities; users with opposing national identities will then support or oppose the sense of being part of an independent nation that differs from the officially recognised country. We describe a methodology that relies on users\u2019 self-reported location to build datasets for three territories \u2013 Catalonia, the Basque Country and Scotland \u2013 and we test language-independent classifiers using four types of features. We show the effectiveness of the approach to build large annotated datasets, and the ability to achieve accurate, language-independent classification performances ranging from 85% to 97% for the three territories under study. A data analysis shows the existence of echo chambers that isolate opposing national identities from each other.", "output": "Stance Classification of Social Media Users in Independence Movements", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "This paper concerns the single machine total weighted tardiness scheduling with sequence-dependent setup times, usually referred as 1|sij | \u2211 wjTj . In this NP-hard problem, each job has an associated processing time, due date and a weight. For each pair of jobs i and j, there may be a setup time before starting to process j in case this job is scheduled immediately after i. The objective is to determine a schedule that minimizes the total weighted tardiness, where the tardiness of a job is equal to its completion time minus its due date, in case the job is completely processed only after its due date, and is equal to zero otherwise. Due to its complexity, this problem is most commonly solved by heuristics. The aim of this work is to develop a simple yet effective limitation strategy that speeds up the local search procedure without a significant loss in the solution quality. Such strategy consists of a filtering mechanism that prevents unpromising moves to be evaluated. The proposed strategy has been embedded in a local search based metaheuristic from the literature and tested in classical benchmark instances. Computational experiments revealed that the limitation strategy enabled the metaheuristic to be extremely competitive when compared to other algorithms from the literature, since it allowed the use of a large number of neighborhood structures without a significant increase in the CPU time and, consequently, high quality solutions could be achieved in a matter of seconds. In addition, we analyzed the effectiveness of the proposed strategy in two other well-known metaheuristics. Further experiments were also carried out on benchmark instances of problem 1|sij | \u2211 Tj .", "output": "Efficient local search limitation strategy for single machine total weighted tardiness scheduling with sequence-dependent setup times", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In Pawlak\u2019s rough set theory, a set is approximated by a pair of lower and upper approximations. To measure numerically the roughness of an approximation, Pawlak introduced a quantitative measure of roughness by using the ratio of the cardinalities of the lower and upper approximations. Although the roughness measure is effective, it has the drawback of not being strictly monotonic with respect to the standard ordering on partitions. Recently, some improvements have been made by taking into account the granularity of partitions. In this paper, we approach the roughness measure in an axiomatic way. After axiomatically defining roughness measure and partition measure, we provide a unified construction of roughness measure, called strong Pawlak roughness measure, and then explore the properties of this measure. We show that the improved roughness measures in the literature are special instances of our strong Pawlak roughness measure and introduce three more strong Pawlak roughness measures as well. The advantage of our axiomatic approach is that some properties of a roughness measure follow immediately as soon as the measure satisfies the relevant axiomatic definition.", "output": "An axiomatic approach to the roughness measure of rough sets", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Based on a new atomic norm, we propose a new convex formulation for sparse matrix factorization problems in which the number of nonzero elements of the factors is assumed fixed and known. The formulation counts sparse PCA with multiple factors, subspace clustering and low-rank sparse bilinear regression as potential applications. We compute slow rates and an upper bound on the statistical dimension Amelunxen et al. (2013) of the suggested norm for rank 1 matrices, showing that its statistical dimension is an order of magnitude smaller than the usual l1-norm, trace norm and their combinations. Even though our convex formulation is in theory hard and does not lead to provably polynomial time algorithmic schemes, we propose an active set algorithm leveraging the structure of the convex problem to solve it and show promising numerical results.", "output": "Tight convex relaxations for sparse matrix factorization", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semisupervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the sourceto-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the ChineseEnglish dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.", "output": "Semi-Supervised Learning for Neural Machine Translation", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Epistemic logic with non-standard knowledge operators, especially the \u201cknowing-value\u201d operator, has recently gathered much attention. With the \u201cknowing-value\u201d operator, we can express knowledge of individual variables, but not of the relations between them in general. In this paper, we propose a new operator Kf to express knowledge of the functional dependencies between variables. The semantics of this Kf operator uses a function domain which imposes a constraint on what counts as a functional dependency relation. By adjusting this function domain, different interesting logics arise, and in this paper we axiomatize three such logics in a single agent setting. Then we show how these three logics can be unified by allowing the function domain to vary relative to different agents and possible worlds. A multiagent axiomatization is given in this case.", "output": "Epistemic Logic with Functional Dependency Operator", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "We propose Neural Reasoner , a framework for neural network-based reasoning over natural language sentences. Given a question, Neural Reasoner can infer over multiple supporting facts and find an answer to the question in specific forms. Neural Reasoner has 1) a specific interaction-pooling mechanism, allowing it to examine multiple facts, and 2) a deep architecture, allowing it to model the complicated logical relations in reasoning tasks. Assuming no particular structure exists in the question and facts, Neural Reasoner is able to accommodate different types of reasoning and different forms of language expressions. Despite the model complexity, Neural Reasoner can still be trained effectively in an end-to-end manner. Our empirical studies show that Neural Reasoner can outperform existing neural reasoning systems with remarkable margins on two difficult artificial tasks (Positional Reasoning and Path Finding) proposed in [8]. For example, it improves the accuracy on Path Finding(10K) from 33.4% [6] to over 98%.", "output": "Towards Neural Network-based Reasoning", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "Robust search procedures are a central component in the design of black-box constraint-programming solvers. This paper proposes activity-based search, the idea of using the activity of variables during propagation to guide the search. Activity-based search was compared experimentally to impact-based search and the wdeg heuristics. Experimental results on a variety of benchmarks show that activity-based search is more robust than other heuristics and may produce significant improvements in performance.", "output": "Activity-Based Search for Black-Box Contraint-Programming Solvers", "category": "Title Generation 636.json"}, {"instruction": "In this task, you are given a part of an article. Your task is to generate headline (title) for this text. Preferred headlines are under fifteen words.", "input": "In this paper, we describe a system for generating threedimensional visual simulations of natural language motion expressions. We use a rich formal model of events and their participants to generate simulations that satisfy the minimal constraints entailed by the associated utterance, relying on semantic knowledge of physical objects and motion events. This paper outlines technical considerations and discusses implementing the aforementioned semantic models into such a system.", "output": "Multimodal Semantic Simulations of Linguistically Underspecified Motion Events", "category": "Title Generation 636.json"}]