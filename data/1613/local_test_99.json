[{"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The above pseudocode is agnostic with respect to the choice of fragmentation and environment functions; task-specific choices are described in more detail for each experiment below.\n\nDiscussion \n Question: Which languages do they test on?", "output": "Answer with content missing: (Applications section) We use Wikipedia articles\nin five languages\n(Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English) as well as the Na dataset of Adams\net al. (2017).\nSelect:\nKinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. \n Question: How is the dataset collected?", "output": "A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al HEOT obtained from one of the past studies done by Mathur et al", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We use the following three evaluation metrics:\n\nPhoneme Error Rate (PER) is the Levenshtein distance between the predicted phoneme sequences and the gold standard phoneme sequences, divided by the length of the gold standard phoneme sequences.\n\nWord Error Rate (WER) is the percentage of words in which the predicted phoneme sequence does not exactly match the gold standard phoneme sequence.\n\nWord Error Rate 100 (WER 100) is the percentage of words in the test set for which the correct guess is not in the first 100 guesses of the system. \n Question: what evaluation metrics were used?", "output": "Phoneme Error Rate (PER) Word Error Rate (WER) Word Error Rate 100 (WER 100)", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In order to provide directions for future work, we analyze the errors made by the classifier trained on the extended features on the four prediction tasks.\n\nErrors on Intention (I) prediction: The lack of background is a major problem when identifying trolling comments. Non-cursing aggressions and insults This is a challenging problem, since the majority of abusive and insulting comments rely on profanity and swearing.  Another source of error is the presence of controversial topic words such as \u201cblack\u201d,\u201cfeminism\u201d, \u201ckilling\u201d, \u201cracism\u201d, \u201cbrown\u201d, etc. that are commonly used by trolls. Errors on Disclosure (D) prediction: A major source of error that affects disclosure is the shallow meaning representation obtained from the BOW model even when augmented with the distributional features given by the glove vectors. Errors on Interpretation (R) prediction: it is a common practice from many users to directly ask the suspected troll if he/she is trolling or not.  Errors on Response Strategy (B) prediction: In some cases there is a blurry line between \u201cFrustrate\u201d and \u201cNeutralize\u201d.  Another challenging problem is the distinction between the classes \u201cTroll\u201d and \u201cEngage\u201d.  \n Question: What is an example of a difficult-to-classify case?", "output": "The lack of background Non-cursing aggressions and insults the presence of controversial topic words   shallow meaning representation directly ask the suspected troll if he/she is trolling or not a blurry line between \u201cFrustrate\u201d and \u201cNeutralize\u201d distinction between the classes \u201cTroll\u201d and \u201cEngage\u201d", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We use Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10 as baselines and compare the results with our different fine-tuning strategies using pre-trained BERTbase model.  \n Question: What existing approaches do they compare to?", "output": "Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We therefore propose attention models to infer the latent context, i.e., the series of posts that trigger an intervention. \n Question: What type of latent context is used to predict instructor intervention?", "output": "the series of posts that trigger an intervention", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We conduct experiments on the SQuAD dataset BIBREF3. \n Question: On what datasets are experiments performed?", "output": "SQuAD", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Our conversational agent uses two architectures to simulate a specialized reminiscence therapist. The block in charge of generating questions is based on the work Show, Attend and Tell BIBREF13. This work generates descriptions from pictures, also known as image captioning. \n Question: Is machine learning system underneath similar to image caption ML systems?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "DrQA is a CRC baseline coming with the CoQA dataset. Note that this implementation of DrQA is different from DrQA for SQuAD BIBREF8 in that it is modified to support answering no answer questions by having a special token unknown at the end of the document.  DrQA+CoQA is the above baseline pre-tuned on CoQA dataset and then fine-tuned on $(\\text{RC})_2$ .  BERT is the vanilla BERT model directly fine-tuned on $(\\text{RC})_2$ . We use this baseline for ablation study on the effectiveness of pre-tuning. BERT+review first tunes BERT on domain reviews using the same objectives as BERT pre-training and then fine-tunes on $(\\text{RC})_2$ . We use this baseline to show that a simple domain-adaptation of BERT is not good. BERT+CoQA first fine-tunes BERT on the supervised CoQA data and then fine-tunes on $(\\text{RC})_2$ . We use this baseline to show that pre-tuning is very competitive even compared with models trained from large-scale supervised data. \n Question: What is the baseline model used?", "output": "The baseline models used are DrQA modified to support answering no answer questions, DrQA+CoQA which is pre-tuned on CoQA dataset, vanilla BERT, BERT+review tuned on domain reviews, BERT+CoQA tuned on the supervised CoQA data", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Shared Self-attention Layers As our model providing two outputs from one input, there is a bifurcation setting for how much shared part should be determined. Both constituent and dependency parsers share token representation and 8 self-attention layers at most. Assuming that either parser always takes input information flow through 8 self-attention layers as shown in Figure FIGREF4, then the number of shared self-attention layers varying from 0 to 8 may reflect the shared degree in the model. When the number is set to 0, it indicates only token representation is shared for both parsers trained for the joint loss through each own 8 self-attention layers. When the number is set to less than 8, for example, 6, then it means that both parsers first shared 6 layers from token representation then have individual 2 self-attention layers. For different numbers of shared layers, the results are in Table TABREF14. We respectively disable the constituent and the dependency parser to obtain a separate learning setting for both parsers in our model. The comparison in Table TABREF14 indicates that even though without any shared self-attention layers, joint training of our model may significantly outperform separate learning mode. At last, the best performance is still obtained from sharing full 8 self-attention layers. \n Question: How are different network components evaluated?", "output": "For different numbers of shared layers, the results are in Table TABREF14. We respectively disable the constituent and the dependency parser to obtain a separate learning setting for both parsers in our model. ", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We first describe our models individually and then the ensembling technique that we employ. In the following, MN denotes Memory Networks to encode conversational history, RCNN signify R-CNN for object level representations of an image, Wt represents additional linear layer in the decoder, and LF a late fusion mechanism as defined in BIBREF0.\n\nModels ::: LF-RCNN\nLate fusion encoder BIBREF0 with concatenated history. We use two-layered LSTMs with 512 hidden units for embedding questions and history. The object-level features are weighed using only question embeddings. The word embeddings from Glove vectors are frozen and are not fine-tuned. Figure FIGREF6 gives an overview of the architecture.\n\nModels ::: MN-RCNN\nMemory network encoder BIBREF0 with bi-directional GRUs and word embeddings fine-tuned. Object-level features are weighed by question and caption embedding. The rest of the scheme is same as above. (Figure FIGREF6)\n\nModels ::: MN-RCNN-Wt\nSame as above but with an additional linear layer applied to the dot product of candidate answer and encoder output, and gated using tanh function. Compare Figure FIGREF6 with Figure FIGREF6 \n Question: Which three discriminative models did they use?", "output": "LF-RCNN MN-RCNN MN-RCNN-Wt", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We showed in BIBREF22 that Pearson correlation is badly suited for STS. Instead, we compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels. \n Question: What metrics are used for the STS tasks?", "output": " Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Although MCDN doesn't obtain the highest precision, it increases F1-score by 10.2% and 3% compared with the existing best systems $LS \\cup KLD \\cup CONN$ and $KLD \\cup LS \\cup LS_{inter}$. \n Question: What performance did proposed method achieve, how much better is than previous state-of-the-art?", "output": "increases F1-score by 10.2% and 3% compared with the existing best systems $LS \\cup KLD \\cup CONN$ and $KLD \\cup LS \\cup LS_{inter}$", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We feed these representations into a clustering algorithm that produces exactly eleven clusters of five cities each and evaluate them against both Calvino's original labels and crowdsourced human judgments. While the overall correlation with Calvino's labels is low, both computers and humans can reliably identify some thematic groups associated with concrete objects. \n Question: How do they obtain human judgements?", "output": "Using crowdsourcing ", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Where possible, we tried to restrict the content of each text to the abstract and conclusion of the original work. However, since in many cases these sections were too short, we also considered other parts of the original works such as introduction or discussion sections. To ensure that the extracted text portions are appropriate for the AV task, each original work was preprocessed manually. More precisely, we removed tables, formulas, citations, quotes and sentences that include non-language content such as mathematical constructs or specific names of researchers, systems or algorithms. \n Question: What is a self-compiled corpus?", "output": " restrict the content of each text to the abstract and conclusion of the original work considered other parts of the original works such as introduction or discussion sections extracted text portions are appropriate for the AV task, each original work was preprocessed manually removed tables, formulas, citations, quotes and sentences that include non-language content such as mathematical constructs or specific names of researchers, systems or algorithms", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Fig. FIGREF4 shows the distributions of seven question types grouped deterministically from the lexicons. \n Question: How many question types do they find in the datasets analyzed?", "output": "seven ", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The output of a system with the target words in the predicted order is compared to the gold ranking of the DURel data set. As the metric to assess how well the model's output fits the gold ranking Spearman's $\\rho $ was used. The higher Spearman's rank-order correlation the better the system's performance. \n Question: How is evaluation performed?", "output": "As the metric to assess how well the model's output fits the gold ranking Spearman's $\\rho $ was used", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Therefore, we decided to compile our own corpora based on English documents, which we crawled from different publicly accessible sources. \n Question: Do they report results only on English data?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In this chapter, we apply an ensemble of deep learning and linguistics to tackle both the problem of aspect extraction and subjectivity detection. \n Question: How are aspects identified in aspect extraction?", "output": "apply an ensemble of deep learning and linguistics t", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Curriculum Plausibility ::: Conversational Attributes ::: Specificity\nA notorious problem for neural dialogue generation model is that the model is prone to generate generic responses. Curriculum Plausibility ::: Conversational Attributes ::: Repetitiveness\nRepetitive responses are easy to generate in current auto-regressive response decoding, where response generation loops frequently, whereas diverse and informative responses are much more complicated for neural dialogue generation. Curriculum Plausibility ::: Conversational Attributes ::: Continuity\nA coherent response not only responds to the given query, but also triggers the next utterance. Curriculum Plausibility ::: Conversational Attributes ::: Model Confidence\nDespite the heuristic dialogue attributes, we further introduce the model confidence as an attribute, which distinguishes the easy-learnt samples from the under-learnt samples in terms of the model learning ability. Curriculum Plausibility ::: Conversational Attributes ::: Query-relatedness\nA conversation is considered to be coherent if the response correlates well with the given query. \n Question: What five dialogue attributes were analyzed?", "output": "Model Confidence Continuity Query-relatedness Repetitiveness Specificity", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Furthermore, we employ two different parsers for comparison: 1) the PCFGLA-based parser, viz. Berkeley parser BIBREF5 , and 2) a minimal span-based neural parser BIBREF6 . \n Question: What is the baseline model for the agreement-based mode?", "output": "PCFGLA-based parser, viz. Berkeley parser BIBREF5 minimal span-based neural parser BIBREF6", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In this work, we introduce a new logical inference engine called MonaLog, which is based on natural logic and work on monotonicity stemming from vanBenthemEssays86. Since our logic operates over surface forms, it is straightforward to hybridize our models. We investigate using MonaLog in combination with the language model BERT BIBREF20, including for compositional data augmentation, i.e, re-generating entailed versions of examples in our training sets.  We perform two experiments to test MonaLog. We first use MonaLog to solve the problems in a commonly used natural language inference dataset, SICK BIBREF1, comparing our results with previous systems. Second, we test the quality of the data generated by MonaLog. To do this, we generate more training data (sentence pairs) from the SICK training data using our system, and performe fine-tuning on BERT BIBREF20, a language model based on the transformer architecture BIBREF23, with the expanded dataset.  \n Question: How do they combine MonaLog with BERT?", "output": "They use Monalog for data-augmentation to fine-tune BERT on this task", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In addition, in order to estimate annotation reliability and provide for better evaluation, every question in the test set is answered by at least two additional experts. \n Question: Are the answers double (and not triple) annotated?", "output": "Yes", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We quantify the use of these strategies by comparing the airtime debaters devote to talking points . For a side INLINEFORM0 , let the self-coverage INLINEFORM1 be the fraction of content words uttered by INLINEFORM2 in round INLINEFORM3 that are among their own talking points INLINEFORM4 ; and the opponent-coverage INLINEFORM5 be the fraction of its content words covering opposing talking points INLINEFORM6 .  We use all conversational features discussed above. For each side INLINEFORM0 we include INLINEFORM1 , INLINEFORM2 , and their sum. We also use the drop in self-coverage given by subtracting corresponding values for INLINEFORM3 , and the number of discussion points adopted by each side. \n Question: what aspects of conversation flow do they look at?", "output": "The time devoted to self-coverage, opponent-coverage, and the number of adopted discussion points.", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "The second Turkish dataset is the Twitter corpus which is formed of tweets about Turkish mobile network operators. Those tweets are mostly much noisier and shorter compared to the reviews in the movie corpus. In total, there are 1,716 tweets. 973 of them are negative and 743 of them are positive. These tweets are manually annotated by two humans, where the labels are either positive or negative. \n Question: What details are given about the Twitter dataset?", "output": "Those tweets are mostly much noisier and shorter compared to the reviews in the movie corpus. In total, there are 1,716 tweets. 973 of them are negative and 743 of them are positive.", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "In the sentiment analysis experiments, we found that a tweet may contain multiple sentiments. The groundtruth labels contain 210 positive sentiments, 521 neutral sentiments, and 305 negative sentiments to the candidates. \n Question: Is datasets for sentiment analysis balanced?", "output": "No", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "Wizard interface: the interface shown to participants with the Wizard role provides possible actions on the right-hand side of the browser window. These actions could be verbal, such as sending a message, or non-verbal, such as switching on/off a button to activate a robot.  Dialogue structure: we introduced structured dialogues through a Finite State Machine (FSM) that controls the current dialogue state and offers multiple suitable and relevant state transitions (actions) to the Wizard depending on the point in the interaction, the state of the world and the history. A graph of dialogue states, transitions and utterances is loaded when the system is initialised, and each chat room has its own dialogue state, which changes through actions. System-changing actions: actions trigger transitions between the states in the FSM. We differentiate two types of actions:\n\nVerbal actions, such as the dialogue options available at that moment. The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard. Predefined messages can also trigger other associated events such as pop-ups or follow-up non-verbal actions.\n\nNon-verbal actions, such as commands to trigger events. These can take any form, but we used buttons to control robots in our data collection.\n\nSubmitting an action would change the dialogue state in the FSM, altering the set of actions available in the subsequent turn visible to the Wizard. Some dialogue options are only possible at certain states, in a similar way as to how non-verbal actions are enabled or disabled depending on the state.  \n Question: How does framework made sure that dialogue will not breach procedures?", "output": "The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard. Predefined messages can also trigger other associated events such as pop-ups or follow-up non-verbal actions.", "category": "Question Answering 1087.json"}, {"instruction": "In this task, you will be presented with a context from an academic paper and a question separated with a \n. You have to answer the question based on the context.", "input": "We use one public dataset Social Honeypot dataset and one self-collected dataset Weibo dataset to validate the effectiveness of our proposed features. Before directly performing the experiments on the employed datasets, we first delete some accounts with few posts in the two employed since the number of tweets is highly indicative of spammers. For the English Honeypot dataset, we remove stopwords, punctuations, non-ASCII words and apply stemming. For the Chinese Weibo dataset, we perform segmentation with \"Jieba\", a Chinese text segmentation tool. After preprocessing steps, the Weibo dataset contains 2197 legitimate users and 802 spammers, and the honeypot dataset contains 2218 legitimate users and 2947 spammers. \n Question: What is the benchmark dataset and is its quality high?", "output": "Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "category": "Question Answering 1087.json"}]