[{"instruction": "In this task, you are given a paper review. Based on the review, your job is to identify language and generate \"en\" if the review is in English or generate \"es\" if the review is in Spanish. Note that URLs in the text have been replaced with [Link].", "input": "Buena pr\u00e1ctica para garantizar calidad en la educci\u00f3n de requisitos  - El resumen est\u00e1 mal redactado, no deber\u00eda haber explicaciones te\u00f3ricas - La traducci\u00f3n del resumen es p\u00e9sima - El art\u00edculo no est\u00e1 escrito en tercera persona - La Secci\u00f3n POKA YOKE deber\u00eda ir antes de la secci\u00f3n VARIABILIDAD DE LOS REQUISITOS Y COSTO DE CALIDAD. - Hay varios errores de ortograf\u00eda", "output": "es", "category": "Language Identification 893.json"}, {"instruction": "In this task, you are given a paper review. Based on the review, your job is to identify language and generate \"en\" if the review is in English or generate \"es\" if the review is in Spanish. Note that URLs in the text have been replaced with [Link].", "input": "This paper combines variational RNN (VRNN) and domain adversarial networks (DANN) for domain adaptation in the sequence modelling domain.  The VRNN is used to learn representations for sequential data, which is the hidden states of the last time step.  The DANN is used to make the representations domain invariant, therefore achieving cross domain adaptation.  Experiments are done on a number of data sets, and the proposed method (VRADA) outperforms baselines including DANN, VFAE and R-DANN on almost all of them.  I don't have questions about the proposed model, the model is quite clear and seems to be a simple combination of VRNN and DANN.  But a few questions came up during the pre-review question phase:  - As the authors have mentioned, DANN in general outperforms MMD based methods, however, the VFAE method which is based on MMD regularization on the representations seems to outperform DANN across the board.  That seems to indicate VRNN + MMD should also be a good combination.  - One baseline the authors showed in the experiments is R-DANN, which is an RNN version of DANN.  There are two differences between R-DANN and VRADA: (1) R-DANN uses deterministic RNN for representation learning, while VRADA uses variational RNN; (2) on target domain R-DANN only optimizes adversarial loss, while VRADA optimizes both adversarial loss and reconstruction loss for feature learning.  It would be good to analyze further where the performance gain comes from.", "output": "en", "category": "Language Identification 893.json"}, {"instruction": "In this task, you are given a paper review. Based on the review, your job is to identify language and generate \"en\" if the review is in English or generate \"es\" if the review is in Spanish. Note that URLs in the text have been replaced with [Link].", "input": "- Strengths: *- Task *- Simple model, yet the best results on SQuAD (single model0 *- Evaluation and comparison  - Weaknesses: *- Analysis of errors/results (See detailed comments below)  - General Discussion: In this paper the authors present a method for directly querying Wikipedia to answer open domain questions. The system consist of two components - a module to query/fetch wikipedia articles and a module to answer the question given the fetched set of wikipedia articles.   The document retrieval system is a traditional IR system relying on term frequency models and ngram counts.  The answering system uses a feature representation for paragraphs that consists of word embeddings, indicator features to determine whether a paragraph word occurs in a question, token-level features including POS, NER etc and a soft feature for capturing similarity between question and paragraph tokens in embedding space. A combined feature representation is used as an input to a bi-direction LSTM RNN for encoding. For questions an RNN that works on the word embeddings is used.  These are then used to train an overall classifier independently for start and end spans of sentences within a paragraph to answer questions.  The system has been trained using different Open Domain QA datasets such as SQuAD and WebQuestions by modifying the training data to include articles fetched by the IR engine instead of just the actual correct document/passage.  Overall, an easy to follow interesting paper but I had a few questions: 1) The IR system has a Accuracy@5 of over 75 %, and individually the document reader performs well and can beat the best single models on SquAD. What explains the significant drop in Table 6. The authors mention that instead of the fetched results, if they test using the best paragraph the accuracy reaches just 0.49 (from 0.26) but that is still significantly below the 0.78-79 in the SQuAD task.  So, presumably the error is this large because the neural network for matching isnt doing as good a job in learning the answers when using the modified training set (which includes fetched articles) instead of the case when training and testing is done for the document understanding task. Some analysis of whats going on here should be provided. What was the training accuracy in the both cases? What can be done to improve it? To be fair, the authors to allude to this in the conclusion but I think it still needs to be part of the paper to provide some meaningful insights.  2) I understand the authors were interested in treating this as a pure machine comprehension task and therefore did not want to rely on external sources such as Freebase which could have helped with entity typing        but that would have been interesting to use. Tying back to my first question -- if the error is due to highly relevant topical sentences as the authors mention, could entity typing have helped?  The authors should also refer to QuASE (Sun et. al 2015 at WWW2015) and similar systems in their related work. QuASE is also an Open domain QA system that answers using fetched passages - but it relies on the web instead of just Wikipedia.", "output": "en", "category": "Language Identification 893.json"}, {"instruction": "In this task, you are given a paper review. Based on the review, your job is to identify language and generate \"en\" if the review is in English or generate \"es\" if the review is in Spanish. Note that URLs in the text have been replaced with [Link].", "input": "El art\u00edculo presenta un experimento que compara distintas arquitecturas de navegaci\u00f3n para un robot m\u00f3vil. Se describen las arquitecturas (algoritmos, enfoques) utilizados, as\u00ed como las m\u00e9tricas definidas para evaluar la performance en la navegaci\u00f3n. Los resultados son analizados estad\u00edsticamente utilizando an\u00e1lisis de varianza.  El art\u00edculo est\u00e1 bien escrito y el experimento se presenta en forma clara. Las referencias bibliogr\u00e1ficas parecen son adecuadas. En la introducci\u00f3n se justifica correctamente los objetivos de comparaci\u00f3n y se describen en forma general los enfoques posibles para la navegaci\u00f3n de un robot.  La selecci\u00f3n de m\u00e9tricas est\u00e1 justificada e intenta ir m\u00e1s all\u00e1 de una combinaci\u00f3n de mediciones simples. El experimento est\u00e1 bien descrito, aunque se podr\u00eda explicar por qu\u00e9 no se incluyeron otras configuraciones de obst\u00e1culos en la observaci\u00f3n. Los m\u00e9todos estad\u00edsticos (an\u00e1lisis de varianza) est\u00e1n aplicados correctamente, aunque se podr\u00eda incluir un an\u00e1lisis de la normalidad en la distribuci\u00f3n.  Uno de los puntos d\u00e9biles del trabajo es que hace falta una discusi\u00f3n m\u00e1s profunda de los resultados. M\u00e1s all\u00e1 de indicar que uno de las arquitecturas tiene mejor rendimiento, se deber\u00eda discutir las razones y realizar nuevas conjeturas.  La generalizaci\u00f3n de los resultados y la discusi\u00f3n de amenazas a la validez debe ser parte de la discusi\u00f3n. Por ejemplo: \u00bfEn qu\u00e9 tipo de obst\u00e1culos funciona mejora cada algoritmo? \u00bfCu\u00e1l es el esfuerzo de programaci\u00f3n de cada uno de las arquitecturas? \u00bfCu\u00e1les son los objetivos de navegaci\u00f3n de alto nivel del robot (comportamiento deliberativo)?", "output": "es", "category": "Language Identification 893.json"}, {"instruction": "In this task, you are given a paper review. Based on the review, your job is to identify language and generate \"en\" if the review is in English or generate \"es\" if the review is in Spanish. Note that URLs in the text have been replaced with [Link].", "input": "I agree with the other reviewer that the application areas are limited in the paper. I agree with the overall sentiment of the paper to evaluate effectiveness of some of the more recent techniques in this area, in conjunction with the recurrent networks.   The paper advertises itself as a method (or a list of methods) of improving the recurrent baselines when performing experiments, however fails (or not shown) to generalize to other tasks. Effectiveness of these methods need to be shown across a wide variety of tasks if we intend to replace traditional baselines in general, rather than a specific subset of applications.  I like the desire to evaluate many of the recent techniques and having many replications of experiments towards this end (which is a strong point of the paper). However, whether there are synergies of some of the enhancements with sentiment analysis or not, we cannot see from these results. It would be interesting to see whether some of these results generalize across a wide variety of tasks.", "output": "en", "category": "Language Identification 893.json"}, {"instruction": "In this task, you are given a paper review. Based on the review, your job is to identify language and generate \"en\" if the review is in English or generate \"es\" if the review is in Spanish. Note that URLs in the text have been replaced with [Link].", "input": "This paper introduces a mechanism for active learning with convolutional neural networks (CNNs). I would not go as far as the authors in calling these \"deep\", seeing that they seem to have only 2 hidden layers with only 20 filters each. The active learning criterion is a greedy selection scheme based on variational free energy and a series of approximations.  The paper is sometimes hard to read, due to (a) many grammatical errors and (b) sloppy notation in some places (e.g., on page 5, line 1, f is used but never introduced before). Overall, I give an accepting score, but a weak one because of the grammatical errors. If the paper is accepted, these should be fixed for the final version, optimally by a native speaker.   The paper's topic is interesting, and the paper appears to succeed in its goal of showing a proof of concept for active learning in CNNs (if only on toy datasets). I'm surprised by the new results on uncertainty sampling and curriculum learning the authors added: why do these methods both break for USPS? In particular, uncertainty sampling did very well (in fact, better than the authors' new method) on MNIST, but apparently horribly on USPS; some explanation for this would be useful.  I have one more question: why is it necessary to first sample a larger subset D \\subset U, from which we select using active learning? Is this merely done for reasons of computational efficiency, or can it actually somehow improve results? (If so, it would be instrumental to see the worse results when this is not done.)", "output": "en", "category": "Language Identification 893.json"}, {"instruction": "In this task, you are given a paper review. Based on the review, your job is to identify language and generate \"en\" if the review is in English or generate \"es\" if the review is in Spanish. Note that URLs in the text have been replaced with [Link].", "input": "The paper presents a way to \"learn\" approximate data structures. They train neural networks (ConvNets here) to perform as an approximate abstract data structure by having an L2 loss (for the unrolled NN) on respecting the axioms of the data structure they want the NN to learn. E.g. you NN.push(8), NN.push(6), NN.push(4), the loss is proportional to the distance with what is NN.pop()ed three times and 4, 6, 8 (this example is the one of Figure 1).  There are several flaws:  - In the case of the stack: I do not see a difference between this and a seq-to-seq RNN trained with e.g. 8, 6, 4 as input sequence, to predict 4, 6, 8.  - While some of the previous work is adequately cited, there is an important body of previous work (some from the 90s) on learning Peano's axioms, stacks, queues, etc. that is not cited nor compared to. For instance [Das et al. 1992], [Wiles & Elman 1995], and more recently [Graves et al. 2014], [Joulin & Mikolov 2015], [Kaiser & Sutskever 2016]...  - Using MNIST digits, and not e.g. a categorical distribution on numbers, is adding complexity for no reason.  - (Probably the biggest flaw) The experimental section is too weak to support the claims. The figures are adequate, but there is no comparison to anything. There is also no description nor attempt to quantify a form of \"success rate\" of learning such data structures, for instance w.r.t the number of examples, or w.r.t to the size of the input sequences. The current version of the paper (December 9th 2016) provides, at best, anecdotal experimental evidence to support the claims of the rest of the paper.  While an interesting direction of research, I think that this paper is not experimentally sound enough for ICLR.", "output": "en", "category": "Language Identification 893.json"}, {"instruction": "In this task, you are given a paper review. Based on the review, your job is to identify language and generate \"en\" if the review is in English or generate \"es\" if the review is in Spanish. Note that URLs in the text have been replaced with [Link].", "input": "RESUMEN:  El trabajo se presenta y describa la aplicaci\u00f3n de una red social para noticias (4x4News), donde las noticias m\u00e1s relevantes estar\u00e1n presentes (entre otras caracter\u00edsticas). Esta aplicaci\u00f3n se caracteriza por una pantalla divida en una matriz de 16x16, donde cada celda  es una noticia.   EVALUACI\u00d3N GENERAL:  El trabajo est\u00e1 bien escrito y sigue un formato (casi) completamente adecuado para INFONOR. La aplicaci\u00f3n suena interesante y potencialmente \u00fatil como un add-on para redes sociales ya com\u00fanmente usadas (ej. Facebook).  Hay varios puntos independientes que considero que son importantes de resolver:  1) Si bien es un trabajo de ingenier\u00eda interesante, no puedo ver cu\u00e1l es el aporte/contribuci\u00f3n desde el punto cient\u00edfico \u00bfQu\u00e9 se puede re-utilizar de este trabajo para un futuro trabajo cient\u00edfico?  2) Dado que ustedes ofrecen una aplicaci\u00f3n, creo que es fundamental un link para (com)probar esta aplicaci\u00f3n.  3) En mi opini\u00f3n, el sitio Web [Link] se aproxima a lo propuesto por este trabajo. Est\u00f3 es as\u00ed, porque terra permite registro de usuarios (perfiles), comentar noticias, valorar noticias, y configurar el sitio Web para mostrar las noticias que un usuario desea.  4) Este podr\u00eda ser un detalle menor, pienso que el uso de redes sociales se podr\u00eda usar para algo m\u00e1s que solamente mostrar la noticia m\u00e1s importante (ej. mostrar una especie de tracking (social) de una(s) noticia(s) similar).  DETALLES MENORES:  - Al final de la introducci\u00f3n, el uso de BOLD no es recomendado. - \u201cLos titulares no realizan un buen an\u00e1lisis\u201d. \u00bfQu\u00e9 es un \u201cbuen\u201d an\u00e1lisis?. Durante el trabajo, se observa varios adjetivos usados de manera ambiguas y que son innecesarios (ej. \u201ccomplejo proceso\u201d en la secci\u00f3n 2.1). - Dentro de la secci\u00f3n 2: \u201cDesarrollo\u201d, se encuentra el trabajo relacionado (secci\u00f3n 2.1). Esta secci\u00f3n deber\u00eda ser re-estructurada. - Las citas del tipo \u201cX, y otros, A\u00d1O\u201d (ej. secci\u00f3n 2.2 \u201cWilson, y otros, 2014\u201d), creo que est\u00e1n incorrectas. Deber\u00edan usar \u201cet al\u201d. - Secci\u00f3n 2.2, \u00bfla coincidencia de palabras es suficiente para decir que dos noticias son similares? Creo que un an\u00e1lsis m\u00e1s profundo es un necesario. - Explicar un poco m\u00e1s el concepto \u201cbundles\u201d. - En las referencias se usan n\u00fameros [1], pero en el texto no usa n\u00fameros. Recomiendo seguir el formato de INFONOR.", "output": "es", "category": "Language Identification 893.json"}]